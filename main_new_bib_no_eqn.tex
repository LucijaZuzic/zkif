%Version 3 October 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%
\usepackage{anyfontsize}
\usepackage{gensymb}

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
%\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

%\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

%\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\usepackage[acronym]{glossaries}

\makeglossaries
\newacronym{dst}{Dst}{Disturbance Storm-Time}
\newacronym{tec}{TEC}{Total Electron Content}
\newacronym{dtec}{dTEC}{detrended Total Electron Content}
\newacronym{gnss}{GNSS}{Global Navigation Satellite System}
\newacronym{pnt}{PNT}{Positioning, Navigation, and Timing}
\newacronym{svm}{SVM}{Support Vector Machine}
\newacronym{svn}{SVN}{Support Vector Network}
\newacronym{svc}{SVC}{Support Vector Clustering}
\newacronym{dt}{DT}{Decision Tree}
\newacronym{tdidt}{TDIDT}{Top-Down Induction of Decision Trees}
\newacronym{nb}{NB}{Naive Bayes}
\newacronym{ci}{CI}{Confidence Interval}
\newacronym{nir}{NIR}{No Information Rate}
\newacronym{nn}{NN}{Neural Network}
\newacronym{ann}{ANN}{Artificial Neural Network}
\newacronym{pls}{PLS}{Partial Least Squares}
\newacronym{pls-da}{PLS-DA}{Partial Least Squares Discriminant Analysis}
\newacronym{fda}{FDA}{Flexible Discriminant Analysis}
\newacronym{lda}{LDA}{Linear Discriminant Analysis}
\newacronym{pca}{PCA}{Principal Component Analysis}
\newacronym{manova}{MANOVA}{Multivariate Analysis Of Variance}
\newacronym{mars}{MARS}{Multivariate Adaptive Regression Splines}
\newacronym{nda}{NDA}{Normal Discriminant Analysis}
\newacronym{nas}{NAS}{Neural Architecture Search}
\newacronym{dbscan}{DBSCAN}{Density-Based Spatial Clustering}
\newacronym{cnn}{CNN}{Convolutional Neural Network}
\newacronym{rf}{RF}{Random Forest}
\newacronym{tp}{TP}{True Positive}
\newacronym{tn}{TN}{True Negative}
\newacronym{fp}{FP}{False Positive}
\newacronym{fn}{FN}{False Negative}
\newacronym{tpr}{TPR}{True Positive Rate}
\newacronym{tnr}{TNR}{True Negative Rate}
\newacronym{ppv}{PPV}{Positive Predictive Value}
\newacronym{npv}{NPV}{Negative Predictive Value}
\newacronym{dr}{DR}{Detection Rate}
\newacronym{dp}{DP}{Detection Prevalence}
\newacronym{ba}{BA}{Balanced Accuracy}
\newacronym{sar}{SAR}{Synthetic Aperture Radar}
\newacronym{cca}{CCA}{Canonical Correlation Analysis}
\newacronym{pod}{POD}{Proper Orthogonal Decomposition}
\newacronym{svd}{SVD}{Singular Value Decomposition}
\newacronym{eof}{EOF}{Empirical Orthogonal Functions}
\newacronym{evd}{EVD}{Eigenvalue Decomposition}
\newacronym{gps}{GPS}{Global Positioning System}
\newacronym{klt}{KLT}{Karhunen–Loève Theorem}
\newacronym{gpu}{GPU}{Graphics Processing Unit}
\newacronym{cpu}{CPU}{Central Processing Unit}
\newacronym{ram}{RAM}{Random Access Memory}
\newacronym{ml}{ML}{machine learning}
\newacronym{aa2}{(AA)2}{Ambient-Aware Application-Aligned}
\newacronym{tid}{TID}{Traveling Ionospheric Disturbance}
\newacronym{epb}{EPB}{Equatorial Plasma Bubbles}
\newacronym{gec}{GEC}{Global Electric Current}
\newacronym{spdf}{SPDF}{Space Physics Data Facility}
\newacronym{noaa}{NOAA}{National Oceanic and Atmospheric Administration}
\newacronym{swpc}{SWPC}{Space Weather Prediction Center}
\newacronym{nrcan}{NRCAN}{Natural Resources Canada}
\newacronym{gfz}{GFZ}{German Research Centre for Geosciences}
\newacronym{nasa}{NASA}{National Aeronautics and Space Administration}
\newacronym{euv}{EUV}{Extreme ultraviolet}

\begin{document}

\title[A $Dst$-based space weather conditions machine learning classification  model for GNSS PNT performance analysis]{A $Dst$-based space weather conditions machine learning classification  model for GNSS PNT performance analysis}

\author[1,2]{\fnm{Lucija} \sur{Žužić}}\email{lucija.zuzic@uniri.hr}
\equalcont{These authors contributed equally to this work.}

\author[3]{\fnm{Teodor B.} \sur{Iliev}}\email{tiliev@uni-ruse.bg}

\author*[1,2,4]{\fnm{Renato} \sur{Filjar}}\email{renato.filjar@uniri.hr}

\affil*[1]{\orgdiv{Department of Computer Engineering}, \orgname{Faculty of Engineering, University of Rijeka}, \orgaddress{\street{Vukovarska 58}, \city{Rijeka}, \postcode{51000}, \state{Primorje-Gorski Kotar County}, \country{Croatia}}}

\affil[2]{\orgdiv{Center for Artificial Intelligence and Cybersecurity}, \orgname{University of Rijeka}, \orgaddress{\street{Radmile Matejcic 2}, \city{Rijeka}, \postcode{51000}, \state{Primorje-Gorski Kotar County}, \country{Croatia}}}

\affil[3]{\orgdiv{Department of Telecommunication}, \orgname{University of Ruse}, \orgaddress{\street{8 Studentska str.}, \city{Ruse}, \postcode{7017}, \country{Bulgaria}}}

\affil[4]{\orgdiv{Laboratory for Spatial Intelligence}, \orgname{Hrvatsko Zagorje Krapina University of Applied Sciences}, \orgaddress{\street{Setaliste hrvatskog narodnog preporoda 6}, \city{Krapina}, \postcode{49000}, \state{Krapina-Zagorje County}, \country{Croatia}}}

\abstract{Ambient conditions classification enables systematic mitigation of adversarial effects on the Global Navigation Satellite System (GNSS) Positioning, Navigation, and Timing (PNT) performance. This research contributes to the problem by proposing a classification model of space weather events for sub-equatorial regions. The proposed model uses machine learning-based classification applied to the experimental observations of geomagnetic field components, observed Total Electron Content (TEC), and Disturbance Storm-Time (Dst)-index. A SVM with a Polynomial Kernel, C5.0 Decision Tree (DT), Naive Bayes (NB), shallow Neural Network (NN), Partial Least Squares (PLS), Flexible Discriminant Analysis (FDA), and shallow NN using Principal Component Analysis (PCA) was applied to develop the candidate model to classify observations of the geomagnetic field in $TEC$, combined with other variables, into one of the scenarios of space weather conditions. Performance is assessed using a confusion matrix and development time to yield the NB as the best performer. The proposed $Dst$-based classification model serves as an indicator of a geomagnetic/ionospheric storm in progress, thus alerting GNSS users of potential degradation in GNSS PNT performance and setting up a framework for the development of a tailored GNSS ionospheric correction model for specific classes of the space weather conditions.}

\keywords{Global Navigation Satellite System (GNSS) Positioning, Navigation, and Timing (PNT), space weather conditions, machine learning (ML), classification model, Decision Tree (DT), Neural Network (NN), sensor observations aggregation, geomagnetic field, Total Electron Content (TEC), Disturbance Storm-Time (Dst)-index}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}
%\label{sec:Intro}

The Global Navigation Satellite System (GNSS) and its Positioning, Navigation, and Timing (PNT) service have matured to become an essential part of national infrastructure, public goods, and enablers of a vast number of emerging technology and socio-economic applications \cite{filjar2022application}. Maintaining the GNSS PNT quality has appeared fundamental for the sustainable development of modern economy and society \cite{filjar2022application}. 

Overcoming the shortcomings and vulnerabilities of GNSS PNT is a scientific challenge, and the need of a wide variety of scientists, developers, operators, regulators, and users of GNSS-based systems and services \cite{filjar2024ambient, filjar2022application}. The Earth’s ionosphere, a part of the Earth’s atmosphere stretching from $50$ $km$ to $2000$ $km$ above the Earth’s surface and comprised of rare but mostly charged atoms and molecules, is the major natural cause of GNSS PNT degradation \cite{davies1990ionospheric, filic2018modelling}. 

This phenomenon especially affects most currently used GNSS receivers, which work as single-frequency receivers exposed to GNSS ionospheric effects \cite{spilker1996global, filjar2022application}. Driven by powerful and random flows of energy and particles from the Sun (space weather conditions), the ionospheric conditions define the properties of GNSS signal propagation through the Earth’s atmosphere and the resulting ionospheric delay \cite{davies1990ionospheric, oxley2017uncertainties, filic2018modelling}. The GNSS ionospheric delay causes errors in GNSS PNT algorithm deployment, designed to produce position, velocity, and time estimates for a GNSS receiver \cite{spilker1996global, filic2018modelling}. 

The ionosphere affects GNSS satellite signals for position estimation by introducing signal propagation delay \cite{spilker1996global, filic2018modelling}. The GNSS ionospheric delay is a stochastic variable, whose value is determined by complex physical processes of space weather \cite{filic2018modelling, filjar2022application}.

How space weather affects GNSS PNT performance was explained using the Space weather – GNSS PNT performance coupling model \cite{filic2018modelling}, as depicted in Table~\ref{tab:SpaceWeather}.

\begin{table}[!ht]
    \centering
    \caption{The Space weather – GNSS PNT performance coupling model, after \cite{filic2018modelling}.}
    \label{tab:SpaceWeather}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        SPACE & GEOMAGNETIC & IONOSPHERE & GNSS & GNSS \\
        WEATHER & FIELD & & PSEUDO- & POSITIONING \\
         & & & RANGES & PERFORMANCE \\
        \hline
    \end{tabular}
\end{table}

The GNSS signal encounters a certain number of charged particles from the satellite aerial to a mobile unit’s (GNSS receiver’s) aerial \cite{filic2018modelling, filic2018modelling}. Such an encounter is quantified using the Total Electron Content (TEC) defined by Equation~1 in $electrons/m^{2}$  \cite{filic2018modelling, davies1990ionospheric}, where $h$ denotes the height above the Earth’s mean sea level in $m$, $N(h)$ represents the vertical ionospheric profile, the volume density of charged particles at height $h$, in $electrons/m^{2}$.

\begin{equation}
	TEC = \int_{lower \quad ionsopheric \quad boundary}^{upper \quad ionsopheric \quad boundary}N(h)dh
	\quad\left(1\right)
\end{equation}

It should be noted that in the sense of Equation~1, $TEC$ is defined as a result, a consequence, of the ionospheric conditions, and not their descriptor \cite{filjar2022application}. The GNSS ionospheric delay may be determined by derivation from the Appleton-Hartree equation, as given in Equation~2, where $\Delta t_{iono}$ denotes the GNSS ionospheric delay in $s$, $c$ denotes the velocity of an electromagnetic wave in vacuum in $m/s$, and $f$ denotes the carrier wave frequency of the satellite signal in $Hz$ \cite{spilker1996global, filic2018modelling}.

\begin{equation}
	\Delta t_{iono} = \frac{40.3}{c f^{2}} \int_{lower \quad ionsopheric \quad boundary}^{upper \quad ionsopheric \quad boundary}N(h)dh
	\quad\left(2\right)
\end{equation}

Combining Equation~1 and Equation~2, one can conclude the linear relation between $\Delta t_{iono}$ and $TEC$, as given in Equation~3, a sub-model incorporated in the Space weather – GNSS PNT performance coupling model \cite{filic2018modelling}.

\begin{equation}
	\Delta t_{iono} = \frac{40.3}{c f^{2}} TEC
	\quad\left(3\right)
\end{equation}

The GNSS ionospheric delay has been identified as a source of GNSS PNT degradation since the dawn of GNSS. GNSS systems offer various standard GNSS ionospheric delay estimation (correction) models to mitigate the deteriorating effects on GNSS PNT, such as the Klobuchar model \cite{spilker1996global, filic2018modelling}. The standard ionospheric correction models are global, and insufficiently flexible to update to mitigate GNSS ionospheric delay to satisfy rising demands on GNSS PNT performance \cite{filjar2024ambient, filic2018modelling}. The development of regional and local models attempts to solve the problem of GNSS PNT sustainable performance in various ionospheric conditions \cite{sikirica2021risk}. In recent developments, our team has proposed the Ambient-Aware Application-Aligned ((AA)2) PNT to take into account the actual ionospheric and geomagnetic conditions near a mobile unit (a GNSS receiver) \cite{filjar2022application, filic2018modelling, filjar2024ambient}. Direct measurements of the immediate geomagnetic and ionospheric condition variables may be supplied to a machine learning-based adapted GNSS ionospheric correction model, thus solving the single-frequency GNSS problem \cite{filjar2022application}. Previous research has identified predictors and target variables (descriptors of geomagnetic, ionospheric, and GNSS PNT conditions) \cite{filic2018modelling, natras2022ensemble, natras2023regional}. The classification of different scenarios of GNSS ionospheric conditions with adverse effects on the GNSS PNT remained an unsolved precondition needed for the development of a machine learning-based GNSS ionospheric delay correction model to render the GNSS PNT algorithm ionospheric conditions-agnostic \cite{filjar2022application, filjar2024ambient}.

Here we propose a methodology for a machine learning-based classification of the ionospheric conditions based entirely on observations of geomagnetic indices. The proposed method is inspired by previous lightweight machine-learning techniques \cite{filjar2020comparison} for classifying sensor readings. The model is sufficiently simple to be applied on computationally capable platforms with suitable geomagnetic field sensors, such as smartphones and connected/autonomous vehicles. The research presented acquires the ambient data and analyses its statistical properties. The data set is split into training and test sets. Several candidates for the GNSS ionospheric delay model are developed in this study based on Disturbance Storm-Time (Dst). The machine learning (ML) models include a Support Vector Machine (SVM) with a Polynomial Kernel, C5.0 Decision Tree (DT), Naive Bayes (NB), shallow Neural Network (NN), Partial Least Squares (PLS), Flexible Discriminant Analysis (FDA) and shallow Neural Network (NN) using Principal Component Analysis (PCA) of the input data. A tailored set of validation methods is used to assess their performance. The optimal GNSS ionospheric delay correction model is identified based on GNSS PNT-related objective criteria, and its performance is demonstrated in an independent case study.

The main contributions of this paper are:

\begin{itemize}
    \item Defining multiple $Dst$-based classes using theoretical knowledge and statistics
    \item Classifying values of $TEC$ and other geomagnetic field variables based on $Dst$
    \item Proposing a machine learning-based classification model of space weather events
    \item Achieving an accuracy over $99\%$ for the Naive Bayes (NB) model and any input variables
\end{itemize}

The findings indicate the classification generated by the NB model could be used in real-life systems and support our hypothesis.

The rest of the paper is structured as follows. Section~\ref{sec:Dataset} describes this work's methodology, dataset, and evaluation metrics. Section~\ref{sec:Results} presents the results obtained in this study. Section~\ref{sec:Discussion} discusses the implications of the results. The main points are summarised and the conclusion is given in Section~\ref{sec:Conclusion}.

\section{Method and Data}
\label{sec:Dataset}

\subsection{Method}
%\label{subsec:Method}

A Support Vector Machine (SVM) with a Polynomial Kernel, C5.0 Decision Tree (DT), Naive Bayes (NB), Neural Network (NN), Partial Least Squares (PLS), Flexible Discriminant Analysis (FDA) and shallow Neural Network (NN) using Principal Component Analysis (PCA) of the input data were tested based on their ability to classify a set of observations of the geomagnetic field in $TEC$, and other predictors, into one of the scenarios of space weather conditions based on $Dst$. Multiple $Dst$-dependent classes were predefined using theoretical knowledge. Statistical analysis of the data confirmed that distributions of other variables change for different $Dst$ ranges, and not for different $TEC$ ranges, supporting the validity of the classification. The study assumes that the dependent output variable, the $Dst$ class, can be predicted based on the independent variables used as input.

The models were selected because they represent larger families of classification methods. SVM models are supervised maximum margin models. DT models also apply supervised learning. NB classifiers are probabilistic classifiers that can be parametric or non-parametric, but this study uses a non-parametric approach. PLS is a non-parametric linear regression model. FDA uses multiple non-parametric linear regression models to create a non-linear classification. PCA is a linear dimensionality reduction technique that extracts a predefined number of components for training an NN model. NN models imitate the brain using artificial neurons to produce outputs based on the input and the activation function. NN models require that the structure be predefined, and hyperparameters are usually tuned or predefined based on assumptions. An automated tuning procedure was used in this study.

All NN models were applied based on research by Kuhn for the \textit{R} \textit{caret} package \cite{kuhn2013applied, KuhnCaret2024, rprojectProjectStatistical}.

\subsubsection{Support Vector Machine}
%\label{subsubsec:SupportVectorMachine}

In machine learning, a Support Vector Machine (SVM) or Support Vector Network (SVN) model is a supervised maximum margin model with associated learning algorithms that analyzes data for classification and regression. In addition to linear classification, SVM models are also effective for non-linear classification using the hyperplane kernel trick \cite{Boser1992}, implicitly mapping their inputs to high-dimensional feature spaces.

An SVM model can also be used for regression tasks where the objective becomes $\epsilon$ -sensitive. A "soft margin" to separate data that is not linearly separable was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995 \cite{Cortes1995}. Support Vector Clustering (SVC) \cite{BenHur2001} applies support vector statistics to unlabeled data in unsupervised learning to find new natural groupings. Meyer, Leisch, and Hornik compared SVM with other classifiers \cite{Meyer2003}, but it is unclear whether SVM predictions perform better than other linear models, such as logistic, and linear regression.

A SVM constructs a set of hyperplanes in a high-dimensional space, used for classification, regression, or outlier detection \cite{scikit-learn2023}. Intuitively, a good separation is achieved by the hyperplane with the greatest distance to the nearest point in the training data belonging to any class \cite{HastieRosset2009}.

The original maximum margin hyperplane algorithm proposed by Vapnik in 1963 constructed a linear classifier. In 1992, Bernhard Boser, Isabelle Guyon, and Vladimir Vapnik proposed creating nonlinear classifiers using the kernel trick \cite{Boser1992, Aizerman1964}. A multidimensional feature space increases the generalization error of SVM models, so additional samples are needed to enhance performance \cite{Jin2012}. To keep the computational burden reasonable, a kernel probability density function $k(x, y)$ is chosen to fit the problem \cite{Press2007}.

SVM models have been used to solve various real-world problems. SVM models are useful in text and hypertext categorization, as their application can significantly reduce the need for labeled training instances in standard inductive and transductive settings \cite{Joachims1998}. Some methods for shallow semantic parsing are based on SVM models \cite{Pradhan2004}. Image classification and segmentation can also be performed using SVM models, including a modified version of SVM that uses privileged access as proposed by Vapnik \cite{Laurent2014, Barghout2015}.

Classification of satellite data such as Synthetic Aperture Radar (SAR) data is possible using a supervised SVM \cite{Maity2016}. Handwritten characters can be recognized using SVM models \cite{Decoste2002, Maitra2015}. The SVM algorithm is widely used in biological and other sciences. They were used to classify proteins with up to $90\%$ compounds correctly classified. Permutation tests based on SVM weights have been proposed as a mechanism for interpreting SVM models \cite{Gaonkar2013, Cuingnet2011}. SVM weights were also used to interpret SVM models \cite{Statnikov2006}.

The performance of SVM depends on the kernel probability density function, the parameters of the specified kernel probability density function, and the "soft" margin parameter $\lambda$. Each combination of parameter choices is usually cross-validated, and the parameters with the best accuracy are selected. The final model is used to test and classify new data and is trained using the best parameters \cite{Hsu2003}.

\subsubsection{Decision Tree}
%\label{subsubsec:DecisionTree}

Decision Tree (DT) models are an approach to supervised learning in statistics, data mining, and machine learning. They represent predictive or decision-making model used to make inferences about a series of observations. Tree models in which the target variable can take on a discrete set of values are called classification trees. DT models where the target variable can take on continuous values (usually real numbers) are called regression trees. A regression tree can be extended to any object with pairwise differences such as categorical arrays \cite{Studer2011}. DT models are among the most popular ML algorithms due to their comprehensibility and simplicity \cite{Wu2008}. A DT describes the input data and the resulting classification tree can be the input for decision-making in data mining \cite{Rokach2014}.

A tree is built by dividing the original set, which forms the root node, into subsets that form descendants, or successors. The segmentation is made using a set of rules based on classification features \cite{ShalevShwartz2014}. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is complete when the subset at a node all have the same target variable values or the division no longer adds value to the predictions. This process of Top-Down Induction of Decision Trees (TDIDT) \cite{Quinlan1986} is an example of a greedy algorithm and is by far the most common strategy for learning DT models from data \cite{Rokach2005}. C5.0, used in the \textit{caret} package in \textit{R}, has a similar approach and improves the ID3 and C4.5 algorithms.

DT models can also be described as a combination of mathematical and computational techniques that help describe, categorize, and generalize a given data set in data mining. Data comes in records of the form $(x, Y) = (x_{1}, x_{2}, x_{3}, \ldots, x_{k}, Y)$. The dependent variable $Y$ is the target variable we need to understand, classify, or generalize. Features $x_{1}, x_{2}, x_{3}, \ldots, x_{k}$ form the vector $X$ used for that task.

\subsubsection{Naive Bayes}
%\label{subsubsec:NaiveBayes}

In statistics, Naive Bayes (NB) models classifiers are a family of linear "probabilistic classifiers" that assume that, given a target class, the features are conditionally independent. The classifier got its name from this strong and possibly naive assumption. These classifiers are among the simplest Bayesian network models \cite{McCallum2011}. NB classifiers are highly scalable and require several parameters. The number of parameters depends linearly on the number of variables. Variables represent features or predictors in the learning problem. Maximum likelihood training can be performed by evaluating a closed-form expression \cite{Russell1999}, which requires linear time, instead of the expensive iterative approximation used for many other classifiers.

In the statistical literature, NB models are known by various names, including simple Bayes and independent Bayes \cite{Hand2001}. These names refer to Bayes' theorem in the classifier's decision rule, but a NB classifier model is not necessarily a Bayesian method \cite{Russell1999, Hand2001}. In other words, one can work with a NB model without accepting Bayesian probability or methods. In many practical applications, parameter estimation for NB models uses the maximum likelihood method.

Despite their simple design and seemingly oversimplified assumptions, NB classifier models have performed quite well in many complex real-world situations. In 2004, an analysis of the Bayesian classification problem showed that there are reasonable theoretical reasons for the seemingly incredible performance of NB classifier models \cite{Zhang2004}. However, a comprehensive comparison with other classification algorithms in 2006 showed that Bayesian classification performed worse than other approaches, such as boosted trees or random forests \cite{Caruana2006}. The advantage of NB models is a small amount of training data used to estimate the parameters needed for classification \cite{stackexchange}.

In the abstract, NB models are conditional probability models. The model assigns probabilities $p(C_{k}\mid x_{1},\ldots, x_{n})$ to each of the $K$ possible outcomes or classes $C_{k}$ given the problem instance to be classified, represented by the vector $x = (x_{1},\ldots, x_{n})$ which encodes $n$ features representing mutually independent variables \cite{Murty2011}. The problem with the above formulation is that if the number of features $n$ is large or the feature has a large value range, it is infeasible to base a model on probability tables. The model must therefore be reformulated using conditional probability represented by Bayes' theorem in Equation~4.

\begin{equation}
	p(C_{k}\mid \mathbf{x})={\frac{p(C_{k})\ p(\mathbf{x} \mid C_{k})}{p(\mathbf{x})}}
	\quad\left(4\right)
\end{equation}

To estimate feature distribution parameters, we must assume a parametric distribution, or generate non-parametric models for the features from the training data set \cite{John2013}. Assumptions about the distribution of features are called an event model or Bayesian classifier, which can introduce confusion by creating both a continuous and discrete model \cite{Mccallum2001, Metsis2006}. Multinomial and Bernoulli distributions have often been used for discrete features in document classification, or spam filtering. The Gaussian NB classifier model was used since the features used in this study assumed continuous values.

Clustering is often used to discretize continuous values. The new set of features obtained by discretization follows the Bernoulli distribution. Some literature claims that discretization is necessary to use a NB classifier model, but discretization may discard the information needed to distinguish between classes \cite{Hand2001}.

Sometimes the distribution of class-based marginal densities is far from normal. The marginal probability density of each class can be used to estimate the kernel probability density function in such cases. This method, introduced by John and Langley \cite{John2013}, can significantly increase the accuracy of the classifier \cite{Piryonesi2020, HastieRosset2009}.

\subsubsection{Neural Networks}
%\label{NeuralNetworks}

The neurons of human or animal brains provide the basis for a Neural Network (NN) or Artificial Neural Network (ANN) with connected units or nodes called artificial neurons in machine learning \cite{mitExplainedNeural, brahme2014comprehensive}. 

Shallow NN models are simple and easy to train quickly with fewer parameters, and computational resources. They typically contain only a few hidden layers for processing between the input layer that receives the data, and the final layer that produces the output \cite{olden2002illuminating, ozesmi1999artificial}.  A network with at least two hidden layers \cite{bishop2006pattern} is considered a deep NN model.

NN models are typically trained through empirical risk minimization. This method optimizes the network's parameters to minimize the difference or empirical risk, between the predicted output and the actual target values in a given dataset \cite{vapnik2013nature}. Gradient-based methods such as backpropagation are usually used to estimate network parameters  \cite{vapnik2013nature}. During the training phase, ANN models learn from labeled training data by iteratively updating their parameters to minimize a defined loss function \cite{goodfellow2016deep}.

Using ANN models requires an understanding of their characteristics. The model suitable for a specific task varies depending on how the data is stored and other practical considerations. Many model parameters may need to be adjusted and differ for each model. The hyperparameters for the learning algorithm may also be modified to suit the problem \cite{probst2019tunability} during an extensive tuning process experimenting on unseen training data, and various learning algorithms are available.

The network design process can be simplified using previously defined ML techniques like Neural Architecture Search (NAS). These networks have performed well even when pitted against architectures selected by humans. The proposed model is tested and the responses are used to train the NAS network \cite{zoph2016neural}. \textit{AutoML} and \textit{AutoKeras} \cite{jin2019auto} provide extensive frameworks for this process. The \textit{scikit-learn} library features various classification, regression, and clustering algorithms including SVM, Random Forest (RF), gradient boosting, k-means, and Density-Based Spatial Clustering (DBSCAN). The hyperparameters must be predefined, are not trained, and include how many neurons are in each layer, learning rate, step, stride, depth, receptive field, and padding for Convolutional Neural Network (CNN) \cite{claesen2015hyperparameter}. \textit{TensorFlow} and \textit{Keras} are used to build a custom network in \textit{Python}, and the user can define layers, models, or metrics. The data, number of hidden layer units, learning rate, and number of iterations are required parameters for model training based on a book by Liu \cite{liu2020python}.

\subsubsection{Partial Least Squares}
%\label{subsubsec:PLS}

Partial Least Squares (PLS) regression, or projection to latent structures, \cite{wold2001pls, abdi2010partial}, is a linear regression statistical model that transforms the predicted and the observable variables to a new space instead of identifying maximum variance hyperplanes used in a linear regression model. Svante and Herman O. A. Wold, a Swedish father and son, founded this approach. PLS methods are bilinear factor models because the $X$ and $Y$ are projected to new spaces. In Partial Least Squares Discriminant Analysis (PLS-DA), $Y$ is categorical \cite{saebo2008lpls}.

Using $n$ paired observations $\left(\vec{x_{i}}, \vec{y_{i}}\right), i \in 1, \dots, n$. PLS finds the normalized direction $\vec{p_{j}}, \vec{q_{j}}$ that maximizes the covariance in the first step $j = 1$ \cite{youtubeYouTube}, shown in Equation~5. Matrix notation is used in further discussion of the model.

\begin{equation}
	\max_{{\vec{p}}_{j},{\vec{q}}_{j}}\operatorname{E} [\underbrace{({\vec{p}}_{j}\cdot {\vec{X}})}_{t_{j}}\underbrace{({\vec{q}}_{j}\cdot {\vec{Y}})}_{u_{j}}
	\quad\left(5\right)
\end{equation}

The scores form an orthogonal basis in PLS regression, and the loadings are chosen to achieve this. In PCA orthogonality is not imposed on scores but onto loadings instead \cite{lindgren1993kernel, de1994comments, dayal1997improved, de1993simpls, rannar1994pls, abdi2010partial}. Many versions of PLS exist for estimating the factor and loading matrices, such as the PLS1 algorithm \cite{takane2016pls, hoskuldsson1988pls}. 

\subsubsection{Flexible Discriminant Analysis}
%\label{subsubsec:FDA}

Flexible Discriminant Analysis (FDA) is a general methodology that creates the discriminant surface for a multigroup non-linear classification model based on a mixture of non-parametric linear regression models \cite{hastie1995penalized}, such as Multivariate Adaptive Regression Splines (MARS) and Linear Discriminant Analysis (LDA).

Many predictors can be used at once in FDA \cite{HastieTibshirani2009}, variable interactions are automatically noted \cite{hastie1994flexible}, it is complex but execution time and computational load are adequate \cite{reynes2006choice}. The algorithm is not significantly affected by outliers \cite{phillips2017applying}.

Modifying different settings has a high impact on FDA \cite{hallgren2019species}, and the estimation might be unsuccessful if the predictor distributions are highly correlated. FDA is prone to overfitting \cite{thuiller2016ensemble}, and challenging to comprehend or explain \cite{quillfeldt2017influence}. The assumption of normality for the continuous independent variables is necessary \cite{zhang2019using} to determine the response or grouping categorical variable. 

LDA, Normal Discriminant Analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear LDA defined in 1936 \cite{cohen2013applied}. The results of LDA may be utilized directly or to reduce dimensionality before classification, which is more often used \cite{hansen2005using}. LDA finds a linear combination of features or measurements differentiating two or more sample classes \cite{fisher1936use, mclachlan2005discriminant}. Discriminant analysis has continuous independent variables and a categorical dependent variable representing the groups \cite{wetcher2011analyzing}.

LDA is closely related to PCA and factor analysis since they identify linear functions of multiple variables that fit the input with the smallest error \cite{martinez2001pca}. Discriminant correspondence analysis \cite{abdi2007discriminant, perriere2003use} is equivalent to LDA for categorical independent variables. Points in discriminant function analysis are rated based on quantitative predictors and a group measure \cite{cokluk2008discriminant} to represent classification \cite{venables2013modern}.

The assumptions of Multivariate Analysis Of Variance (MANOVA) are also true for discriminant analysis. The analysis is highly affected by outliers so there must be fewer predictor variables than samples in the smallest class \cite{cokluk2008discriminant}. In every level of the grouping variable, independent variables must be normal to achieve multivariate normality \cite{cokluk2008discriminant, hansen2005using}. Box's M statistic \cite{hansen2005using} test whether homogeneity of variance/covariance is established and whether variances among group variables change for predictor levels. 

Quadratic instead of linear discriminant analysis should be utilized if covariances are different \cite{cokluk2008discriminant}. The scores of one predictor variable should not depend on other predictor variables for any sample, and the samples should be chosen at random \cite{cokluk2008discriminant, hansen2005using}. Discriminant analysis can handle small deviations from these requirements \cite{lachenbruch1979discriminant} and may be used even if multivariate normality is not achieved, such as for dichotomous variables \cite{klecka1980discriminant}.

The discriminant rules suggest that if $x\in \mathbb{R}_{j}$, then $x\in j$ for a group $j$, and $\mathbb{R}_{j}$ sets of sample space. The discriminant analysis method identifies regions of $\mathbb{R}_{j}$ that decrease errors of classification \cite{Hardle2003AppliedMS}. A discriminant score describes how well a discriminant differentiates classes. \cite{archive765Discriminant}.
 
The discriminant analysis eigenvalue represents the characteristic root of each function, indicating the quality of the group separation produced by the function. The function differentiates better for a larger eigenvalue \cite{cokluk2008discriminant}. Eigenvalues do not have an upper limit \cite{cokluk2008discriminant, hansen2005using}, so the results should not be taken at face value.

Even though it is not universally accepted, some suggest eigenvalues be utilized as effect size measures \cite{hansen2005using}. The percent of correctly classified data, such as the kappa value, can also be used as the effect size while accounting for random matches in classification \cite{hansen2005using}. Kappa is not biased for any particular class, whether performance is high or low, and instead normalizes across all classes \cite{israel2006performance}. 

Fisher's linear discriminant and LDA are sometimes considered synonyms. However, Fisher \cite{fisher1936use} describes a slightly different discriminant, which does not make some of the assumptions of LDA such as a normal distribution or the equality of groups covariances. The application of the Fisher discriminant is expanded to find a subspace capturing all class variability \cite{archive765Discriminant} for multiple groups, as suggested by C. R. Rao \cite{rao1948utilization}.

\subsubsection{Principal Component Analysis}
%\label{subsubsec:PCA}

Principal Component Analysis (PCA) is a linear dimensionality reduction technique with applications in exploratory data analysis, visualization, and data preprocessing. It is used in preprocessing before training a Neural Network in the $pcaNNe$ method in the \textit{caret} package in \textit{R} \cite{ripley2007pattern}. The data is linearly transformed into a new coordinate system such that the directions (principal components) capturing the largest variation in the data can be easily identified. Many studies use the first two principal components to plot the data and visually identify clusters of closely related data points \cite{jolliffe2016principal}.

PCA is closely related to factor and Canonical Correlation Analysis (CCA). PCA is the simplest of the true eigenvector-based multivariate analyses. PCA defines a new orthogonal coordinate system that optimally describes the variance in a single dataset \cite{barnett1987origins, hsu2012spectral, markopoulos2017efficient, chachlakis2019l1}. CCA does the same for the cross-covariance between two datasets. Robust and L1-norm-based variants of standard PCA have also been proposed \cite{chachlakis2019l1, markopoulos2014optimal, zhan2015robust, ke2005robust}.

Karl Pearson \cite{pearson1901liii} conceived of PCA in 1901 as a counterpart of the principal axis theorem in mechanics \cite{stewart2019introduction}. Harold Hotelling discovered the Hotelling transform in multivariate quality control in the 1930s without knowledge of the previous work \cite{hotelling1933analysis, hotelling1992relations}. Synonyms for PCA include the discrete Proper Orthogonal Decomposition (POD) \cite{berkooz1993proper, karhunen1946spektraltheorie, loeve1977elementary, sirovich1987turbulence} in mechanical engineering and the Kosambi–Karhunen–Loève or Karhunen–Loève Theorem (KLT) \cite{sapatnekar2011overcoming, ghoman2012pod, archiveKarhunenLoeveTransform, giambartolomei2016, mallat1999wavelet, tang1998texture} in signal processing. Singular Value Decomposition (SVD) of $X$, established in the final quarter of the $19^{th}$ century \cite{stewart1993early}, is also similar to PCA. Eigenvalue Decomposition (EVD) of a matrix $\mathbf{X}^{\mathsf{T}}\mathbf{X}$ in linear algebra shares the mathematical foundations of PCA \cite{gloub1996matrix, hayden2002observations}. Factor analysis differs in several key features \cite{jolliffe2002principal}, but has many aspects in common with PCA. Empirical Orthogonal Functions (EOF) \cite{lorenz1956empirical} in meteorological science were developed by Lorenz in 1956 \cite{lorenz1956empirical}. The Eckart–Young theorem \cite{eckart1936approximation} and quasiharmonic modes \cite{dove1993introduction} are both connected to PCA. Empirical modal analysis in structural dynamics and spectral decomposition in noise and vibration also take a similar approach to PCA.

The data in PCA is converted to a new coordinate system using a scalar projection, an orthogonal linear transformation on a real inner product space. The first coordinate or principal component covers the largest portion of the variance, decreasing with each following component \cite{jolliffe2002principal, bengio2013representation}. The significance of the principal components can be checked with parametric bootstrap for a smaller dataset to help decide how many principal components to keep \cite{forkman2019hypothesis}.

\subsection{Data Description and Analysis}
%\label{subsec:Data}

The performance of GNSS is linked to many dynamic space weather conditions. Solar activity can \cite{tobiska1996current} can induce various effects that degrade the accuracy, availability, and reliability of GNSS PNT.

Disturbances caused by geomagnetic storms \cite{vitinsky1986statistics} can also affect the high-frequency GPS signals \cite{lanyi1988comparison} passing through the ionosphere layer \cite{rishbeth1969introduction, ratcliffe1972introduction}, motivating work on error modeling of the ionosphere \cite{klobuchar1986design, zolesi2014ionospheric}. Observed changes in the ionosphere layer include the density distribution in the ionosphere, $TEC$ values, and the current balance in the ionosphere \cite{komjathy2023global}.

Geomagnetic storms \cite{NOAA2024aRadioCommunications} cause signal deterioration by affecting Global Electric Current (GEC) variability \cite{Gray2019, Gray2021}. The ionosphere \cite{wild1994ionosphere} may show changes \cite{hawarey2010uzay} related to geographical and geomagnetic location, geomagnetic and solar activity, the appearance of sunspots, local time \cite{kotz2005encyclopedia}, seasonality, thunderstorms \cite{vellinov1992ionospheric}, nuclear experiments on Earth, earthquakes \cite{liu20142013}, and other phenomena. These effects are characterized by geomagnetic storm $K_{p}$-indices and geomagnetic activity $Dst$-indices, where active changes from space and Earth stations are measured and assessed.

Many parameters \cite{ulukavak2018analysis} can describe the Earth's geomagnetic field. This study focuses on parameters describing disturbances of the Earth's geomagnetic field, most importantly $a_{p}$ indices calculated from $K_{p}$ and $K$ indices, $Total Electron Content (TEC)$, $detrended Total Electron Content (dTEC)$, and $Disturbance Storm-Time (Dst)$. 

Incorporating parameters such as the $K_{p}$-indices and $a_{p}$-indices, which provide global measures of geomagnetic activity, alongside local $TEC$ and $Dst$-index values, allows for a more detailed assessment of the space environment and its potential effects on GNSS signals. $a_{p}$, $TEC$, $dTEC$, $Dst$ were used with $B_x$, $B_y$, and $B_z$ representing components of the Earth's magnetic field to train machine-learning models.

\subsubsection{Magnetic Field Indices}
%\label{subsec:BxByBz}

The Earth's magnetic field has similarities to that of a bar magnet. However, plasma gushes \cite{schwenn2001solar} coming from the solar corona and the domain of the Sun influence the interplanetary magnetic field \cite{melnikov1990relationships}. $B_{x}$, $B_{y}$ and $B_{z}$ vector represent interplanetary magnetic field indices. $B_{x}$ and $B_{y}$ are parallel to the plane of orbits, and the third component $B_{z}$ is perpendicular. 

The Sun's magnetic field is $100$ times larger than the Earth's. Sunspots \cite{harden2005solar} appear when the Sun's activity is highest. Sunspots are filled with magnetism and large magnetic field lines, and matter sputtering can occur \cite{gonzalez1994geomagnetic}. Affected by solar winds \cite{Viereck2024}, the Earth's magnetic field lines are compressed in the sun's direction and expand in the wind's direction. Twin poles (dipoles) are locations with a high concentration of magnetic field lines. The result is a tear-drop shape of the Earth's magnetosphere. The Sun's magnetic field lines are distant along the equatorial line but closer together in the poles, just like the Earth's \cite{aschwanden2006physics}.

The $B_{z}$ magnetic field component turns south in the initial phase of a magnetic storm while conditions on the north side are stable \cite{abraha2014total}. The electric ($E$) and magnetic ($B$) field is oriented westward in quiet conditions, with $E \times B$ rotation in the $B_{y}$ electric field. During magnetic disturbances in equatorial regions, the $B_{y}$ electric field decreases, as does the downward slip from a positive to a negative value, leading to the positive phase of the storm. When severe deterioration occurs, the $B_{y}$ downward slip increases and the negative phase of a storm begins \cite{turunen1980examples, adeniyi1986magnetic}. An expansion of the F2 layer is also observed due to excess plasma production. Magnetic field indices are available in one-hour resolution from the web interface of the National Aeronautics and Space Administration (NASA) Goddard Space Physics Data Facility (SPDF) \cite{Papitashvili2024About, Papitashvili2024Explorer}.
 
\subsubsection{Geomagnetic Storm Indices}
%\label{subsec:K}

The geomagnetic storm $K$-index is an integer from $0$ to $9$ measuring disturbances in global geomagnetic activity derived from $B_{x}$ and $B_{y}$, the horizontal component of the Earth's magnetic field \cite{de2007ionosphere}. The maximum positive and negative fluctuations of $B_{x}$ and $B_{y}$, relative to a quiet day, and based on magnetometer observations during $3$ hours, are added together to determine the total maximum fluctuation.

Each observatory uses different threshold values to convert the maximum $nT$ (nanotesla) fluctuation to a $K$-index value. The threshold values for each observatory are adjusted so that the historical rate of occurrence for each $K$-index value is similar across all observatories. Observatories with a lower geomagnetic latitude use a lower fluctuation in $B_{x}$ and $B_{y}$ to achieve each $K$-index value. 

The threshold valid in Boulder, Colorado \cite{NOAA2024bKindex} for transforming the maximum $B_{x}$ and $B_{y}$ fluctuation measured in $nT$ using a magnetometer into a $K$-index value is given in Table~\ref{tab:K}.

\begin{table}[!ht]
    \centering
    \caption{The relationship between the $K$-index and  magnetometer observations of $B_{x}$ and $B_{y}$ in $nT$ in Boulder, Colorado.}
    \label{tab:K}
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        $K$ & $0$ & $1$ & $2$ & $3$ & $4$ \\ \hline
        $B_{x}$ and $B_{y}$ ($nT$)& $0$ - $5$ & $5$ - $10$ & $10$ - $20$ & $20$ - $40$ & $40$ - $70$ \\ \hline
        $K$ & $5$ & $6$ & $7$ & $8$ & $9$ \\ \hline
        $B_{x}$ and $B_{y}$ ($nT$) & $70$ - $120$ & $120$ - $200$ & $200$ - $330$ & $330$ - $500$ & $>500$ \\ \hline
    \end{tabular}
\end{table}

\subsubsection{Planetary Geomagnetic Storm Indices}
%\label{subsec:Kp}

The planetary geomagnetic storm $K_{p}$-index \cite{Matzka2024} is derived from $3$-hour-based $K$ indices from $13$ magnetometer stations between $44$ and $60$ degrees of north and south latitude. Announcements and warnings \cite{NOAA2024cAlertsWatches} of geomagnetic changes and disturbances in the Earth's magnetic field are based on the $K_{p}$-index.

Hourly geomagnetic storm index data are available on the NASA Goddard SPDF \cite{Papitashvili2024About, Papitashvili2024Explorer} web pages. The scale values of the $K_{p}$-index are determined by the change of the geomagnetic field and the geomagnetic storm effect in $nT$.

The official planetary $K_{p}$-index is a weighted average of $K$-indices from multiple observatories. When $K$-index data is not available in real-time, operators such as The National Oceanic and Atmospheric Administration (NOAA) Space Weather Prediction Center (SWPC) calculate near real-time estimates of the $K_{p}$-index \cite{Myint2022}.

The NOAA SWPC \cite{NOAA2024dPlanetary} uses ground-based magnetometers located in Sitka, (Alaska), Meanook (Canada), Ottawa (Canada), Fredericksburg (Virginia), Hartland (UK), Wingst (Germany), Niemegk (Germany), and Canberra (Australia). The SWPC collaborates with the U.S. Geological Survey, Natural Resources Canada (NRCAN), the British Geological Survey, the German Research Centre for Geosciences (GFZ), and Geoscience Australia to gather additional data. The Institut de Physique du Globe de Paris and the Korean Space Weather also contributed magnetometer observations. 

$K$-index Watches are issued when the highest predicted NOAA estimated $K_{p}$-indices for a day are $K$ $=$ $5$, $6$, $7$, or $\geq 8$ and is reported in terms of the NOAA G scale. $K$-index Warnings are issued when NOAA estimated $K_{p}$-indices of $4$, $5$, $6$, and $7$ or greater are expected. $K$-index Alerts are issued when the NOAA estimated $K_{p}$-indices reach $4$, $5$, $6$, $7$, $8$, or $9$. The $K_{p}$-index as related to geomagnetic storm descriptions and warnings using the NOAA G scale \cite{NOAA2024eSpace, NOAA2024fScales} is given in Table~\ref{tab:Kp}.

In March 2021, $K_{p}$ was assigned a DOI with a dataset \cite{Matzka2021a} and a scientific publication \cite{Matzka2021b} for reference.

\begin{table}[!ht]
    \centering
    \caption{The $K_{p}$-index as related to geomagnetic storm descriptions and warnings using the NOAA G scale \cite{NOAA2024eSpace, NOAA2024fScales}.}
    \label{tab:Kp}
    \begin{tabular}{|c|c|c|}
        \hline
        $K_{p}$-index & NOAA Space Weather Scale Geomagnetic Storm Level \\ \hline
        $0\leq K_{p}<5$ & G0 \\ \hline
        $K_{p}=5$ & G1 \\ \hline
        $K_{p}=6$ & G2 \\ \hline
        $K_{p}=7$ & G3 \\ \hline
        $K_{p}=8, 9_{-}$ & G4 \\ \hline
        $K_{p}=9$ & G5 \\ \hline
    \end{tabular}
\end{table}

\subsubsection{Equivalent Three Hourly Range Geomagnetic Storm Indices}
%\label{subsec:A}

The $A$-index \cite{NOAA2024gGeomagneticIndices} represents a daily average level of magnetic activity. Because the relationship between the $K$-index and magnetometer fluctuations is not linear, the $K$-index values are not directly used for calculating average values. Each $K$-index or $K_{p}$-index is converted into the "equivalent three hourly range" $a$-index or $a_{p}$-index that uses a linear scale. Table~\ref{tab:a} illustrates the conversion between $K$-index and $a$-index values \cite{NOAA2024bKindex}. Table~\ref{tab:ap} illustrates the conversion between $K_{p}$-index and $a_{p}$-index values \cite{NOAA2024gGeomagneticIndices}. An average of $8$ $a$-indices (lowercase) is used as the daily $A$-index (uppercase). As an example, if $8$ $K$-indices were $3$, $4$, $6$, $5$, $3$, $2$, $2$, and $1$, the $a$-indices equal $15$, $27$, $80$, $48$, $15$, $7$, $7$, and $3$, and the $A$-index equals $A = (15 + 27 + 80 + 48 + 15 + 7 + 7 + 3)/8 = 202 / 8 = 25.25$.

\begin{table}[!ht]
    \centering
    \caption{The $a$-index values for each $K$-index, from \cite{NOAA2024bKindex}.}
    \label{tab:a}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
        \hline
        $K$ & $0$ & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $8$ & $9$ \\ \hline
        $a$ & $0$ & $3$ & $7$ & $15$ & $27$ & $48$ & $80$ & $140$ & $240$ & $400$ \\ \hline
    \end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{The $a_{p}$-index values for each $K_{p}$-index, from \cite{NOAA2024gGeomagneticIndices}.}
    \label{tab:ap}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
        \hline
        $K_{p}$ & $0_{o}$ & $0_{+}$ & $1_{-}$ & $1_{o}$ & $1_{+}$ & $2_{-}$ & $2_{o}$ & $2_{+}$ & $3_{-}$ & $3_{o}$ & $3_{+}$ & $4_{-}$ & $4_{o}$ & $4_{+}$ \\ \hline
        $a_{p}$ & $0$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $9$ & $12$ & $15$ & $18$ & $22$ & $27$ & $32$ \\ \hline
        $K_{p}$ & $5_{-}$ & $5_{o}$ & $5_{+}$ & $6_{-}$ & $6_{o}$ & $6_{+}$ & $7_{-}$ & $7_{o}$ & $7_{+}$ & $8_{-}$ & $8_{o}$ & $8_{+}$ & $9_{-}$ & $9_{o}$ \\ \hline
        $a_{p}$ & $39$ & $48$ & $56$ & $67$ & $80$ & $94$ & $111$ & $132$ & $154$ & $179$ & $207$ & $236$ & $300$ & $400$ \\ \hline
 \end{tabular}
\end{table}

\subsubsection{Total Electron Content}
%\label{subsubsec:TEC}

The content of gaseous atoms and molecules in the atmosphere increases due to daytime Extreme ultraviolet (EUV) rays and X-rays emitted from the Sun, causing photoionization. Photoionization is a process in which positively charged ions and negatively charged free electrons form and change the electron-ion concentration \cite{prolss2012physics}.

Changes in $Total Electron Content (TEC)$ levels \cite{liu2009seismoionospheric} can increase signal delay which can cause positioning errors, especially critical for high-precision applications \cite{durmaz2015regional, jin2012m_dcb}. $TEC$ can be calculated as indicated in Equation~1, Equation~2, and Equation~3. 

The Global Positioning System (GPS) ionospheric delay \cite{schaer1999mapping} on the radio ray path between the satellite and receiver is defined in Equation~6 \cite{spilker1996global} where $\Delta t$ represents the GPS ionospheric delay, $c$ is the velocity of light, and $n(l)$ is the refraction index on the path $l$.

\begin{equation}
	\Delta t = \frac{1}{c} \int_{satellite}^{receiver} \left[ n(l) - l \right]dl
	\quad\left(6\right)
\end{equation}

The GPS ionospheric delay $\Delta \rho \dots$ related to distance measurements can be defined using Equation~7 \cite{spilker1996global}.

\begin{equation}
	\Delta \rho = \frac{1}{c} \int_{satellite}^{receiver} \left[ n(l) - l \right]dl
	\quad\left(7\right)
\end{equation}

Appleton’s formula in Equation~8 reasonably approximates the relationship between the refraction index and the free electron density distribution \cite{davies1990ionospheric, spilker1996global}. $N(h)$ is the vertical distribution of the free electron density over a height $h$ above the surface of the Earth, and $f$ is the frequency of the radio signal.

\begin{equation}
	n(l) = 1 + \frac{40.3 N(h)}{f^{2}}
	\quad\left(8\right)
\end{equation}

The combination of Equation~6 and Equation~8 yields Equation~9 that directly describes the relationship between the free electron density profile and the GPS ionospheric delay \cite{spilker1996global}.

\begin{equation}
	\Delta t = \frac{40.3}{c f^{2}} \int_{receiver}^{satellite} N(h)dh
	\quad\left(9\right)
\end{equation}

The $TEC$ \cite{spilker1996global} in Equation~10 is calculated using the integral on the right side of Equation~9.

\begin{equation}
	TEC = \int_{receiver}^{satellite} N(h)dh
	\quad\left(10\right)
\end{equation}

Using equation Equation~10, Equation~9 can be written as shown in Equation~11.

\begin{equation}
	\Delta t = \frac{40.3}{c f^{2}} TEC
	\quad\left(11\right)
\end{equation}

Following the same analogy, the equivalent GPS ionospheric delay is calculated in Equation~12.

\begin{equation}
	\Delta \rho = \frac{40.3}{c f^{2}} TEC
	\quad\left(12\right)
\end{equation}

Considering the pseudo-range measurements taken on both GPS transmitting frequencies ($f_{1}=1575.42$ $MHz$ and $f_{2}=1227.60$ $MHz$) and using Equation~12, the relation between the $TEC$ and GPS pseudo-range measurements can be established in Equation~13 \cite{spilker1996global}:
 
\begin{equation}
	\rho_{1} - \rho_{2} = 40.3 TEC \left( \frac{1}{f_{1}^{2}} - \frac{1}{f_{2}^{2}} \right)
	\quad\left(13\right)
\end{equation}

Equation~13 can be rewritten as in Equation~14, defining $TEC$. The actual value of the $TEC$ can be computed utilizing a dual-frequency GPS receiver and Equation~14.

\begin{equation}
	TEC = \frac{\rho_{1} - \rho_{2}}{40.3} \frac{{\left(f_{1}f_{2}\right)}^{2}}{f_{2}^{2}-f_{1}^{2}}
	\quad\left(14\right)
\end{equation}

Considering the imposed regulations for common GPS dual-frequency receiver use, a different method for estimating the $TEC$ and GPS ionospheric delay is needed. A standard GPS ionospheric delay model, such as the Klobuchar model in Equation~15 \cite{enge1994global,spilker1996global}. A cosine-like daily dynamic of GPS ionospheric delay values is assumed in the Klobuchar model, with the daily maximum appearing around 14:00 in local time \cite{klobuchar1987ionospheric}.

\begin{equation}
	\Delta t = F \left[ t_{n} + A \cos \frac{2\pi\left(t-t_{0}\right)}{P} \right]
	\quad\left(15\right)
\end{equation}

\subsubsection{Detrended Total Electron Content}
%\label{subsubsec:dTEC}

Some ionospheric phenomena, such as Traveling Ionospheric Disturbance (TID) and Equatorial Plasma Bubbles (EPB), are most clearly discernable by examining detrended TEC data instead of absolute TEC measurements of the state of the ionosphere derived from GNSS. A TEC data detrending method developed by Pradipta et al. \cite{Pradipta2015} can differentiate wavelike fluctuations associated with TID and deep depletions associated with EPB. Subsequent 2-D geospatial latitude/longitude mapping of net detrended Total Electron Content (dTEC) values from more than stations in South America reveal the drift movement throughout the evening and the spatial structures of EPB over that region, closely aligned with the geomagnetic meridian. Spatial awareness of ionospheric irregularities can be enhanced using 2-D latitude/longitude maps of an affected area \cite{Pradipta2019}.

\subsubsection{Disturbance Storm-Time}
%\label{subsubsec:Dst}

The next relevant parameter is $Disturbance Storm-Time (Dst)$, also known as the geomagnetic activity $Dst$-index, another measure of geomagnetic storm intensity. The $Dst$-index is a geomagnetic indicator of magnetic flux changes derived from measurements taken by a network of ground-based magnetometer stations near the magnetic equator, which continuously monitor $B_{x}$ and $B_{y}$, the horizontal components of Earth's magnetic field \cite{zolesi2014ionospheric}. The $Dst$-index describes ring currents forming above the sub-equatorial region and affecting the ionospheric regions in mid-latitudes.

To calculate the $Dst$-index, variations in $B_{x}$ and $B_{y}$, the horizontal magnetic field, are obtained from multiple stations and averaged. The average is subtracted from a baseline value representing the quiet-time magnetic field. The resulting $Dst$ value in $nT$ measures the intensity of geomagnetic disturbances, with increasingly negative values indicating stronger geomagnetic storms. 

The $Dst$-indice measurements as an hourly average were evaluated and published on a web interface by the NASA Goddard SPDF \cite{Papitashvili2024About, Papitashvili2024Explorer}, and the Geomagnetism and Space Magnetism Data Analysis Center of the Institute of Science, Kyoto University in Japan.

Loewe and Prölss, in 1997 \cite{loewe1997classification} classified magnetic activity $Dst$-indices into five storm classes, as shown in Table~\ref{tab:loewe1997}. Gonzalez et al. \cite{gonzalez1994geomagnetic} used three groups for the same data in 1994, as shown in Table~\ref{tab:gonzalez1994}, similar to Kamide et al. in 1998 \cite{kamide1998two}, Rozhnoi et al. in 2004 \cite{rozhnoi2004middle}, and Contadakis et al. in 2012 \cite{contadakis2012total}.

\begin{table}[!ht]
    \centering
    \caption{Storm classification from Loewe and Prölss, in 1997 \cite{loewe1997classification}.}
    \label{tab:loewe1997}
    \begin{tabular}{lrrrrrcr}
        \hline
        \\
        Storm class & \multicolumn{2}{c}{Fraction} & $Dst_{min}$-Range & $\overline{Dst_{min}}$ & $\overline{a_{p_{max}}}$ & $\overline{K_{p_{max}}}$ & $\overline{A_{E_{max}}}$ \\
        \\
        \hline
        \\
        weak & $482$ & $44\%$ & $-30$ to $-50$ $nT$ & $-36$ $nT$ & $27$ & $4_{o}$ & $542$ $nT$ \\
        moderate & $346$ & $32\%$ & $-50$ to $-100$ $nT$ & $-68$ $nT$ & $48$ & $5_{o}$ & $728$ $nT$ \\
        strong & $206$ & $19\%$ & $-100$ to $-200$ $nT$ & $-131$ $nT$ & $111$ & $7_{-}$ & $849$ $nT$ \\
        severe & $45$ & $4\%$ & $-200$ to $-350$ $nT$ & $-254$ $nT$ & $236$ & $8_{+}$ & $1017$ $nT$ \\
        great & $6$ & $1\%$ & $<-350$ $nT$ & $-427$ $nT$ & $300$ & $9_{-}$ & $1335$ $nT$ \\
        \\
        \hline
        \\
        \multicolumn{8}{l}{The bars above the magnetic indices indicate median values.} \\
    \end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{Storm classification from Gonzales et al., in 1994 \cite{gonzalez1994geomagnetic}.}
    \label{tab:gonzalez1994}
    \begin{tabular}{lrrc}
        \hline
        \\
         & $Dst$, $nT$ & $B_{z}$, $nT$ & $\Delta T$, hours \\
        \\
        \hline
        \\
        Intense & $-100$ & $-10$ & $3$ \\
        Moderate & $-50$ & $-5$ & $2$ \\
        Small & $-30$ & $-3$ & $1$ \\
        (typical substorm) & & & \\
        \\
        \hline
    \end{tabular}
\end{table}

To generate classes for this study, samples were split into $5$ ranges, P (positive, $Dst$ from $15$ to $50$), N (normal, $Dst$ from $-20$ to $15$), R (recovery, $Dst$ from $-55$ to $-20$), T (through, $Dst$ from $-85$ to $-55$), and E (extreme, $Dst$ from $-120$ to $-85$), based on $Dst$ values derived from theoretical knowledge of different storm phases. Table \ref{tab:Dstranges} lists the ranges used in this study.

\begin{table}[!ht]
    \centering
    \caption{$Dst$-based classification rules used in this study.}
    \label{tab:Dstranges}
    \begin{tabular}{|c|c|}
        \hline
        $Dst$ & Storm phase classification \\ \hline
        $15\dots50$ & positive phase (P) \\ \hline
        $-20\dots15$ & normal (N) \\ \hline
        $-55\dots-20$ & recovery phase (R) \\ \hline
        $-85\dots-55$ & through (T) \\ \hline
        $-120\dots-85$ & extreme (E) \\ \hline
    \end{tabular}
\end{table}

\subsubsection{Correlation}
%\label{subsubsec:Correlation}

To verify if variables other than $dTEC$ were truly independent of each other and individually dependent on $dTEC$, which was the assumption used in the hypothesis of this study, the data was examined before training classification models. A Correlation plot formatted as a heat map, box plots, histograms, and scatter plots were generated and studied for different variable ranges and combinations. The minimum, $1^{st}$ quartile, median, arithmetic mean, $3^{rd}$ quartile, and maximum values for all variables are also included in the analysis.

Figure~\ref{fig:correlation} contains a heat map of the correlation between all variables used in this study. Blue represents a low correlation, while red represents a higher correlation. Swapping the row and column leads to the same combination of variables, so the matrix is symmetrical concerning the secondary diagonal. The values on the secondary diagonal equal $1$ and can be disregarded because all variables are fully correlated with themselves. Besides these values on the secondary diagonal, the highest correlation of $0.78$ is between $dTEC$ and $TEC$, as expected based on previously outlined theoretical aspects of these variables. $B_{x}$ and $Dst$ have a correlation coefficient of $0.52$, the second highest value not on the secondary diagonal. This is the only other value larger than $0.5$ not on the secondary diagonal. A high correlation of $B_{x}$ and $Dst$ supports the hypothesis that we can predict $Dst$ values, ranges, or classes using $B_{x}$. The third highest correlation coefficient not on the secondary diagonal is between $B_{x}$ and $B_{y}$ and equals $0.44$. The properties and definitions of these variables explain this correlation.

\begin{figure}
 \centering
 \includegraphics[width=0.9\linewidth]{correlation.pdf}
    \caption{A heat map of the correlation between all variables used in this study, where red represents a higher correlation, while blue represents a low correlation. All variables are fully correlated with themselves, so all values on the secondary diagonal equal $1$. The matrix is symmetrical concerning the secondary diagonal because the same combination of correlated variables is achieved when swapping the row and column.}
    \label{fig:correlation}
\end{figure}

The box plots of all variables for different ranges of $Dst$ values in Figure~\ref{fig:dataset2boxplot}, without restriction on the range of $TEC$, and in Figure~\ref{fig:iono3boxplot}, where the $TEC$ is less than $300$ $nT$, demonstrate that the minimum, maximum, and arithmetic mean of $a_{p}$ decrease for larger $Dst$ values regardless of $TEC$ ranges. The opposite is true for $B_{x}$, as indicated by a high correlation of $B_{x}$ and $Dst$ in Figure~\ref{fig:correlation}. $B_{y}$ exhibits the same trend as $B_{x}$, but it is less prominent. $B_{z}$ is the most stable variable with the smallest changes related to $Dst$ when observing $B_{x}$, $B_{y}$, and $B_{z}$, but an increase for larger $Dst$ values can also be observed. Similar conclusions can be reached from the scatter plots of all variables for different ranges of $Dst$ in Figure~\ref{fig:iono3scatterplot}. 

The scatter plots of all variables for different ranges of $TEC$ values in Figure~\ref{fig:dataset2scatterplot} demonstrate that their distribution is not significantly dependent on $TEC$. This is why the box plots in Figure~\ref{fig:dataset2boxplot} and Figure~\ref{fig:iono3boxplot} that only differ in $TEC$ ranges look very similar. A minimal difference is also evident between the histograms of the relative frequency for all variables in Figure~\ref{fig:allTEC}, without restriction on the range of $TEC$, and in Figure~\ref{fig:300TEC}, where the $TEC$ is less than $300$ $nT$. Table~\ref{tab:minmax} provides the minimum, $1^{st}$ quartile, median, arithmetic mean, $3^{rd}$ quartile, and maximum values for all variables, supporting the conclusions made based on Figure~\ref{fig:dataset2boxplot} and Figure~\ref{fig:allTEC}.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{dataset2boxplot_fix.pdf}
    \caption{Box plots of all variables without restriction on the range of $TEC$ for different ranges of $Dst$ values defining the class label used in this study.}
    \label{fig:dataset2boxplot}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{iono3boxplot_fix.pdf}
    \caption{Box plots of all variables when the $TEC$ is less than $300$ $nT$ for different ranges of $Dst$ values defining the class label used in this study.}
    \label{fig:iono3boxplot}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{iono3scatterplot.pdf}
    \caption{Scatter plots of all variables when the $TEC$ is less than $300$ $nT$ for different ranges of $Dst$ values defining the class label used in this study.}
    \label{fig:iono3scatterplot}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{dataset2scatterplot.pdf}
    \caption{Scatter plots of all variables for different ranges of $TEC$ values without restriction.}
    \label{fig:dataset2scatterplot}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{allTEC_fix.pdf}
    \caption{Histograms of the relative frequency for all variables without restriction on the range of $TEC$.}
    \label{fig:allTEC}
\end{figure}

\begin{figure}
 \centering
 \includegraphics[width=0.9\linewidth]{300TEC_fix.pdf}
    \caption{Histograms of the relative frequency for all variables when the $TEC$ is less than $300$ $nT$.}
    \label{fig:300TEC}
\end{figure}

\begin{table}[!ht]
    \centering
    \caption{The minimum, $1^{st}$ quartile, median, arithmetic mean, $3^{rd}$ quartile, and maximum values for all variables.}
    \label{tab:minmax}
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        & $TEC$ & $dTEC$ & $B_{x}$ & $B_{y}$ & $B_{z}$ \\ \hline
        Min. & $1.00$ & $0.01$ & $35268$ & $1900$ & $-29651$ \\ \hline
        $1^{st}$ Qu. & $7.26$ & $1.66$ & $35400$ & $1988$ & $-29623$ \\ \hline
        Median & $19.62$ & $3.13$ & $35410$ & $2001$ & $-29620$ \\ \hline
        Mean & $40.00$ & $15.88$ & $35412$ & $2001$ & $-29617$ \\ \hline
        $3^{rd}$ Qu. & $34.23$ & $6.49$ & $35423$ & $2015$ & $-29613$ \\ \hline
        Max. & $1288.00$ & $997.00$ & $35527$ & $2092$ & $-29570$ \\ \hline
    \end{tabular}
\end{table}

\subsubsection{Data Preprocessing}
%\label{subsubsec:Preprocessing}

Data preprocessing is recommended to increase classification accuracy \cite{Fan2008}, so this step was also applied to the input data before training the models in this study. There are many ways to standardize data, such as minimum-maximum, normalization by decimal scaling, and Z-score \cite{Mohamad2013}. Subtracting the mean and dividing by the variance for each feature are commonly used for SVM models \cite{Fennell2019} and other models tested in this study, so this approach was chosen. 

The values \textit{scale} and \textit{center} were used in the code for this study in the \textit{preProcess} parameter for the \textit{train} function from the \textit{caret} package developed for \textit{R}. The option \textit{center} subtracts the mean of each feature while \textit{center} divides by the standard deviation. 

\subsubsection{Training and Testing Dataset}
%\label{subsubsec:DataTrainTest}

The samples are divided into training and testing datasets as close as possible to a ratio of $80\%$ for training and $20\%$ for testing. During the division, the share of classes in the original data was taken into account, that is, the division was stratified by class so that an approximately equal ratio of classes was present in both the training data and the testing data, which is a feature of the \textit{createDataPartition} function from the \textit{caret} \textit{R} library that was used \cite{KuhnDataSplitting2024, hyndman2018forecasting, createDataPartition2024}.

\subsection{Performance Metrics}
%\label{subsec:Metrics}

\subsubsection{Confusion Matrix}
%\label{subsubsec:ConfusionMatrix}

The metrics and terminology used to evaluate the classifier performance were taken from the confusion matrix defined in the \textit{R} function \textit{confusionMatrix} in the \textit{caret} library \cite{kuhn2008building, altman1994diagnostic1, altman1994diagnostic2, velez2007balanced, confusionMatrix2024}. The confusion matrix is suitable for use in multiclass classification, which is the goal of this project. A confusion matrix is a table where the number of rows and columns corresponds to the number of classes. The cells report the number of samples in each class classified into any class. This enables a more detailed analysis than by observing the share of correct classifications, i.e. accuracy. Accuracy will give results that can lead us to a wrong conclusion if the data set is unbalanced, that is, if the numbers of observations in different classes vary significantly, which was the case in this paper, because out of a total of $1597$ samples, $1170$ of them belonged to the largest N class, and $3$ to the smallest E class.

If there are only two classes (Yes and No), the metrics are calculated using an example of a confusion matrix in Table~\ref{tab:cm}.

A True Positive (TP) classification result correctly indicates that the sample belongs to the positive class.

A True Negative (TN) classification result correctly indicates that the sample belongs to the negative class.
 
A False Positive (FP) classification result or type I error, incorrectly indicates that the sample belongs to the positive class because it truly belongs to the negative class.

A False Negative (FN) classification result or type II error, incorrectly indicates that the sample belongs to the negative class because it truly belongs to the positive class.

\begin{table}[!ht]
    \centering
    \caption{The confusion matrix for a two-class problem.}
	\label{tab:cm}
	\begin{tabular}{|c|c|c|}
		\hline
		 & \multicolumn{2}{|c|}{Reference} \\ \hline
        Prediction & Yes & No \\ \hline
        Yes &TP & FP \\ \hline
        No & FN & TN \\ \hline
	\end{tabular}
\end{table}

Sensitivity, recall, hit rate, or True Positive Rate (TPR), is calculated as $\mathrm{TP}/(\mathrm{TP}+\mathrm{FN})$, that is, the proportion of samples that are correctly classified as positive among all samples that truly belong to the positive class. Higher TPR values indicate better results for the positive class.

Specificity, selectivity, or True Negative Rate (TNR), is calculated using the expression $\mathrm{TN}/(\mathrm{TN}+\mathrm{FP})$, that is, the proportion of samples that are correctly classified as negative among all samples that truly belong to the negative class. Higher TNR values indicate better results for the negative class.

Prevalence is calculated as $(\mathrm{TP}+\mathrm{FN})/(\mathrm{TP}+\mathrm{FP}+\mathrm{FN}+\mathrm{TN})$ for the positive class, that is, the proportion of samples that truly belong to the positive class among all samples. Prevalence values other than an even split by the number of classes indicate a class imbalance.

Precision, or Positive Predictive Value (PPV), is calculated as $\mathrm{TP}/(\mathrm{TP}+\mathrm{FP})$ for a two-class problem, i.e. the proportion of truly positive results among all samples classified as positive. Higher PPV values indicate better results for the positive class.

The Negative Predictive Value (NPV) is calculated as $\mathrm{TN}/(\mathrm{TN}+\mathrm{FN})$ for a two-class problem, i.e. the share of truly negative results among all samples that are classified as negative. Higher NPV values indicate better results for the negative class.

The Detection Rate (DR) is calculated as $\mathrm{TP}/(\mathrm{TP}+\mathrm{FP}+\mathrm{FN}+\mathrm{TN})$ for the positive class, that is, that is, the proportion of samples that are correctly classified as positive among all samples.

The Detection Prevalence (DP) is calculated as $(\mathrm{TP}+\mathrm{FP})/(\mathrm{TP}+\mathrm{FP}+\mathrm{FN}+\mathrm{TN})$ for the positive class, that is, that is, the proportion of samples that are correctly or incorrectly classified as positive among all samples.

The accuracy (Acc) is calculated as $(\mathrm{TP} + \mathrm{TN}) / (\mathrm{TP} + \mathrm{TN} + \mathrm{FP} + \mathrm{FN})$, that is, the proportion of samples that are correctly classified among all samples.

Since accuracy is not usable in the case of unbalanced classes, Balanced Accuracy (BA) is introduced, which is calculated as $(\mathrm{TPR} + \mathrm{TNR}) / 2$ and is arithmetic mean TPR and TNR, which are respectively centered on the positive and the negative class separately.

Another metric that can be used instead of accuracy to consider each class separately is the $F1$ score, i.e. the harmonic mean of PPV and TPR, equal to $2 \times (\mathrm{PPV} \times \mathrm{TPR}) / (\mathrm{PPV} + \mathrm{TPR}) = 2 \times \mathrm{TP} / (2 \times \mathrm{TP} + \mathrm{FP} + \mathrm{FN})$.

If there are more than two classes, results are calculated for each class separately by viewing that class as the positive class and samples of all other classes as negative, a "one versus all" approach.

The unweighted Kappa statistic and a \textit{p}-value from McNemar's test are also computed. McNemar's test produces NA values with sparse tables, such as the one in this experiment, so it was not included in the results.

A $95\%$ confidence interval for the accuracy rate is computed using \textit{binom.test} and a one-sided test \cite{binom.test2024, clopper1934use, conover1999practical, hollander2013nonparametric} to see if the accuracy is significantly larger than the No Information Rate (NIR) which is the percentage of the largest class.

\subsubsection{The Binomial Test}
%\label{subsubsec:Binomial}

The binomial test \cite{howell1992statistical, graphpadGraphPadPrism} for two classes is an exact test of the statistical significance of deviations from a theoretically expected distribution.

The binomial test is often utilized to validate a hypothesis about the probability ($\pi$) of success $H_{0}\colon \pi =\pi_{0}$, where users define $\pi_{0}$ between $0$ and $1$. The expected number of successes would be $n\pi_{0}$ if the null hypothesis $H_{0}$ were correct. If $k$ out of $n$ attempts are successful, the binomial distribution formula defines the probability in Equation~16.

\begin{equation}
	\Pr(X=k)={\binom{n}{k}}p^{k}(1-p)^{n-k}
	\quad\left(16\right)
\end{equation}

The probability of an outcome equally or more extreme than actual occurrences is used to calculate the $p$-value. This is a simple operation for a one-tailed test. 

Equation~17 contains the expression for the $p$-value when testing the hypothesis that $\pi <\pi_{0}$. We can use a range from $k$ to $n$ to test $\pi >\pi_{0}$.

\begin{equation}
	p=\sum_{i=0}^{k}\Pr(X=i)=\sum_{i=0}^{k}{\binom{n}{i}}\pi_{0}^{i}(1-\pi_{0})^{n-i}
	\quad\left(17\right)
\end{equation}

A binomial distribution isn't symmetric if $\pi_{0}\neq 0.5$, so calculating a $p$-value for a two-tailed test is not as simple. The $p$-value from the one-tailed test cannot be doubled to obtain the result since all events must be considered, including ones more extreme than previously seen. These extreme events are equally or less probable than $X=k$. Equation~18 denotes all such events. Equation~19 is used to compute the two-tailed $p$-value.

\begin{equation}
	{\mathcal{I}}=\{i\colon \Pr(X=i)\leq \Pr(X=k)\}
	\quad\left(18\right)
\end{equation}

\begin{equation}
	p=\sum_{i\in {\mathcal{I}}}\Pr(X=i)=\sum_{i\in {\mathcal{I}}}{\binom{n}{i}}\pi_{0}^{i}(1-\pi_{0})^{n-i}
	\quad\left(19\right)
\end{equation}

\subsubsection{Cohen's Kappa Coefficient}
%\label{subsubsec:kappa}

Cohen's kappa coefficient ($\kappa$, lowercase Greek kappa) indicates inter- and intra-rater reliability for classification \cite{mchugh2012interrater}. As it accounts for random matches in ratings, it is a better metric than simply computing the percentage of agreement. Interpreting indices of agreement is not straightforward, and is still a point of contention among experts. Using disagreement instead of agreement in ratings might be simpler \cite{pontius2011death}. Galton was the first to use a statistic similar to Cohen's kappa in 1892 \cite{galton1892finger, smeeton1985early}. Jacob Cohen formally described the kappa coefficient in 1960 in the journal "Educational and Psychological Measurement" \cite{cohen1960coefficient}.

Two raters group $N$ items into $C$ mutually exclusive classes, and Cohen's kappa measures their agreement. Equation~20 defines $\kappa$, where $p_{o}$ is the relative observed agreement among raters, and $p_{e}$ is the hypothetical probability of chance agreement. The observed data influences the computed probabilities of each observer randomly selecting each category.

\begin{equation}
	\kappa \equiv {\frac {p_{o}-p_{e}}{1-p_{e}}}=1-{\frac {1-p_{o}}{1-p_{e}}}
	\quad\left(20\right)
\end{equation}

If the raters completely align then $\kappa = 1$. When there is no agreement other than what would be expected by chance and is given by $p_{e}$, $\kappa = 0$. If $\kappa$ statistic is negative \cite{sim2005kappa} there is no match between the raters, which can reflect a real tendency of the raters to give differing ratings.

Equation~21 is derived from the construction in Equation~22 and holds for $k$ categories with $N$ observations to categorize if $n_{k1}$ is the number of times rater $i$ predicted category $k$. In these equations, ${\widehat{p_{k12}}}$ is the estimated probability that both rater $1$ and rater $2$ will classify the same item as $k$, and ${\widehat {p_{k1}}}$ is the estimated probability that rater $1$ will classify an item as $k$. A similar claim can be made for rater $2$.

\begin{equation}
	p_{e}={\frac {1}{N^{2}}}\sum_{k}n_{k1}n_{k2}
	\quad\left(21\right)
\end{equation}

\begin{equation}
	p_{e}=\sum_{k}{\widehat {p_{k12}}}{\overset{\text{ind.}}{=}}\sum_{k}{\widehat {p_{k1}}}{\widehat {p_{k2}}}=\sum_{k}{\frac {n_{k1}}{N}}{\frac {n_{k2}}{N}}={\frac {1}{N^{2}}}\sum_{k}n_{k1}n_{k2}
	\quad\left(22\right)
\end{equation}

If ratings are independent Equation~23, can be used.

\begin{equation}
	\textstyle{\widehat{p_{k}}}=\sum_{k}{\widehat{p_{k1}}}{\widehat {p_{k2}}}
	\quad\left(23\right)
\end{equation}

Equation~24 is used to estimate the term $\widehat{p_{k1}}$ by using the number of items classified as $k$ by rater $1$ ($n_{k1}$) divided by the total items to classify ($N$).

\begin{equation}
	\widehat {p_{k1}}=\frac{n_{k1}}{N}
	\quad\left(24\right)
\end{equation}
 (and similarly for rater $2$).
 
Equation~25 contains the Cohen's Kappa formula \cite{chicco2021matthews} for the traditional confusion matrix for binary classifications in Table~\ref{tab:cm}.

\begin{equation}
	\kappa ={\frac{2\times (TP\times TN-FN\times FP)}{(TP+FP)\times (FP+TN)+(TP+FN)\times (FN+TN)}}
	\quad\left(25\right)
\end{equation}

In this scenario, Cohen's Kappa equals the Heidke skill score introduced by Myrick Haskell Doolittle in 1888 \cite{heidke1926berechnung, philosophical1887bulletin}.

\section{Research Results}
\label{sec:Results}

The accuracy, CI, NIR, $p$-value, and Kappa statistic for each model when using all variables (all), all variables except $Dst$ (no $Dst$), all variables except $Dst$, $TEC$, and d$TEC$ (no $TEC$), only $B_{x}$, $B_{y}$, and $B_{z}$ (coord), only $B_{x}$, $B_{y}$, and $a_{p}$ ($x$ $y$ $a_{p}$), only $B_{x}$, $B_{z}$, and $a_{p}$ ($x$ $z$ $a_{p}$), or only $B_{y}$, $B_{z}$, and $a_{p}$ ($y$ $z$ $a_{p}$) is displayed in Table~\ref{tab:stats:reverse:all}, Table~\ref{tab:stats:reverse:no_DisturbanceStormTime}, Table~\ref{tab:stats:reverse:no_TotalElectronContent}, Table~\ref{tab:stats:reverse:coord}, Table~\ref{tab:stats:reverse:xyap}, Table~\ref{tab:stats:reverse:xzap}, and Table~\ref{tab:stats:reverse:yzap}.

\begin{table}[!ht]
    \centering
    \caption{The accuracy, CI, NIR, $p$-value, and Kappa statistic for each model when using all variables as input.}
	\label{tab:stats:reverse:all}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Model & Acc & $95\%$ CI & NIR & $p$-value & Kappa \\ \hline
		SVM Poly & $0.7672$ & $(0.7457, 0.7877)$ & $0.7322$ & $0.0007444$ & $0.3607$ \\ \hline
		C5.0 DT & $1$ & $(0.9977, 1)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $1$ \\ \hline
		NB & $0.9912$ & $(0.9853, 0.9952)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $0.9787$ \\ \hline
		NN & $1$ & $(0.9977, 1)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $1$ \\ \hline
		PLS & $0.9086$ & $(0.8934, 0.9223)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $0.7489$ \\ \hline
		FDA & $0.9199$ & $(0.9055, 0.9327)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $0.8017$ \\ \hline
		PCA NN & $0.9975$ & $(0.9936, 0.9993)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $0.9939$ \\ \hline
	\end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{The accuracy, CI, NIR, $p$-value, and Kappa statistic for each model when using all variables except $Dst$ as input.}
	\label{tab:stats:reverse:no_DisturbanceStormTime}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Model & Acc & $95\%$ CI & NIR & $p$-value & Kappa \\ \hline
		SVM Poly & $0.7284$ & $(0.7059, 0.7501)$ & $0.7322$ & $0.6447$ & $0.1137$ \\ \hline
		C5.0 DT & $0.8492$ & $(0.8307, 0.8664)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $0.6165$ \\ \hline
		NB & $0.9906$ & $(0.9846, 0.9947)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $0.9771$ \\ \hline
		NN & $1$ & $(0.9977, 1)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $1$ \\ \hline
		PLS & $0.7929$ & $(0.7722, 0.8125)$ & $0.7322$ & $1.104e-08$ & $0.3635$ \\ \hline
		FDA & $0.8141$ & $(0.7942, 0.8329)$ & $0.7322$ & $9.859e-15$ & $0.5064$ \\ \hline
		PCA NN & $0.8436$ & $(0.8248, 0.861)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $0.5868$ \\ \hline
	\end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{The accuracy, CI, NIR, $p$-value, and Kappa statistic for each model when using all variables except $Dst$, $TEC$, and d$TEC$ as input.}
	\label{tab:stats:reverse:no_TotalElectronContent}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Model & Acc & $95\%$ CI & NIR & $p$-value & Kappa \\ \hline
		SVM Poly & $0.7378$ & $(0.7155, 0.7592)$ & $0.7322$ & $0.3168$ & $0.1066$ \\ \hline
		C5.0 DT & $0.8448$ & $(0.8261, 0.8622)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $0.5882$ \\ \hline
		NB & $1$ & $(0.9977, 1)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $1$ \\ \hline
		NN & $1$ & $(0.9977, 1)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $1$ \\ \hline
		PLS & $0.7922$ & $(0.7715, 0.8119)$ & $0.7322$ & $1.556e-08$ & $0.3634$ \\ \hline
		FDA & $0.8166$ & $(0.7968, 0.8353)$ & $0.7322$ & $1.416e-15$ & $0.5092$ \\ \hline
		PCA NN & $0.8379$ & $(0.8189, 0.8557)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $0.5647$ \\ \hline
	\end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{The accuracy, CI, NIR, $p$-value, and Kappa statistic for each model when using only $B_{x}$, $B_{y}$, and $B_{z}$ as input.}
	\label{tab:stats:reverse:coord}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Model & Acc & $95\%$ CI & NIR & $p$-value & Kappa \\ \hline
		SVM Poly & $0.7409$ & $(0.7187, 0.7623)$ & $0.7322$ & $0.2234$ & $0.0887$ \\ \hline
		C5.0 DT & $0.826$ & $(0.8065, 0.8443)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $0.5208$ \\ \hline
		NB & $1$ & $(0.9977, 1)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $1$ \\ \hline
		NN & $1$ & $(0.9977, 1)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $1$ \\ \hline
		PLS & $0.7835$ & $(0.7625, 0.8035)$ & $0.7322$ & $1.284e-06$ & $0.298$ \\ \hline
		FDA & $0.7979$ & $(0.7773, 0.8173)$ & $0.7322$ & $6.224e-10$ & $0.4541$ \\ \hline
		PCA NN & $0.8254$ & $(0.8059, 0.8437)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $0.5242$ \\ \hline
	\end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{The accuracy, CI, NIR, $p$-value, and Kappa statistic for each model when using only $B_{x}$, $B_{y}$, and $a_{p}$ as input.}
	\label{tab:stats:reverse:xyap}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Model & Acc & $95\%$ CI & NIR & $p$-value & Kappa \\ \hline
		SVM Poly & $0.7416$ & $(0.7193, 0.7629)$ & $0.7322$ & $0.2068$ & $0.103$ \\ \hline
		C5.0 DT & $0.8292$ & $(0.8098, 0.8473)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $0.5386$ \\ \hline
		NB & $1$ & $(0.9977, 1)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $1$ \\ \hline
		NN & $1$ & $(0.9977, 1)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $1$ \\ \hline
		PLS & $0.781$ & $(0.7599, 0.801)$ & $0.7322$ & $3.978e-06$ & $0.3153$ \\ \hline
		FDA & $0.8204$ & $(0.8007, 0.8389)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $0.5099$ \\ \hline
		PCA NN & $0.8367$ & $(0.8176, 0.8545)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $0.5579$ \\ \hline
	\end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{The accuracy, CI, NIR, $p$-value, and Kappa statistic for each model when using only $B_{x}$, $B_{z}$, and $a_{p}$ as input.}
	\label{tab:stats:reverse:xzap}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Model & Acc & $95\%$ CI & NIR & $p$-value & Kappa \\ \hline
		SVM Poly & $0.739$ & $(0.7168, 0.7604)$ & $0.7322$ & $0.2775$ & $0.1013$ \\ \hline
		C5.0 DT & $0.8104$ & $(0.7903, 0.8293)$ & $0.7322$ & $1.602e-13$ & $0.4714$ \\ \hline
		NB & $1$ & $(0.9977, 1)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $1$ \\ \hline
		NN & $1$ & $(0.9977, 1)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $1$ \\ \hline
		PLS & $0.7735$ & $(0.7521, 0.7938)$ & $0.7322$ & $8.468e-05$ & $0.2742$ \\ \hline
		FDA & $0.7979$ & $(0.7773, 0.8173)$ & $0.7322$ & $6.224e-10$ & $0.4343$ \\ \hline
		PCA NN & $0.8148$ & $(0.7948, 0.8335)$ & $0.7322$ & $6.107e-15$ & $0.4948$ \\ \hline
	\end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{The accuracy, CI, NIR, $p$-value, and Kappa statistic for each model when using only $B_{y}$, $B_{z}$, and $a_{p}$ as input.}
	\label{tab:stats:reverse:yzap}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Model & Acc & $95\%$ CI & NIR & $p$-value & Kappa \\ \hline
		SVM Poly & $0.7347$ & $(0.7123, 0.7562)$ & $0.7322$ & $0.4233$ & $0.0251$ \\ \hline
		C5.0 DT & $0.7866$ & $(0.7657, 0.8065)$ & $0.7322$ & $2.88e-07$ & $0.3926$ \\ \hline
		NB & $0.9987$ & $(0.9955, 0.9998)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $0.9969$ \\ \hline
		NN & $1$ & $(0.9977, 1)$ & $0.7322$ & $< 2.2 \times {10}^{-16}$ & $1$ \\ \hline
		PLS & $0.7566$ & $(0.7348, 0.7774)$ & $0.7322$ & $0.01419$ & $0.2075$ \\ \hline
		FDA & $0.7672$ & $(0.7457, 0.7877)$ & $0.7322$ & $0.0007444$ & $0.3367$ \\ \hline
		PCA NN & $0.7797$ & $(0.7586, 0.7998)$ & $0.7322$ & $6.857e-06$ & $0.3878$ \\ \hline
	\end{tabular}
\end{table}

The execution time in $s$ is calculated using the \textit{R} \textit{system.time} function \cite{system.time2024} for each model when using different combinations of variables as input is displayed in Table~\ref{tab:time:total}. The experiment was run on \textit{Windows} 11 using \textit{R Studio} version 2024.04.2+764 and \textit{R} version 4.4.1, the AMD Radeon RX 6600 Graphics Processing Unit (GPU), $16$ GB of Random Access Memory (RAM), and the AMD Ryzen 5 PRO 4650G Central Processing Unit (CPU) with $6$ cores.

\begin{table}[!ht]
    \centering
    \caption{The execution time in seconds for each model when using different combinations of variables as input.}
	\label{tab:time:total}
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline
		Model & SVM Poly & C5.0 DT & NB & NN & PLS & FDA & pcaNNet \\ \hline
		all & $103.52$ & $169.28$ & $192.05$ & $494.53$ & $49.99$ & $51.57$ & $291.98$ \\ \hline
		no $Dst$ & $276.24$ & $592.32$ & $175.38$ & $479.67$ & $42.89$ & $49.34$ & $305.16$ \\ \hline
		no $TEC$ & $285.58$ & $393.72$ & $137.99$ & $453.82$ & $33.11$ & $60.53$ & $294.58$ \\ \hline
		coord & $331.13$ & $277.63$ & $118.86$ & $441.37$ & $22.28$ & $67.94$ & $294.97$ \\ \hline
		$x$ $y$ $a_{p}$ & $319.97$ & $295.20$ & $121.73$ & $465.61$ & $26.72$ & $70.97$ & $302.48$ \\ \hline
		$x$ $z$ $a_{p}$ & $348.47$ & $259.81$ & $121.05$ & $475.05$ & $25.56$ & $76.04$ & $321.05$ \\ \hline
		$y$ $z$ $a_{p}$ & $416.75$ & $277.62$ & $124.02$ & $489.08$ & $29.89$ & $95.64$ & $345.05$ \\ \hline
	\end{tabular}
\end{table}

\subsection{Comprehensive Results for the Naive Bayes Model}
%\label{subsec:ResultsNB}

The confusion matrix and the performance indicators derived from it when using the Naive Bayes (NB) model and all input variables are depicted in Table~\ref{tab:cm:all:nb} and Table~\ref{tab:cs:reverse:all:nb}. The same values when using all input variables except $Dst$ are marked in Table~\ref{tab:cm:no_DisturbanceStormTime:nb} and Table~\ref{tab:cs:reverse:no_DisturbanceStormTime:nb}, and Table~\ref{tab:cm:yzap:nb} and Table~\ref{tab:cs:reverse:yzap:nb} when using only $B_{y}$, $B_{z}$, and $a_{p}$ variables as input. These results are displayed since the NB model has low computational load and high accuracy. The combinations of input variables that produced a $100\%$ accuracy when used for the NB model were not included since there are no errors in classification.

\begin{table}[!ht]
    \centering
    \caption{The confusion matrix for the Naive Bayes model when using all variables as input.}
	\label{tab:cm:all:nb}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		 & \multicolumn{5}{|c|}{Reference} \\ \hline
		 Prediction & E & N & P & R & T \\ \hline
		 E & $3$ & $1$ & $0$ & $0$ & $0$ \\ \hline
		 N & $0$ & $1161$ & $1$ & $1$ & $0$ \\ \hline
		 P & $0$ & $5$ & $26$ & $2$ & $0$ \\ \hline
		 R & $0$ & $3$ & $0$ & $375$ & $0$ \\ \hline
		 T & $0$ & $0$ & $0$ & $1$ & $19$ \\ \hline
	\end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{The performance indicators derived from the confusion matrix for the Naive Bayes model when using all variables as input.}
	\label{tab:cs:reverse:all:nb}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		 & \multicolumn{5}{c|}{Class} \\ \hline
		Statistics & E & N & P & R & T \\ \hline
		Sensitivity & $100\%$ & $99.23\%$ & $96.296\%$ & $98.94\%$ & $100\%$ \\ \hline
		Specificity & $99.9373\%$ & $99.53\%$ & $99.554\%$ & $99.75\%$ & $99.937\%$ \\ \hline
		PPV & $75\%$ & $99.83\%$ & $78.788\%$ & $99.21\%$ & $95\%$ \\ \hline
		NPV & $100\%$ & $97.93\%$ & $99.936\%$ & $99.67\%$ & $100\%$ \\ \hline
		DR & $0.1877\%$ & $72.65\%$ & $1.627\%$ & $23.47\%$ & $1.189\%$ \\ \hline
		DP & $0.2503\%$ & $72.78\%$ & $2.065\%$ & $23.65\%$ & $1.252\%$ \\ \hline
		BA & $99.9687\%$ & $99.38\%$ & $97.925\%$ & $99.35\%$ & $99.968\%$ \\ \hline
	\end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{The confusion matrix for the Naive Bayes model when using all variables except $Dst$ as input.}
	\label{tab:cm:no_DisturbanceStormTime:nb}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		 & \multicolumn{5}{|c|}{Reference} \\ \hline
		 Prediction & E & N & P & R & T \\ \hline
		 E & $3$ & $1$ & $0$ & $0$ & $0$ \\ \hline
		 N & $0$ & $1161$ & $2$ & $2$ & $0$ \\ \hline
		 P & $0$ & $5$ & $25$ & $1$ & $0$ \\ \hline
		 R & $0$ & $3$ & $0$ & $375$ & $0$ \\ \hline
		 T & $0$ & $0$ & $0$ & $1$ & $19$ \\ \hline
	\end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{The performance indicators derived from the confusion matrix for the Naive Bayes model when using all variables except $Dst$ as input.}
	\label{tab:cs:reverse:no_DisturbanceStormTime:nb}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		 & \multicolumn{5}{c|}{Class} \\ \hline
		Statistics & E & N & P & R & T \\ \hline
		Sensitivity & $100\%$ & $99.23\%$ & $92.593\%$ & $98.94\%$ & $100\%$ \\ \hline
		Specificity & $99.9373\%$ & $99.07\%$ & $99.618\%$ & $99.75\%$ & $99.937\%$ \\ \hline
		PPV & $75\%$ & $99.66\%$ & $80.645\%$ & $99.21\%$ & $95\%$ \\ \hline
		NPV & $100\%$ & $97.92\%$ & $99.872\%$ & $99.67\%$ & $100\%$ \\ \hline
		DR & $0.1877\%$ & $72.65\%$ & $1.564\%$ & $23.47\%$ & $1.189\%$ \\ \hline
		DP & $0.2503\%$ & $72.9\%$ & $1.94\%$ & $23.65\%$ & $1.252\%$ \\ \hline
		BA & $99.9687\%$ & $99.15\%$ & $96.105\%$ & $99.35\%$ & $99.968\%$ \\ \hline
	\end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{The confusion matrix for the Naive Bayes model when using only $B_{y}$, $B_{z}$, and $a_{p}$ as input.}
	\label{tab:cm:yzap:nb}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		 & \multicolumn{5}{|c|}{Reference} \\ \hline
		 Prediction & E & N & P & R & T \\ \hline
		 E & $2$ & $0$ & $0$ & $0$ & $0$ \\ \hline
		 N & $1$ & $1170$ & $1$ & $0$ & $0$ \\ \hline
		 P & $0$ & $0$ & $26$ & $0$ & $0$ \\ \hline
		 R & $0$ & $0$ & $0$ & $379$ & $0$ \\ \hline
		 T & $0$ & $0$ & $0$ & $0$ & $19$ \\ \hline
	\end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{The performance indicators derived from the confusion matrix for the Naive Bayes model when using only $B_{y}$, $B_{z}$, and $a_{p}$ as input.}
	\label{tab:cs:reverse:yzap:nb}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		 & \multicolumn{5}{c|}{Class} \\ \hline
		Statistics & E & N & P & R & T \\ \hline
		Sensitivity & $66.6667\%$ & $100\%$ & $96.296\%$ & $100\%$ & $100\%$ \\ \hline
		Specificity & $100\%$ & $99.53\%$ & $100\%$ & $100\%$ & $100\%$ \\ \hline
		PPV & $100\%$ & $99.83\%$ & $100\%$ & $100\%$ & $100\%$ \\ \hline
		NPV & $99.9373\%$ & $100\%$ & $99.936\%$ & $100\%$ & $100\%$ \\ \hline
		DR & $0.1252\%$ & $73.22\%$ & $1.627\%$ & $23.72\%$ & $1.189\%$ \\ \hline
		DP & $0.1252\%$ & $73.34\%$ & $1.627\%$ & $23.72\%$ & $1.189\%$ \\ \hline
		BA & $83.3333\%$ & $99.77\%$ & $98.148\%$ & $100\%$ & $100\%$ \\ \hline
	\end{tabular}
\end{table}

\section{Discussion}
\label{sec:Discussion}

The Neural Network (NN) model has the highest execution time for any subset of the input variables due to its complexity and extensive training, evident from the data in Table~\ref{tab:time:total}. However, in each case, it achieved a $100\%$ accuracy, which is shown in Table~\ref{tab:stats:reverse:all}, Table~\ref{tab:stats:reverse:no_DisturbanceStormTime}, Table~\ref{tab:stats:reverse:no_TotalElectronContent}, Table~\ref{tab:stats:reverse:coord}, Table~\ref{tab:stats:reverse:xyap}, Table~\ref{tab:stats:reverse:xzap}, and Table~\ref{tab:stats:reverse:yzap}. The Naive Bayes (NB) model has the second highest accuracy, over $99\%$. The NB model achieved a $100\%$ accuracy, except when using all input variables, all input variables except Disturbance Storm-Time (Dst), or only $B_{y}$, $B_{z}$, and $a_{p}$. The difference in accuracy between the NN and the NB model is negligible, and the training time for the NN model is more than twice as long. Despite their simple design and seemingly oversimplified assumptions, NB classifiers have performed quite well in many complex real-world situations. In 2004, an analysis of the Bayesian classification problem showed that there are reasonable theoretical reasons for the seemingly incredible performance of NB classifiers \cite{Zhang2004}.

All other models fail to achieve an accuracy over $90\%$, except when using all input variables, including $Dst$. The $Dst$ class was derived by thresholding the continuous $Dst$ value and discretizing it by converting it to a single character, so the original $Dst$ value should not be used as model input. This labeling method explains the increase in accuracy when adding the $Dst$ input variable in all except the two best-performing models. For example, the C5.0 DT model did not consider any variable except $Dst$, indicated by a $100\%$ variable importance. The Support Vector Machine (SVM) model with a Polynomial Kernel is consistently the worst-performing for any subset of input variables, never achieving an accuracy over $80\%$. None of the models achieved an accuracy under $70\%$, so they were all moderately successful.

When studying the performance of the NB model by individual class in Table~\ref{tab:cm:all:nb}, Table~\ref{tab:cs:reverse:all:nb}, Table~\ref{tab:cm:no_DisturbanceStormTime:nb}, Table~\ref{tab:cs:reverse:no_DisturbanceStormTime:nb}, Table~\ref{tab:cm:yzap:nb} and Table~\ref{tab:cs:reverse:yzap:nb}, samples of the T class are all correctly classified, but the testing is less extensive since it is the second smallest class. Samples of the R class are sometimes erroneously assigned to the N, P, or less commonly the T class. Samples of the P class are rarely mistaken for the N class. Two of the most common misclassifications were assigning samples of the N class with the highest detection rate and prevalence to the P or R classes with the third and second highest detection rate and prevalence. This is due to the largest number of samples in the N class between the P and R classes with the narrowest $Dst$ range. Samples of the N class are least often included in the E class, whose range is the furthest apart from the N class. A sample of the E class was attributed to the N class only once. The E class is the smallest with only three samples, which must be accounted for when interpreting these results, such as the lowest balanced accuracy of $83.33\%$ and sensitivity of $66.67\%$ when using only $B_{y}$, $B_{z}$, and $a_{p}$ as input, or the lowest positive predictive value of $75\%$ when using all variables as input.

\section{Conclusion}
\label{sec:Conclusion}

The presented study aims to classify ambient conditions of space weather events for sub-equatorial regions. Global Navigation Satellite System (GNSS) Positioning, Navigation, and Timing (PNT) performance is significantly affected by such events. it would be beneficial to indicate to users that a geomagnetic/ionospheric storm is in progress. 

Classification models using machine learning were applied to descriptions of the geomagnetic field expressed in Total Electron Content (TEC) and other input data. It was assumed observations contained independent variables to generate the dependent variable representing the Disturbance Storm-Time (Dst) class. 

Statistical analysis confirmed that other variables change distribution based on $Dst$, not $TEC$. Continuous $Dst$ values in different ranges were converted into discrete classes based on statistics, previous theories, and research. 

An Support Vector Machine (SVM) with a Polynomial Kernel, C5.0 Decision Tree (DT), Naive Bayes (NB), Neural Network (NN), Partial Least Squares (PLS), Flexible Discriminant Analysis (FDA), and Principal Component Analysis (PCA) NN model created a $Dst$-based classification from multiple combinations of input variables.

The NB model achieved perfect accuracy for most tested combinations of input variables, and over $99\%$ in all cases. It is slightly less successful than the NN model, and the total execution time is at least two times shorter, making it more suitable for use in compact, low-performance, and low-cost portable devices such as smartphones.

\clearpage

\section{Acronyms}
\label{sec:Acronyms}

\textit{(AA)2} Ambient-Aware Application-Aligned


\\[2\baselineskip]

\textit{ANN} Artificial Neural Network


\\[2\baselineskip]

\textit{BA} Balanced Accuracy


\\[2\baselineskip]

\textit{CCA} Canonical Correlation Analysis

\textit{CI} Confidence Interval

\textit{CNN} Convolutional Neural Network

\textit{CPU} Central Processing Unit


\\[2\baselineskip]

\textit{DBSCAN} Density-Based Spatial Clustering

\textit{DP} Detection Prevalence

\textit{DR} Detection Rate

\textit{DT} Decision Tree

\textit{Dst} Disturbance Storm-Time


\\[2\baselineskip]

\textit{EOF} Empirical Orthogonal Functions

\textit{EPB} Equatorial Plasma Bubbles

\textit{EUV} Extreme ultraviolet

\textit{EVD} Eigenvalue Decomposition


\\[2\baselineskip]

\textit{FDA} Flexible Discriminant Analysis

\textit{FN} False Negative

\textit{FP} False Positive


\\[2\baselineskip]

\textit{GEC} Global Electric Current

\textit{GFZ} German Research Centre for Geosciences

\textit{GNSS} Global Navigation Satellite System

\textit{GPS} Global Positioning System

\textit{GPU} Graphics Processing Unit


\\[2\baselineskip]

\textit{KLT} Karhunen–Loève Theorem


\\[2\baselineskip]

\textit{LDA} Linear Discriminant Analysis


\\[2\baselineskip]

\textit{MANOVA} Multivariate Analysis Of Variance

\textit{MARS} Multivariate Adaptive Regression Splines

\textit{ML} machine learning


\\[2\baselineskip]

\textit{NAS} Neural Architecture Search

\textit{NASA} National Aeronautics and Space Administration

\textit{NB} Naive Bayes

\textit{NDA} Normal Discriminant Analysis

\textit{NIR} No Information Rate

\textit{NN} Neural Network

\textit{NOAA} National Oceanic and Atmospheric Administration

\textit{NPV} Negative Predictive Value

\textit{NRCAN} Natural Resources Canada


\\[2\baselineskip]

\textit{PCA} Principal Component Analysis

\textit{PLS} Partial Least Squares

\textit{PLS-DA} Partial Least Squares Discriminant Analysis

\textit{PNT} Positioning, Navigation, and Timing

\textit{POD} Proper Orthogonal Decomposition

\textit{PPV} Positive Predictive Value


\\[2\baselineskip]

\textit{RAM} Random Access Memory

\textit{RF} Random Forest


\\[2\baselineskip]

\textit{SAR} Synthetic Aperture Radar

\textit{SPDF} Space Physics Data Facility

\textit{SVC} Support Vector Clustering

\textit{SVD} Singular Value Decomposition

\textit{SVM} Support Vector Machine

\textit{SVN} Support Vector Network

\textit{SWPC} Space Weather Prediction Center


\\[2\baselineskip]

\textit{TDIDT} Top-Down Induction of Decision Trees

\textit{TEC} Total Electron Content

\textit{TID} Traveling Ionospheric Disturbance

\textit{TN} True Negative

\textit{TNR} True Negative Rate

\textit{TP} True Positive

\textit{TPR} True Positive Rate


\\[2\baselineskip]

\textit{dTEC} detrended Total Electron Content



\section{Declarations}
%\label{sec:Declarations}

\subsection{Availability of data and materials}
%\label{subsec:Availability}

The datasets used during the current study are available from the corresponding author upon reasonable request.

\subsection{Competing interests}
%\label{subsec:Competing}

The authors declare no conflict of interest.

\subsection{Funding}
%\label{subsec:Funding}

This work was fully supported by

\subsection{Authors' contributions}
%\label{subsec:Contributions}

\textbf{Lucija Žužić:} Conceptualization, Methodology, Software, Validation, Investigation, Data Curation, Writing -- Original Draft, Visualization.

\textbf{Teodor B. Iliev:} Conceptualization, Methodology, Validation, Formal analysis, Investigation, Resources, Writing -- Original Draft, Writing -- Review \& Editing, Supervision, Project administration, Funding acquisition.

\textbf{Renato Filjar:} Conceptualization, Methodology, Validation, Formal analysis, Investigation, Resources, Writing -- Original Draft, Writing -- Review \& Editing, Supervision, Project administration, Funding acquisition.

\subsection{Acknowledgements}
%\label{subsec:Acknowledgements}

This work was fully supported by

\subsection{Authors' information}
%\label{subsec:Information}

Lucija Žužić is a doctoral student at the Faculty of Engineering, University of Rijeka, currently employed as an assistant at the Department of Computer Engineering. Her research interests include applied machine learning in biology, chemistry, medicine, and transportation. Maritime transportation is also the subject of her master's thesis.

Teodor B. Iliev

Renato Filjar

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\begin{thebibliography}{000}
\bibitem{filjar2022application}Filjar, R. An application-centred resilient GNSS position estimation algorithm based on positioning environment conditions awareness. {\em Proceedings Of The 2022 International Technical Meeting Of The Institute Of Navigation}. pp. 1123-1136 (2022)
\bibitem{filjar2024ambient}Filjar, R., Hedji, I., Prpić-Oršić, J. \& Iliev, T. An Ambient Adaptive Global Navigation Satellite System Total Electron Content Predictive Model for Short-Term Rapid Geomagnetic Storm Events. {\em Remote Sensing}. \textbf{16}, 3051 (2024)
\bibitem{davies1990ionospheric}Davies, K. Ionospheric Radio. (Institution of Engineering & Technology,1990), https://books.google.hr/books?id=qdWUKSj5PCcC
\bibitem{filic2018modelling}Filić, M. \& Filjar, R. modelling the relation between GNSS positioning performance degradation, and space weather and ionospheric conditions using RReliefF features selection. {\em ION GNSS+ 2018 Meeting}. pp. 1999-2006 (2018)
\bibitem{spilker1996global}Spilker Jr, J., Axelrad, P., Parkinson, B. \& Enge, P. Global positioning system: theory and applications, volume I. (American Institute of Aeronautics,1996)
\bibitem{oxley2017uncertainties}Oxley, A. Uncertainties in GPS Positioning: A mathematical discourse. (Academic Press,2017)
\bibitem{sikirica2021risk}Sikirica, N., Dimc, F., Jukic, O., Iliev, T., Spoljar, D. \& Filjar, R. A Risk Assessment of Geomagnetic Conditions Impact on GPS Positioning Accuracy Degradation in Tropical Regions Using Dst Index. {\em Proceedings Of The 2021 International Technical Meeting Of The Institute Of Navigation}. pp. 606-615 (2021)
\bibitem{natras2022ensemble}Natras, R., Soja, B. \& Schmidt, M. Ensemble machine learning of random forest, AdaBoost and XGBoost for vertical total electron content forecasting. {\em Remote Sensing}. \textbf{14}, 3547 (2022)
\bibitem{natras2023regional}Natras, R., Goss, A., Halilovic, D., Magnet, N., Mulic, M., Schmidt, M. \& Weber, R. Regional ionosphere delay models based on CORS data and machine learning. {\em NAVIGATION: Journal Of The Institute Of Navigation}. \textbf{70} (2023)
\bibitem{filjar2020comparison}Filjar, R., Sklebar, I. \& Horvat, M. A COMPARISON OF MACHINE LEARNING-BASED INDIVIDUAL MOBILITY CLASSIFICATION MODELS DEVELOPED ON SENSOR READINGS FROM LOOSELY ATTACHED SMARTPHONES.. {\em Komunikácie}. \textbf{22} (2020)
\bibitem{kuhn2013applied}Kuhn, M. Applied predictive modeling. (Springer,2013)
\bibitem{KuhnCaret2024}Kuhn, M. The caret Package — topepo.github.io. (https://topepo.github.io/caret/,2024), Accessed 11.09.2024.
\bibitem{rprojectProjectStatistical}RCoreTeam R: The R Project for Statistical Computing — r-project.org. (https://www.r-project.org/,2024), Accessed 11.09.2024.
\bibitem{Boser1992}Boser, B., Guyon, I. \& Vapnik, V. A training algorithm for optimal margin classifiers. {\em Proceedings Of The Fifth Annual Workshop On Computational Learning Theory}. (1992,7)
\bibitem{Cortes1995}Cortes, C. \& Vapnik, V. Support-vector networks. {\em Mach. Learn.}. \textbf{20}, 273-297 (1995,9)
\bibitem{BenHur2001}Ben-Hur, A., Horn, D., Siegelmann, H. \& Vapnik, V. Support Vector Clustering. {\em Journal Of Machine Learning Research}. \textbf{2} pp. 125-137 (2001,11)
\bibitem{Meyer2003}Meyer, D., Leisch, F. \& Hornik, K. The support vector machine under test. {\em Neurocomputing}. \textbf{55}, 169-186 (2003,9)
\bibitem{scikit-learn2023}Developers, P. 1.4. Support Vector Machines — scikit-learn.org. (http://scikit-learn.org/stable/modules/svm.html,2023), Accessed 25.01.2024.
\bibitem{HastieRosset2009}Hastie, T., Rosset, S., Zhu, J. \& Zou, H. Multi-class AdaBoost. {\em Stat. Interface}. \textbf{2}, 349-360 (2009)
\bibitem{Aizerman1964}Aizerman, M., Braverman, E. \& Rozonoer, L. Theoretical foundations of the potential function method in pattern recognition learning. {\em Automation And Remote Control}., 821-837 (1964)
\bibitem{Jin2012}Jin, C. \& Wang, L. Dimensionality dependent PAC-Bayes margin bound. {\em Advances In Neural Information Processing Systems}. \textbf{2} pp. 1034-1042 (2012,1)
\bibitem{Press2007}Press, W., Teukolsky, S., Vetterling, W. \& Flannery, B. Numerical recipes 3rd edition. (Cambridge University Press,2007,9)
\bibitem{Joachims1998}Joachims, T. Text categorization with Support Vector Machines: Learning with many relevant features. {\em Machine Learning: ECML-98}. pp. 137-142 (1998)
\bibitem{Pradhan2004}Pradhan, S., Ward, W., Hacioglu, K., Martin, J. \& Jurafsky, D. Shallow Semantic Parsing using Support Vector Machines. {\em Proceedings Of The Human Language Technology Conference Of The North American Chapter Of The Association For Computational Linguistics: HLT-NAACL 2004}. pp. 233-240 (2004), https://aclanthology.org/N04-1030
\bibitem{Laurent2014}Laurent, A., Strauss, O., Bouchon-Meunier, B. \& Yager, R. Information Processing and Management of Uncertainty in Knowledge-Based Systems. {\em 16th International Conference, IPMU}. pp. 442 (2014)
\bibitem{Barghout2015}Barghout, L. Spatial-taxon information granules as used in iterative fuzzy-decision-making for image segmentation. {\em Studies In Big Data}. pp. 285-318 (2015)
\bibitem{Maity2016}Maity, A. Supervised classification of RADARSAT-2 polarimetric data for different land features. {\em ArXiv Preprint ArXiv:1608.00501}. (2016,8)
\bibitem{Decoste2002}Decoste, D. \& Schölkopf, B. Training Invariant Support Vector Machines. {\em Mach. Learn.}. \textbf{46}, 161-190 (2002)
\bibitem{Maitra2015}Maitra, D., Bhattacharya, U. \& Parui, S. CNN based common approach to handwritten character recognition of multiple scripts. {\em 2015 13th International Conference On Document Analysis And Recognition (ICDAR)}. (2015,8)
\bibitem{Gaonkar2013}Gaonkar, B. \& Davatzikos, C. Analytic estimation of statistical significance maps for support vector machine based multi-variate image analysis and classification. {\em Neuroimage}. \textbf{78} pp. 270-283 (2013,9)
\bibitem{Cuingnet2011}Cuingnet, R., Rosso, C., Chupin, M., Lehéricy, S., Dormont, D., Benali, H., Samson, Y. \& Colliot, O. Spatial regularization of SVM for the detection of diffusion alterations associated with stroke outcome. {\em Med. Image Anal.}. \textbf{15}, 729-737 (2011,10)
\bibitem{Statnikov2006}Statnikov, A., Hardin, D. \& Aliferis, C. Using SVM Weight-Based Methods to Identify Causally Relevant and Non-Causally Relevant Variables. {\em Sign}. \textbf{1} (2006,1)
\bibitem{Hsu2003}Hsu, C., Chang, C. \& Lin, C. A Practical Guide to Support Vector Classification. (Department of Computer Science, National Taiwan University,2003)
\bibitem{Studer2011}Studer, M., Ritschard, G., Gabadinho, A. \& Müller, N. Discrepancy analysis of state sequences. {\em Sociol. Methods Res.}. \textbf{40}, 471-510 (2011,8)
\bibitem{Wu2008}Wu, X., Kumar, V., Quinlan, R., Ghosh, J., Yang, Q., Motoda, H., McLachlan, G., Ng, A., Liu, B., Yu, P., Zhou, Z., Steinbach, M., Hand, D. \& Steinberg, D. Top 10 algorithms in data mining. {\em Knowl. Inf. Syst.}. \textbf{14}, 1-37 (2008,1)
\bibitem{Rokach2014}Rokach, L. \& Maimon, O. Data mining with decision trees. (World Scientific Publishing Company,2014,10)
\bibitem{ShalevShwartz2014}Shalev-Shwartz, S. \& Ben-David, S. Decision Trees. {\em Understanding Machine Learning: From Theory To Algorithms}. pp. 212-218 (2014)
\bibitem{Quinlan1986}Quinlan, J. Induction of decision trees. {\em Mach. Learn.}. \textbf{1}, 81-106 (1986,3)
\bibitem{Rokach2005}Rokach, L. \& Maimon, O. Top-down induction of decision trees classifiers—A survey. {\em IEEE Trans. Syst. Man Cybern. C Appl. Rev.}. \textbf{35}, 476-487 (2005,11)
\bibitem{McCallum2011}McCallum, A. Graphical Models Lecture 2: Bayesian Network Representation. (https://people.cs.umass.edu/ mccallum/courses/gm2011/02-bn-rep.pdf,2011), Accessed 25.01.2024.
\bibitem{Russell1999}Russell, S. \& Norvig, P. Artificial intelligence: a modern approach. (Pearson,2016)
\bibitem{Hand2001}Hand, D. \& Yu, K. Idiot's Bayes: Not So Stupid after All?. {\em Int. Stat. Rev.}. \textbf{69}, 385 (2001,12)
\bibitem{Zhang2004}Zhang, H. The Optimality of Naive Bayes. {\em Proceedings Of The Seventeenth International Florida Artificial Intelligence Research Society Conference, FLAIRS 2004}. \textbf{2} (2004,1)
\bibitem{Caruana2006}Caruana, R. \& Niculescu-Mizil, A. An Empirical Comparison of Supervised Learning Algorithms. {\em Proceedings Of The 23rd International Conference On Machine Learning - ICML '06}. \textbf{2006} pp. 161-168 (2006,6)
\bibitem{stackexchange}StackExchange Why is the SVM margin equal to 2/w? — math.stackexchange.com. (https://math.stackexchange.com/questions/1305925/why-is-the-svm-margin-equal-to-frac2-mathbfw,2024), Accessed 25.01.2024.
\bibitem{Murty2011}Murty, M. \& Devi, V. Pattern recognition: An algorithmic approach. (Springer Science & Business Media,2011)
\bibitem{John2013}John, G. \& Langley, P. Estimating Continuous Distributions in Bayesian Classifiers.  (2013)
\bibitem{Mccallum2001}Mccallum, A. \& Nigam, K. A Comparison of Event Models for Naive Bayes Text Classification. {\em Work Learn Text Categ}. \textbf{752} (2001,5)
\bibitem{Metsis2006}Metsis, V., Androutsopoulos, I. \& Paliouras, G. Spam Filtering with Naive Bayes - Which Naive Bayes?. {\em In CEAS}. (2006,1)
\bibitem{Piryonesi2020}Piryonesi, S. \& El-Diraby, T. Role of data analytics in infrastructure asset management: Overcoming data size and quality problems. {\em J. Transp. Eng. B Pavements}. \textbf{146}, 04020022 (2020,6)
\bibitem{mitExplainedNeural}MIT Explained: Neural networks — news.mit.edu. (https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414,2017), Accessed 27.09.2024
\bibitem{brahme2014comprehensive}Brahme, A. Comprehensive biomedical physics. (Newnes,2014)
\bibitem{olden2002illuminating}Olden, J. \& Jackson, D. Illuminating the "black box": a randomization approach for understanding variable contributions in artificial neural networks. {\em Ecological Modelling}. \textbf{154}, 135-150 (2002)
\bibitem{ozesmi1999artificial}Özesmi, S. \& Özesmi, U. An artificial neural network approach to spatial habitat modelling with interspecific interaction. {\em Ecological Modelling}. \textbf{116}, 15-31 (1999)
\bibitem{bishop2006pattern}Bishop, C. Pattern Recognition and Machine Learning. (Springer,2006)
\bibitem{vapnik2013nature}Vapnik, V. The nature of statistical learning theory. (Springer science & business media,2013)
\bibitem{goodfellow2016deep}Goodfellow, I. Deep learning. (MIT press,2016)
\bibitem{probst2019tunability}Probst, P., Boulesteix, A. \& Bischl, B. Tunability: Importance of hyperparameters of machine learning algorithms. {\em Journal Of Machine Learning Research}. \textbf{20}, 1-32 (2019)
\bibitem{zoph2016neural}Zoph, B. Neural architecture search with reinforcement learning. {\em ArXiv Preprint ArXiv:1611.01578}. (2016)
\bibitem{jin2019auto}Jin, H., Song, Q. \& Hu, X. Auto-keras: An efficient neural architecture search system. {\em Proceedings Of The 25th ACM SIGKDD International Conference On Knowledge Discovery & Data Mining}. pp. 1946-1956 (2019)
\bibitem{claesen2015hyperparameter}Claesen, M. \& De Moor, B. Hyperparameter search in machine learning. {\em ArXiv Preprint ArXiv:1502.02127}. (2015)
\bibitem{liu2020python}Liu, Y. Python Machine Learning by Example: Build Intelligent Systems Using Python, TensorFlow 2, PyTorch, and Scikit-Learn. (Packt Publishing Ltd,2020)
\bibitem{wold2001pls}Wold, S., Sjöström, M. \& Eriksson, L. PLS-regression: a basic tool of chemometrics. {\em Chemometrics And Intelligent Laboratory Systems}. \textbf{58}, 109-130 (2001)
\bibitem{abdi2010partial}Abdi, H. Partial least squares regression and projection on latent structure regression (PLS Regression). {\em Wiley Interdisciplinary Reviews: Computational Statistics}. \textbf{2}, 97-106 (2010)
\bibitem{saebo2008lpls}Sæbø, S., Almøy, T., Flatberg, A., Aastveit, A. \& Martens, H. LPLS-regression: a method for prediction and classification under the influence of background information on predictor variables. {\em Chemometrics And Intelligent Laboratory Systems}. \textbf{91}, 121-132 (2008)
\bibitem{youtubeYouTube}Asada, H. Fall Term (AY 2020-2021) - 2.160 Identification, Estim, & Learn Lecture 6: Partial Least Squares Regression. (https://www.youtube.com/watch?v=Px2otK2nZ1c&t=46s,2020), Accessed 26.09.2024
\bibitem{lindgren1993kernel}Lindgren, F., Geladi, P. \& Wold, S. The kernel algorithm for PLS. {\em Journal Of Chemometrics}. \textbf{7}, 45-59 (1993)
\bibitem{de1994comments}De Jong, S. \& Ter Braak, C. Comments on the PLS kernel algorithm. {\em Journal Of Chemometrics}. \textbf{8}, 169-174 (1994)
\bibitem{dayal1997improved}Dayal, B. \& MacGregor, J. Improved PLS algorithms. {\em Journal Of Chemometrics: A Journal Of The Chemometrics Society}. \textbf{11}, 73-85 (1997)
\bibitem{de1993simpls}De Jong, S. SIMPLS: an alternative approach to partial least squares regression. {\em Chemometrics And Intelligent Laboratory Systems}. \textbf{18}, 251-263 (1993)
\bibitem{rannar1994pls}Rännar, S., Lindgren, F., Geladi, P. \& Wold, S. A PLS kernel algorithm for data sets with many variables and fewer objects. Part 1: Theory and algorithm. {\em Journal Of Chemometrics}. \textbf{8}, 111-125 (1994)
\bibitem{takane2016pls}Takane, Y. \& Loisel, S. On the PLS algorithm for multiple regression (PLS1). {\em The Multiple Facets Of Partial Least Squares And Related Methods: PLS, Paris, France, 2014 8}. pp. 17-28 (2016)
\bibitem{hoskuldsson1988pls}Höskuldsson, A. PLS regression methods. {\em Journal Of Chemometrics}. \textbf{2}, 211-228 (1988)
\bibitem{hastie1995penalized}Hastie, T., Buja, A. \& Tibshirani, R. Penalized discriminant analysis. {\em The Annals Of Statistics}. \textbf{23}, 73-102 (1995)
\bibitem{HastieTibshirani2009}Hastie, T., Tibshirani, R., Friedman, J. \& Friedman, J. The elements of statistical learning: data mining, inference, and prediction. (Springer,2009)
\bibitem{hastie1994flexible}Hastie, T., Tibshirani, R. \& Buja, A. Flexible discriminant analysis by optimal scoring. {\em Journal Of The American Statistical Association}. \textbf{89}, 1255-1270 (1994)
\bibitem{reynes2006choice}Reynès, C., Sabatier, R. \& Molinari, N. Choice of B-splines with free parameters in the flexible discriminant analysis context. {\em Computational Statistics & Data Analysis}. \textbf{51}, 1765-1778 (2006)
\bibitem{phillips2017applying}Phillips, N., Reid, N., Thys, T., Harrod, C., Payne, N., Morgan, C., White, H., Porter, S. \& Houghton, J. Applying species distribution modelling to a data poor, pelagic fish complex: the ocean sunfishes. {\em Journal Of Biogeography}. \textbf{44}, 2176-2187 (2017)
\bibitem{hallgren2019species}Hallgren, W., Santana, F., Low-Choy, S., Zhao, Y. \& Mackey, B. Species distribution models can be highly sensitive to algorithm configuration. {\em Ecological Modelling}. \textbf{408} pp. 108719 (2019)
\bibitem{thuiller2016ensemble}Thuiller, W., Georges, D., Engler, R. \& Breiner, F. Ensemble platform for species distribution modeling. {\em R Package Version}. pp. 3-1 (2016)
\bibitem{quillfeldt2017influence}Quillfeldt, P., Engler, J., Silk, J. \& Phillips, R. Influence of device accuracy and choice of algorithm for species distribution modelling of seabirds: a case study using black-browed albatrosses. {\em Journal Of Avian Biology}. \textbf{48}, 1549-1555 (2017)
\bibitem{zhang2019using}Zhang, Z., Xu, S., Capinha, C., Weterings, R. \& Gao, T. Using species distribution model to predict the impact of climate change on the potential distribution of Japanese whiting Sillago japonica. {\em Ecological Indicators}. \textbf{104} pp. 333-340 (2019)
\bibitem{cohen2013applied}Cohen, J., Cohen, P., West, S. \& Aiken, L. Applied multiple regression/correlation analysis for the behavioral sciences. (Routledge,2013)
\bibitem{hansen2005using}Hansen, J. Using SPSS for windows and macintosh: analyzing and understanding data. (Taylor & Francis,2005)
\bibitem{fisher1936use}Fisher, R. The use of multiple measurements in taxonomic problems. {\em Annals Of Eugenics}. \textbf{7}, 179-188 (1936)
\bibitem{mclachlan2005discriminant}McLachlan, G. Discriminant analysis and statistical pattern recognition. (John Wiley & Sons,2005)
\bibitem{wetcher2011analyzing}Wetcher-Hendricks, D. Analyzing quantitative data: An introduction for social researchers. (John Wiley & Sons,2011)
\bibitem{martinez2001pca}Martinez, A. \& Kak, A. PCA versus LDA. {\em IEEE Transactions On Pattern Analysis And Machine Intelligence}. \textbf{23}, 228-233 (2001)
\bibitem{abdi2007discriminant}Abdi, H. Discriminant correspondence analysis. {\em Encyclopedia Of Measurement And Statistics}. \textbf{2007} pp. 1-10 (2007)
\bibitem{perriere2003use}Perriere, G. \& Thioulouse, J. Use of correspondence discriminant analysis to predict the subcellular location of bacterial proteins. {\em Computer Methods And Programs In Biomedicine}. \textbf{70}, 99-105 (2003)
\bibitem{cokluk2008discriminant}Cokluk, B. \& Buyukozturk, S. Discriminant function analysis: concept and application. {\em Eğitim Araştırmaları Dergisi}. \textbf{33} pp. 73-92 (2008)
\bibitem{venables2013modern}Venables, W. \& Ripley, B. Modern applied statistics with S-PLUS. (Springer Science & Business Media,2013)
\bibitem{lachenbruch1979discriminant}Lachenbruch, P. \& Goldstein, M. Discriminant analysis. {\em Biometrics}. pp. 69-85 (1979)
\bibitem{klecka1980discriminant}Klecka, W. Discriminant analysis. (Sage,1980)
\bibitem{Hardle2003AppliedMS}Härdle, W. Applied multivariate statistical analysis. (Springer Nature,2003)
\bibitem{archive765Discriminant}Garson, G. PA 765: Discriminant Function Analysis — web.archive.org. (https://web.archive.org/web/20080312065328/http://www2.chass.ncsu.edu/garson/pA765/discrim.htm,2008), Accessed 26.09.2024
\bibitem{israel2006performance}Israel, S. Performance metrics: how and when. {\em Geocarto International}. \textbf{21}, 23-32 (2006)
\bibitem{rao1948utilization}Rao, C. The utilization of multiple measurements in problems of biological classification. {\em Journal Of The Royal Statistical Society. Series B (Methodological)}. \textbf{10}, 159-203 (1948)
\bibitem{ripley2007pattern}Ripley, B. Pattern recognition and neural networks. (Cambridge university press,2007)
\bibitem{jolliffe2016principal}Jolliffe, I. \& Cadima, J. Principal component analysis: a review and recent developments. {\em Philosophical Transactions Of The Royal Society A: Mathematical, Physical And Engineering Sciences}. \textbf{374}, 20150202 (2016)
\bibitem{barnett1987origins}Barnett, T. \& Preisendorfer, R. Origins and levels of monthly and seasonal forecast skill for United States surface air temperatures determined by canonical correlation analysis. {\em Monthly Weather Review}. \textbf{115}, 1825-1850 (1987)
\bibitem{hsu2012spectral}Hsu, D., Kakade, S. \& Zhang, T. A spectral algorithm for learning hidden Markov models. {\em Journal Of Computer And System Sciences}. \textbf{78}, 1460-1480 (2012)
\bibitem{markopoulos2017efficient}Markopoulos, P., Kundu, S., Chamadia, S. \& Pados, D. Efficient L1-norm principal-component analysis via bit flipping. {\em IEEE Transactions On Signal Processing}. \textbf{65}, 4252-4264 (2017)
\bibitem{chachlakis2019l1}Chachlakis, D., Prater-Bennette, A. \& Markopoulos, P. L1-norm Tucker tensor decomposition. {\em IEEE Access}. \textbf{7} pp. 178454-178465 (2019)
\bibitem{markopoulos2014optimal}Markopoulos, P., Karystinos, G. \& Pados, D. Optimal algorithms for L1-subspace signal processing. {\em IEEE Transactions On Signal Processing}. \textbf{62}, 5046-5058 (2014)
\bibitem{zhan2015robust}Zhan, J. \& Vaswani, N. Robust PCA with partial subspace knowledge. {\em IEEE Transactions On Signal Processing}. \textbf{63}, 3332-3347 (2015)
\bibitem{ke2005robust}Ke, Q. \& Kanade, T. Robust l/sub 1/norm factorization in the presence of outliers and missing data by alternative convex programming. {\em 2005 IEEE Computer Society Conference On Computer Vision And Pattern Recognition (CVPR'05)}. \textbf{1} pp. 739-746 (2005)
\bibitem{pearson1901liii}Pearson, K. LIII. On lines and planes of closest fit to systems of points in space. {\em The London, Edinburgh, And Dublin Philosophical Magazine And Journal Of Science}. \textbf{2}, 559-572 (1901)
\bibitem{stewart2019introduction}Stewart, F. Introduction to linear algebra. (Courier Dover Publications,2019)
\bibitem{hotelling1933analysis}Hotelling, H. Analysis of a complex of statistical variables into principal components.. {\em Journal Of Educational Psychology}. \textbf{24}, 417 (1933)
\bibitem{hotelling1992relations}Hotelling, H. Relations between two sets of variates. {\em Breakthroughs In Statistics: Methodology And Distribution}. pp. 162-190 (1992)
\bibitem{berkooz1993proper}Berkooz, G., Holmes, P. \& Lumley, J. The proper orthogonal decomposition in the analysis of turbulent flows. {\em Annual Review Of Fluid Mechanics}. \textbf{25}, 539-575 (1993)
\bibitem{karhunen1946spektraltheorie}Karhunen, K. Zur spektraltheorie stochasticher. {\em Annales Academiae Scientiarum Fennicae Series A}. \textbf{1} pp. 34 (1946)
\bibitem{loeve1977elementary}Loève, M. Elementary probability theory. (Springer,1977)
\bibitem{sirovich1987turbulence}Sirovich, L. Turbulence and the dynamics of coherent structures. I. Coherent structures. {\em Quarterly Of Applied Mathematics}. \textbf{45}, 561-571 (1987)
\bibitem{sapatnekar2011overcoming}Sapatnekar, S. Overcoming variations in nanometer-scale technologies. {\em IEEE Journal On Emerging And Selected Topics In Circuits And Systems}. \textbf{1}, 5-18 (2011)
\bibitem{ghoman2012pod}Ghoman, S., Wang, Z., Chen, P. \& Kapania, R. A POD-based reduced order design scheme for shape optimization of air vehicles. {\em 53rd AIAA/ASME/ASCE/AHS/ASC Structures, Structural Dynamics And Materials Conference 20th AIAA/ASME/AHS Adaptive Structures Conference 14th AIAA}. pp. 1808 (2012)
\bibitem{archiveKarhunenLoeveTransform}Wang, R. Computer Image Processing and Analysis (E161) lectures, Harvey Mudd College, Karhunen-Loeve Transform (KLT). (https://web.archive.org/web/20161128140401/http://fourier.eng.hmc.edu/e161/lectures/klt/node3.html,2016), Accessed 29.09.2024
\bibitem{giambartolomei2016}Giambartolomei, G. The Karhunen-Loève theorem. (University of Bologna,2016), http://amslaurea.unibo.it/10169/
\bibitem{mallat1999wavelet}Mallat, S. A wavelet tour of signal processing. (Academic Press,1999)
\bibitem{tang1998texture}Tang, X. Texture information in run-length matrices. {\em IEEE Transactions On Image Processing}. \textbf{7}, 1602-1609 (1998)
\bibitem{stewart1993early}Stewart, G. On the early history of the singular value decomposition. {\em SIAM Review}. \textbf{35}, 551-566 (1993)
\bibitem{gloub1996matrix}Gloub, G. \& Van Loan, C. Matrix computations. {\em Johns Hopkins Universtiy Press, 3rd Edtion}. (1996)
\bibitem{hayden2002observations}Hayden, A. \& Twede, D. Observations on the relationship between eigenvalues, instrument noise, and detection performance. {\em Imaging Spectrometry VIII}. \textbf{4816} pp. 355-362 (2002)
\bibitem{jolliffe2002principal}Jolliffe, I. Principal Component Analysis (Springer Series in Statistics), Springer.  (2002)
\bibitem{lorenz1956empirical}Lorenz, E. Empirical orthogonal functions and statistical weather prediction. (Massachusetts Institute of Technology, Department of Meteorology Cambridge,1956)
\bibitem{eckart1936approximation}Eckart, C. \& Young, G. The approximation of one matrix by another of lower rank. {\em Psychometrika}. \textbf{1}, 211-218 (1936)
\bibitem{dove1993introduction}Dove, M. Introduction to lattice dynamics. (Cambridge university press,1993)
\bibitem{bengio2013representation}Bengio, Y., Courville, A. \& Vincent, P. Representation learning: A review and new perspectives. {\em IEEE Transactions On Pattern Analysis And Machine Intelligence}. \textbf{35}, 1798-1828 (2013)
\bibitem{forkman2019hypothesis}Forkman, J., Josse, J. \& Piepho, H. Hypothesis tests for principal component analysis when variables are standardized. {\em Journal Of Agricultural, Biological And Environmental Statistics}. \textbf{24} pp. 289-308 (2019)
\bibitem{tobiska1996current}Tobiska, W. Current status of solar EUV measurements and modeling. {\em Advances In Space Research}. \textbf{18}, 3-10 (1996)
\bibitem{vitinsky1986statistics}Vitinsky, Y., Kopecky, M. \& Kuklin, G. Statistics of Sunspot Activity (Moscow, USSR. (Nauka,1986)
\bibitem{lanyi1988comparison}Lanyi, G. \& Roth, T. A comparison of mapped and measured total ionospheric electron content using global positioning system and beacon satellite observations. {\em Radio Science}. \textbf{23}, 483-492 (1988)
\bibitem{rishbeth1969introduction}Rishbeth, H. \& Garriott, O. Introduction to ionospheric physics. {\em Introduction To Ionospheric Physics}. (1969)
\bibitem{ratcliffe1972introduction}Ratcliffe, J. An introduction to ionosphere and magnetosphere. (CUP Archive,1972)
\bibitem{klobuchar1986design}Klobuchar, J. Design and characteristics of the GPS ionospheric time delay algorithm for single frequency users. {\em PLANS 1986-Position Location And Navigation Symposium}. pp. 280-286 (1986)
\bibitem{zolesi2014ionospheric}Zolesi, B. \& Cander, L. Ionospheric prediction and forecasting. (Springer,2014)
\bibitem{komjathy2023global}Komjathy, A. Global ionospheric total electron content mapping using the Global Positioning System.  (2023)
\bibitem{NOAA2024aRadioCommunications}NOAA HF Radio Communications | NOAA / NWS Space Weather Prediction Center — swpc.noaa.gov. (http://www.swpc.noaa.gov/impacts/hf-radio-communications,2024), Accessed 17.10.2024.
\bibitem{Gray2019}Gray, A. Solar weather monitoring - National Research Council Canada — nrc.canada.ca. (https://nrc.canada.ca/en/research-development/products-services/technical-advisory-services/solar-weather-monitoring,2019), Accessed 17.10.2024.
\bibitem{Gray2021}Gray, A. Introducing the solar radio program — spaceweather.gc.ca. (https://www.spaceweather.gc.ca/forecast-prevision/solar-solaire/solarflux/sx-1-en.php,2021), Accessed 17.10.2024.
\bibitem{wild1994ionosphere}Wild, U. Ionosphere and geodetic satellite systems: permanent GPS tracking data for modelling and monitoring.. {\em Geodätisch-geophysikalische Arbeiten In Der Schweiz}. \textbf{48} (1994)
\bibitem{hawarey2010uzay}Hawarey, M. \& Ayan, T. Uzay mekiği tırmanışı, deprem, ve füze fırlatılışından kaynaklanan TEC değişimlerinin GPS ile belirlenmesi. {\em İTÜDERGİSİ/d}. \textbf{3} (2010)
\bibitem{kotz2005encyclopedia}Kotz, S., Balakrishnan, N., Read, C., Vidakovic, B. \& Johnson, N. Encyclopedia of Statistical Sciences, Volume 1. (John Wiley & Sons,2005)
\bibitem{vellinov1992ionospheric}Vellinov, P., Spassov, C. \& Kolev, S. Ionospheric effects of lightning during the increasing part of solar cycle 22. {\em Journal Of Atmospheric And Terrestrial Physics}. \textbf{54}, 1347-1353 (1992)
\bibitem{liu20142013}Liu, M., Luo, G. \& Wang, H. The 2013 Lushan earthquake in China tests hazard assessments. {\em Seismological Research Letters}. \textbf{85}, 40-43 (2014)
\bibitem{ulukavak2018analysis}Ulukavak, M. \& Yalcinkaya, M. Analysis of ionospheric anomalies due to space weather conditions by using GPS-TEC variations. {\em FIG Congress}. pp. 6-11 (2018)
\bibitem{schwenn2001solar}Schwenn, R. Solar wind: Global properties. {\em Encyclopedia Of Astronomy & Astrophysics}. pp. 1-9 (2001)
\bibitem{melnikov1990relationships}Melnikov, V. Relationships between Microwave, Hard X ray, and Corpuscular Emissions of Solar Flares. (Ph. D. Thesis, Radiophysical Research Institute, Nizhniy Novgorod, Russia,1990)
\bibitem{harden2005solar}Harden, P. Solar Activity & HF Propagation. {\em QRP Amateur Radio Club International}. (2005)
\bibitem{gonzalez1994geomagnetic}Gonzalez, W., Joselyn, J., Kamide, Y., Kroehl, H., Rostoker, G., Tsurutani, B. \& Vasyliunas, V. What is a geomagnetic storm?. {\em Journal Of Geophysical Research: Space Physics}. \textbf{99}, 5771-5792 (1994)
\bibitem{Viereck2024}Viereck, R. Solar EUV Irradiance | NOAA / NWS Space Weather Prediction Center — swpc.noaa.gov. (http://www.swpc.noaa.gov/phenomena/solar-euv-irradiance,2024), Accessed 17.10.2024.
\bibitem{aschwanden2006physics}Aschwanden, M. Physics of the solar corona: an introduction with problems and solutions. (Springer Science & Business Media,2006)
\bibitem{abraha2014total}Abraha, G. Total electron content (TEC) variability of low latitude ionosphere and role of dynamical coupling: quiet and storm-time characteristics. (Addis Ababa University,2014)
\bibitem{turunen1980examples}Turunen, T. \& Rao, M. Examples of the influence of strong magnetic storms on the equatorial F-layer. {\em Journal Of Atmospheric And Terrestrial Physics}. \textbf{42}, 323-330 (1980)
\bibitem{adeniyi1986magnetic}Adeniyi, J. Magnetic storm effects on the morphology of the equatorial F2-layer. {\em Journal Of Atmospheric And Terrestrial Physics}. \textbf{48}, 695-702 (1986)
\bibitem{Papitashvili2024About}Papitashvili, N. SPDF - About OMNIWeb Data — omniweb.gsfc.nasa.gov. (http://omniweb.gsfc.nasa.gov/html/ow_data.html,2024), Accessed 17.10.2024.
\bibitem{Papitashvili2024Explorer}Papitashvili, N. OMNIWeb Data Explorer — omniweb.gsfc.nasa.gov. (http://omniweb.gsfc.nasa.gov/form/dx1.html,2024), Accessed 17.10.2024.
\bibitem{de2007ionosphere}De Canck, M. Ionosphere properties and behaviors. {\em Antennex}. \textbf{119} pp. 6-7 (2007)
\bibitem{NOAA2024bKindex}NOAA swpc.noaa.gov. (https://www.swpc.noaa.gov/sites/default/files/images/u2/TheK-index.pdf,2024), Accessed 17.10.2024.
\bibitem{Matzka2024}Matzka, J. Kp Index — gfz-potsdam.de. (https://www.gfz-potsdam.de/en/section/geomagnetism/data-products-services/geomagnetic-kp-index,2024), Accessed 17.10.2024.
\bibitem{NOAA2024cAlertsWatches}NOAA Alerts, Watches and Warnings | NOAA / NWS Space Weather Prediction Center — swpc.noaa.gov. (https://www.swpc.noaa.gov/products/alerts-watches-and-warnings,2024), Accessed 17.10.2024.
\bibitem{Myint2022}Myint, L., Hozumi, K., Saito, S. \& Supnithi, P. Analysis of local geomagnetic index under the influence of equatorial electrojet (EEJ) at the equatorial Phuket geomagnetic station in Thailand. {\em Advances In Space Research}. \textbf{70}, 1429-1440 (2022), https://www.sciencedirect.com/science/article/pii/S0273117722004963
\bibitem{NOAA2024dPlanetary}NOAA Planetary K-index | NOAA / NWS Space Weather Prediction Center — swpc.noaa.gov. (https://www.swpc.noaa.gov/products/planetary-k-index,2024), Accessed 17.10.2024.
\bibitem{NOAA2024eSpace}NOAA NOAA Space Weather Scales | NOAA / NWS Space Weather Prediction Center — swpc.noaa.gov. (https://www.swpc.noaa.gov/noaa-scales-explanation,2024), Accessed 17.10.2024.
\bibitem{NOAA2024fScales}NOAA swpc.noaa.gov. (https://www.swpc.noaa.gov/sites/default/files/images/NOAAscales.pdf,2024), Accessed 17.10.2024.
\bibitem{Matzka2021a}Matzka, J., Bronkalla, O., Tornow, K., Elger, K. \& Stolle, C. Geomagnetic Kp index. (GFZ Data Services,2021)
\bibitem{Matzka2021b}Matzka, J., Stolle, C., Yamazaki, Y., Bronkalla, O. \& Morschhauser, A. The geomagnetic KP index and derived indices of geomagnetic activity. {\em Space Weather}. \textbf{19} (2021,5)
\bibitem{NOAA2024gGeomagneticIndices}NOAA Geomagnetic kp and ap Indices | NCEI — ngdc.noaa.gov. (https://www.ngdc.noaa.gov/stp/GEOMAG/kp_ap.html,2024), Accessed 17.10.2024.
\bibitem{prolss2012physics}Prölss, G. Physics of the Earth’s space environment: an introduction. (Springer Science & Business Media,2012)
\bibitem{liu2009seismoionospheric}Liu, J., Chen, Y., Chen, C., Liu, C., Chen, C., Nishihashi, M., Li, J., Xia, Y., Oyama, K., Hattori, K. \& Lin, C. Seismoionospheric GPS total electron content anomalies observed before the 12 May 2008 Mw7. 9 Wenchuan earthquake. {\em Journal Of Geophysical Research: Space Physics}. \textbf{114} (2009)
\bibitem{durmaz2015regional}Durmaz, M. \& Karslioglu, M. Regional vertical total electron content (VTEC) modeling together with satellite and receiver differential code biases (DCBs) using semi-parametric multivariate adaptive regression B-splines (SP-BMARS). {\em Journal Of Geodesy}. \textbf{89} pp. 347-360 (2015)
\bibitem{jin2012m_dcb}Jin, R., Jin, S. \& Feng, G. M_DCB: Matlab code for estimating GNSS satellite and receiver differential code biases. {\em GPS Solutions}. \textbf{16}, 541-548 (2012)
\bibitem{schaer1999mapping}Schaer, S. Mapping and predicting the Earth's ionosphere using the Global Positioning System. (Institut für Geodäsie und Photogrammetrie, Eidg. Technische Hochschule,1999)
\bibitem{enge1994global}Enge, P. The global positioning system: Signals, measurements, and performance. {\em International Journal Of Wireless Information Networks}. \textbf{1} pp. 83-105 (1994)
\bibitem{klobuchar1987ionospheric}Klobuchar, J. Ionospheric time-delay algorithm for single-frequency GPS users. {\em IEEE Transactions On Aerospace And Electronic Systems}. pp. 325-331 (1987)
\bibitem{Pradipta2015}Pradipta, R., Valladares, C. \& Doherty, P. An effective TEC data detrending method for the study of equatorial plasma bubbles and traveling ionospheric disturbances. {\em Journal Of Geophysical Research: Space Physics}. \textbf{120}, 11-048 (2015)
\bibitem{Pradipta2019}Pradipta, R. \& Doherty, P. Comparison between 2-D GPS dTEC Data Maps and Digisonde Observations over the South American and Atlantic Regions. {\em 2019 URSI Asia-Pacific Radio Science Conference (AP-RASC)}. pp. 1-1 (2019)
\bibitem{loewe1997classification}Loewe, C. \& Prölss, G. Classification and mean behavior of magnetic storms. {\em Journal Of Geophysical Research: Space Physics}. \textbf{102}, 14209-14213 (1997)
\bibitem{kamide1998two}Kamide, Y., Yokoyama, N., Gonzalez, W., Tsurutani, B., Daglis, I., Brekke, A. \& Masuda, S. Two-step development of geomagnetic storms. {\em Journal Of Geophysical Research: Space Physics}. \textbf{103}, 6917-6921 (1998)
\bibitem{rozhnoi2004middle}Rozhnoi, A., Solovieva, M., Molchanov, O. \& Hayakawa, M. Middle latitude LF (40 kHz) phase variations associated with earthquakes for quiet and disturbed geomagnetic conditions. {\em Physics And Chemistry Of The Earth, Parts A/B/C}. \textbf{29}, 589-598 (2004)
\bibitem{contadakis2012total}Contadakis, M., Arabelos, D., Pikridas, C. \& Spatalas, S. Total electron content variations over southern Europe before and during the M 6.3 Abruzzo earthquake of April 6, 2009. {\em Annals Of Geophysics}. \textbf{55} (2012)
\bibitem{Fan2008}Fan, R., Chang, K., Hsieh, C., Wang, X. \& Lin, C. LIBLINEAR: a library for large linear classification. {\em Journal Of Machine Learning Research}. \textbf{9} pp. 1871-1874 (2008,8)
\bibitem{Mohamad2013}Mohamad, I. \& Usman, D. Standardization and its effects on K-means clustering algorithm. {\em Research Journal Of Applied Sciences, Engineering And Technology}. \textbf{6}, 3299-3303 (2013,9)
\bibitem{Fennell2019}Fennell, P., Zuo, Z. \& Lerman, K. Predicting and explaining behavioral data with structured feature space decomposition. {\em EPJ Data Sci.}. \textbf{8} (2019,12)
\bibitem{KuhnDataSplitting2024}Kuhn, M. 4 Data Splitting | The caret Package — topepo.github.io. (https://topepo.github.io/caret/data-splitting.html,2024), Accessed 28.09.2024
\bibitem{hyndman2018forecasting}Hyndman, R. Forecasting: principles and practice. (OTexts,2018)
\bibitem{createDataPartition2024}Developers, R. createDataPartition function - RDocumentation — rdocumentation.org. (https://www.rdocumentation.org/packages/caret/versions/6.0-94/topics/createDataPartition,2024), Accessed 27.09.2024
\bibitem{kuhn2008building}Kuhn, M. Building predictive models in R using the caret package. {\em Journal Of Statistical Software}. \textbf{28} pp. 1-26 (2008)
\bibitem{altman1994diagnostic1}Altman, D. \& Bland, J. Diagnostic tests. 1: Sensitivity and specificity.. {\em BMJ: British Medical Journal}. \textbf{308}, 1552 (1994)
\bibitem{altman1994diagnostic2}Altman, D. \& Bland, J. Diagnostic test 2: predictive values. {\em BMJ: British Medical Journal}. \textbf{309} pp. 102 (1994)
\bibitem{velez2007balanced}Velez, D., White, B., Motsinger, A., Bush, W., Ritchie, M., Williams, S. \& Moore, J. A balanced accuracy function for epistasis modeling in imbalanced datasets using multifactor dimensionality reduction. {\em Genetic Epidemiology: The Official Publication Of The International Genetic Epidemiology Society}. \textbf{31}, 306-315 (2007)
\bibitem{confusionMatrix2024}Developers, R. confusionMatrix function - RDocumentation — rdocumentation.org. (https://www.rdocumentation.org/packages/caret/versions/6.0-94/topics/confusionMatrix,2024), Accessed 27.09.2024
\bibitem{binom.test2024}Developers, R. binom.test function - RDocumentation — rdocumentation.org. (https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/binom.test,2024), Accessed 28.09.2024
\bibitem{clopper1934use}Clopper, C. \& Pearson, E. The use of confidence or fiducial limits illustrated in the case of the binomial. {\em Biometrika}. \textbf{26}, 404-413 (1934)
\bibitem{conover1999practical}Conover, W. Practical nonparametric statistics. (John Wiley & Sons, Inc,1999)
\bibitem{hollander2013nonparametric}Hollander, M. Nonparametric statistical methods. (John Wiley & Sons Inc,2013)
\bibitem{howell1992statistical}Howell, D. Statistical methods for psychology, Seventh Edition. (Cengage Wadsworth,2009)
\bibitem{graphpadGraphPadPrism}GraphPad Software, I. GraphPad Prism 6 Statistics Guide - The binomial test — graphpad.com. (https://www.graphpad.com/guides/prism/6/statistics/stat_binomial.htm,2024), Accessed 28.09.2024
\bibitem{mchugh2012interrater}McHugh, M. Interrater reliability: the kappa statistic. {\em Biochemia Medica}. \textbf{22}, 276-282 (2012)
\bibitem{pontius2011death}Pontius Jr, R. \& Millones, M. Death to Kappa: birth of quantity disagreement and allocation disagreement for accuracy assessment. {\em International Journal Of Remote Sensing}. \textbf{32}, 4407-4429 (2011)
\bibitem{galton1892finger}Galton, F. Finger prints. (Cosimo Classics,1892)
\bibitem{smeeton1985early}Smeeton, N. Early history of the kappa statistic. {\em Biometrics}. \textbf{41} pp. 795 (1985)
\bibitem{cohen1960coefficient}Cohen, J. A coefficient of agreement for nominal scales. {\em Educational And Psychological Measurement}. \textbf{20}, 37-46 (1960)
\bibitem{sim2005kappa}Sim, J. \& Wright, C. The kappa statistic in reliability studies: use, interpretation, and sample size requirements. {\em Physical Therapy}. \textbf{85}, 257-268 (2005)
\bibitem{chicco2021matthews}Chicco, D., Warrens, M. \& Jurman, G. The Matthews correlation coefficient (MCC) is more informative than Cohen's Kappa and Brier score in binary classification assessment. {\em IEEE Access}. \textbf{9} pp. 78368-78381 (2021)
\bibitem{heidke1926berechnung}Heidke, P. Berechnung des Erfolges und der Güte der Windstärkevorhersagen im Sturmwarnungsdienst. {\em Geografiska Annaler}. \textbf{8}, 301-349 (1926)
\bibitem{philosophical1887bulletin}D.C., T. Bulletin of the Philosophical Society of Washington, D.C.. (The co-operation of the Smithsonian Institution,1887)
\bibitem{system.time2024}Developers, R. system.time function - RDocumentation — rdocumentation.org. (https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/system.time,2024), Accessed 28.09.2024

\end{thebibliography}

\end{document}