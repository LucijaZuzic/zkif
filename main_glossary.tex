%Version 3 October 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
%%%%\documentclass[sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

\let\LaTeXcline\cline\documentclass[sn-mathphys-num]{sn-jnl}\let\cline\LaTeXcline

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%
\usepackage{anyfontsize}

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
%\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

%\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

%\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\usepackage[acronym]{glossaries}
\renewcommand{\glstextformat}[1]{\color{black}#1}

\makeglossaries
\newacronym{doi}{DOI}{Digital Object Identifier}
\newacronym{dst}{Dst}{Disturbance Storm-Time}
\newacronym{rin}{RIN}{The Royal Institute of Navigation}
\newacronym{ion}{ION}{The Institute of Navigation}
\newacronym{ursi}{URSI}{Union Radio-Scientifique Internationale}
\newacronym{rinex}{RINEX}{Receiver Independent Exchange Format}
\newacronym{tec}{TEC}{Total Electron Content}
\newacronym{dtec}{dTEC}{standard deviation of Total Electron Content}
\newacronym{gnss}{GNSS}{Global Navigation Satellite System}
\newacronym{pnt}{PNT}{Positioning, Navigation, and Timing}
\newacronym{svm}{SVM}{Support Vector Machine}
\newacronym{svn}{SVN}{Support Vector Network}
\newacronym{dt}{DT}{Decision Tree}
\newacronym{tdidt}{TDIDT}{Top-Down Induction of Decision Trees}
\newacronym{nb}{NB}{Naive Bayes}
\newacronym{ci}{CI}{Confidence Interval}
\newacronym{nir}{NIR}{No Information Rate}
\newacronym{nn}{NN}{Neural Network}
\newacronym{ann}{ANN}{Artificial Neural Network}
\newacronym{pls}{PLS}{Partial Least Squares}
\newacronym{pls-da}{PLS-DA}{Partial Least Squares Discriminant Analysis}
\newacronym{fda}{FDA}{Flexible Discriminant Analysis}
\newacronym{lda}{LDA}{Linear Discriminant Analysis}
\newacronym{pca}{PCA}{Principal Component Analysis}
\newacronym{manova}{MANOVA}{Multivariate Analysis Of Variance}
\newacronym{mars}{MARS}{Multivariate Adaptive Regression Splines}
\newacronym{nda}{NDA}{Normal Discriminant Analysis}
\newacronym{nas}{NAS}{Neural Architecture Search}
\newacronym{dbscan}{DBSCAN}{Density-Based Spatial Clustering}
\newacronym{cnn}{CNN}{Convolutional Neural Network}
\newacronym{rf}{RF}{Random Forest}
\newacronym{tp}{TP}{True Positive}
\newacronym{tn}{TN}{True Negative}
\newacronym{fp}{FP}{False Positive}
\newacronym{fn}{FN}{False Negative}
\newacronym{tpr}{TPR}{True Positive Rate}
\newacronym{tnr}{TNR}{True Negative Rate}
\newacronym{ppv}{PPV}{Positive Predictive Value}
\newacronym{npv}{NPV}{Negative Predictive Value}
\newacronym{dr}{DR}{Detection Rate}
\newacronym{dp}{DP}{Detection Prevalence}
\newacronym{ba}{BA}{Balanced Accuracy}
\newacronym{sar}{SAR}{Synthetic Aperture Radar}
\newacronym{cca}{CCA}{Canonical Correlation Analysis}
\newacronym{pod}{POD}{Proper Orthogonal Decomposition}
\newacronym{svd}{SVD}{Singular Value Decomposition}
\newacronym{eof}{EOF}{Empirical Orthogonal Functions}
\newacronym{evd}{EVD}{Eigenvalue Decomposition}
\newacronym{gps}{GPS}{Global Positioning System}
\newacronym{klt}{KLT}{Karhunen–Loève Theorem}
\newacronym{gpu}{GPU}{Graphics Processing Unit}
\newacronym{cpu}{CPU}{Central Processing Unit}
\newacronym{ram}{RAM}{Random Access Memory}
\newacronym{ml}{ML}{machine learning}
\newacronym{aa2}{(AA)2}{Ambient-Aware Application-Aligned}
\newacronym{tid}{TID}{Traveling Ionospheric Disturbance}
\newacronym{epb}{EPB}{Equatorial Plasma Bubbles}
\newacronym{gec}{GEC}{Global Electric Current}
\newacronym{spdf}{SPDF}{Space Physics Data Facility}
\newacronym{noaa}{NOAA}{National Oceanic and Atmospheric Administration}
\newacronym{swpc}{SWPC}{Space Weather Prediction Center}
\newacronym{nrcan}{NRCAN}{Natural Resources Canada}
\newacronym{gfz}{GFZ}{German Research Centre for Geosciences}
\newacronym{nasa}{NASA}{National Aeronautics and Space Administration}
\newacronym{euv}{EUV}{Extreme ultraviolet}

\renewcommand*{\acronymname}{List of abbreviations} 

\begin{document}

\title[A $\acrshort{dst}$-based space weather conditions \acrlong{ml} classification model for \acrshort{gnss} \acrshort{pnt} performance analysis]{A $\acrshort{dst}$-based space weather conditions \acrlong{ml} classification model for \acrshort{gnss} \acrshort{pnt} performance analysis}

\author*[1,2]{\fnm{Lucija} \sur{\v{Z}u\v{z}i\'{c}}}\email{lucija.zuzic@uniri.hr}
\equalcont{These authors contributed equally to this work.}

\author[1,2]{\fnm{Deni} \sur{Klen}}\email{deni.klen@uniri.hr}

\author[3]{\fnm{Teodor B.} \sur{Iliev}}\email{tiliev@uni-ruse.bg}

\author[1,2,4]{\fnm{Renato} \sur{Filjar}}\email{renato.filjar@uniri.hr}

\affil*[1]{\orgdiv{Department of Computer Engineering}, \orgname{Faculty of Engineering, University of Rijeka}, \orgaddress{\street{Vukovarska 58}, \city{Rijeka}, \postcode{51000}, \country{Croatia}}}

\affil[2]{\orgdiv{Center for Artificial Intelligence and Cybersecurity}, \orgname{University of Rijeka}, \orgaddress{\street{Radmile Matejcic 2}, \city{Rijeka}, \postcode{51000}, \country{Croatia}}}

\affil[3]{\orgdiv{Department of Telecommunication}, \orgname{University of Ruse}, \orgaddress{\street{8 Studentska str.}, \city{Ruse}, \postcode{7017}, \country{Bulgaria}}}

\affil[4]{\orgdiv{Laboratory for Spatial Intelligence}, \orgname{Hrvatsko Zagorje Krapina University of Applied Sciences}, \orgaddress{\street{Setaliste hrvatskog narodnog preporoda 6}, \city{Krapina}, \postcode{49000}, \country{Croatia}}}

\abstract{Ambient conditions classification enables systematic mitigation of adversarial effects on \acrfull{gnss} \acrfull{pnt} performance. This research contributes to the problem by proposing a classification model of space weather events for sub-equatorial regions. The proposed model uses \acrlong{ml}-based classification applied to the experimental observations of geomagnetic field components, observed \acrlong{tec} ($\acrshort{tec}$), and \acrlong{dst} ($\acrshort{dst}$-index). A \acrfull{svm} with a Polynomial Kernel, C5.0 \acrfull{dt}, \acrfull{nb}, shallow \acrfull{nn}, \acrfull{pls}, \acrfull{fda}, and shallow \acrshort{nn} using \acrfull{pca} was applied to develop the candidate model to classify observations of the geomagnetic field in $\acrshort{tec}$, combined with other variables, into one of the scenarios of space weather conditions. Performance is assessed using a confusion matrix and development time to yield the \acrshort{nb} as the best performer. The proposed $\acrshort{dst}$-based classification model serves as an indicator of a geomagnetic/ionospheric storm in progress, thus alerting \acrshort{gnss} users of potential degradation in \acrshort{gnss} \acrshort{pnt} performance and setting up a framework for the development of a tailored \acrshort{gnss} ionospheric correction model for specific classes of the space weather conditions.}

\keywords{\acrfull{gnss} \acrfull{pnt}, space weather conditions, \acrfull{ml}, classification model, \acrfull{dt}, \acrfull{nn}, sensor observations aggregation, geomagnetic field, \acrlong{tec} ($\acrshort{tec}$), \acrlong{dst} ($\acrshort{dst}$-index)}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}
%\label{sec:Intro}

The \acrfull{gnss} and its \acrfull{pnt} service have matured to become an essential part of national infrastructure, public goods, and enablers of a vast number of emerging technology and socio-economic applications \cite{spilker1996global}. Maintaining the \acrshort{gnss} \acrshort{pnt} quality is crucial for the sustainable development of modern economy and society \cite{schaer1999mapping}. Overcoming the shortcomings and vulnerabilities of \acrshort{gnss} \acrshort{pnt} is a scientific challenge, and the need of a wide variety of scientists, developers, operators, regulators, and users of \acrshort{gnss}-based systems and services \cite{durmaz2015regional, jin2012m_dcb}. The Earth’s ionosphere, a part of the Earth’s atmosphere stretching from $50$ $km$ to $2000$ $km$ above the Earth’s surface and comprised of rare but mostly charged atoms and molecules, is the major natural cause of \acrshort{gnss} \acrshort{pnt} degradation \cite{davies1990ionospheric, liu2009seismoionospheric}. This phenomenon especially affects most currently used \acrshort{gnss} receivers, which work as single-frequency receivers exposed to \acrshort{gnss} ionospheric effects \cite{spilker1996global, prolss2012physics}. Driven by powerful and random flows of energy and particles from the Sun (space weather conditions), the ionospheric conditions define the properties of \acrshort{gnss} signal propagation through the Earth’s atmosphere and the resulting ionospheric delay \cite{davies1990ionospheric, oxley2017uncertainties, prolss2012physics}. The \acrshort{gnss} ionospheric delay causes errors in \acrshort{gnss} \acrshort{pnt} algorithm deployment, designed to produce position, velocity, and time estimates for a \acrshort{gnss} receiver \cite{spilker1996global, schaer1999mapping}. The ionosphere affects \acrshort{gnss} satellite signals for position estimation by introducing signal propagation delay \cite{spilker1996global, schaer1999mapping}. The \acrshort{gnss} ionospheric delay is a stochastic variable, whose value is determined by complex physical processes of space weather \cite{spilker1996global, schaer1999mapping}. How space weather affects \acrshort{gnss} \acrshort{pnt} performance was explained using the Space weather – \acrshort{gnss} \acrshort{pnt} performance coupling model, as depicted in Figure~\ref{fig:SpaceWeather}.

\begin{figure}[!ht]
 \centering
 \includegraphics[width=0.89\linewidth]{SpaceWeather-TwoLines.drawio.pdf}
    \caption{The Space weather – \acrshort{gnss} \acrshort{pnt} performance coupling model.}
    \label{fig:SpaceWeather}
\end{figure}

The \acrshort{gnss} signal encounters a certain number of charged particles from the satellite aerial to a mobile unit’s (\acrshort{gnss} receiver’s) aerial \cite{schaer1999mapping, prolss2012physics}. This encounter is quantified using the \acrlong{tec} ($\acrshort{tec}$) defined by Equation~\ref{eqn:1} in $electrons/m^{2}$ \cite{spilker1996global, schaer1999mapping, davies1990ionospheric}, where $h$ denotes the height above the Earth’s mean sea level in $m$, $N(h)$ represents the vertical ionospheric profile, the volume density of charged particles at height $h$, in $electrons/m^{2}$. For reasons of convenience, $\acrshort{tec}$ may be expressed in $\acrshort{tec}$ Units ($TECU$), with $1$ $TECU$ = $1016$ $electrons/m^{2}$. The $\acrshort{tec}$ dataset used in this study was derived from \acrfull{rinex} $\acrshort{tec}$ observations using \acrfull{gps} $\acrshort{tec}$ software by Seemala \cite{SEEMALA202363}.

\begin{equation}
	\acrshort{tec} = \int_{lower \quad ionospheric \quad boundary}^{upper \quad ionospheric \quad boundary}N(h)dh
	\label{eqn:1}
\end{equation}

It should be noted that in the sense of Equation~\ref{eqn:1}, $\acrshort{tec}$ is defined as a result, a consequence, of the ionospheric conditions, and not their descriptor \cite{spilker1996global}. The \acrshort{gnss} ionospheric delay may be determined by derivation from the Appleton-Hartree equation, as given in Equation~\ref{eqn:2}, where $\Delta t_{iono}$ denotes the \acrshort{gnss} ionospheric delay in $s$, $c$ denotes the velocity of an electromagnetic wave in vacuum in $m/s$, and $f$ denotes the carrier wave frequency of the satellite signal in $Hz$ \cite{spilker1996global, schaer1999mapping}.

\begin{equation}
	\Delta t_{iono} = \frac{40.3}{c f^{2}} \int_{lower \quad ionospheric \quad boundary}^{upper \quad ionospheric \quad boundary}N(h)dh
	\label{eqn:2}
\end{equation}

Combining Equation~\ref{eqn:1} and Equation~\ref{eqn:2}, one can conclude the linear relation between $\Delta t_{iono}$ and $\acrshort{tec}$, as given in Equation~\ref{eqn:3} \cite{spilker1996global}.

\begin{equation}
	\Delta t_{iono} = \frac{40.3}{c f^{2}} \acrshort{tec}
	\label{eqn:3}
\end{equation}

The \acrshort{gnss} ionospheric delay has been identified as a source of \acrshort{gnss} \acrshort{pnt} degradation since the dawn of \acrshort{gnss}. \acrshort{gnss} systems offer various standard \acrshort{gnss} ionospheric delay estimation (correction) models to mitigate the deteriorating effects on \acrshort{gnss} \acrshort{pnt}, such as the Klobuchar model \cite{spilker1996global, klobuchar1987ionospheric}. The standard ionospheric correction models are global, and insufficiently flexible to update to mitigate \acrshort{gnss} ionospheric delay to satisfy rising demands on \acrshort{gnss} \acrshort{pnt} performance \cite{spilker1996global, enge1994global}. The development of regional and local models, such as the one presented in this paper, attempts to solve the problem of \acrshort{gnss} \acrshort{pnt} sustainable performance in various ionospheric conditions. Some of the authors of this paper recently proposed the \acrfull{aa2} \acrshort{pnt} to take into account the actual ionospheric and geomagnetic conditions near a mobile unit (a \acrshort{gnss} receiver) \cite{filjar2024ambient}. Direct measurements of the immediate geomagnetic and ionospheric condition variables may be supplied to a \acrlong{ml}-based adapted \acrshort{gnss} ionospheric correction model, thus solving the single-frequency \acrshort{gnss} problem. Previous research has identified predictors and target variables (descriptors of geomagnetic, ionospheric, and \acrshort{gnss} \acrshort{pnt} conditions) \cite{natras2022ensemble, natras2023regional}. Current space weather severity scales, such as the one provided by \acrfull{noaa}, are based on global space weather and geomagnetic indices averaged over a certain period (for instance, $3$ hours for the global $K_{p}$ index). The current space weather severity scales do not directly address classifying scenarios of \acrshort{gnss} performance deterioration and have limited potential in deployment for a \acrshort{gnss} ionospheric correction model. The classification of different scenarios of \acrshort{gnss} ionospheric conditions with adverse effects on the \acrshort{gnss} \acrshort{pnt} remained an unsolved precondition needed for the development of a \acrlong{ml}-based \acrshort{gnss} ionospheric delay correction model to render the \acrshort{gnss} \acrshort{pnt} algorithm ionospheric conditions-agnostic.

A methodology for a \acrlong{ml}-based classification of ionospheric conditions based entirely on observations of geomagnetic indices is described in this study. The model is sufficiently simple to be applied on computationally capable platforms with suitable geomagnetic field sensors, such as smartphones and connected/autonomous vehicles. The research presented acquires ambient data and analyses its statistical properties. The dataset is split into training and test sets. Several candidates for the \acrshort{gnss} ionospheric delay model are developed based on \acrlong{dst} ($\acrshort{dst}$) data taken from the INTERMAGNET \cite{Intermagnet2022-cj} dataset, and reformatted to match the format of $\acrshort{tec}$ data. The \acrfull{ml} models include a \acrfull{svm} with a Polynomial Kernel, C5.0 \acrfull{dt}, \acrfull{nb}, shallow \acrfull{nn}, \acrfull{pls}, \acrfull{fda} and shallow \acrfull{nn} using \acrfull{pca} of the input data. A tailored set of validation methods is used to assess their performance. The optimal \acrshort{gnss} ionospheric delay correction model is identified based on \acrshort{gnss} \acrshort{pnt}-related objective criteria, and its performance is demonstrated in an independent case study.

\section{Method and data}
\label{sec:Dataset}

A \acrfull{svm} with a Polynomial Kernel, C5.0 \acrfull{dt}, \acrfull{nb}, \acrfull{nn}, \acrfull{pls}, \acrfull{fda} and shallow \acrfull{nn} using \acrfull{pca} of the input data were tested based on their ability to classify a set of observations of the geomagnetic field in $\acrshort{tec}$, and other predictors, into one of the scenarios of space weather conditions based on $\acrshort{dst}$. Multiple $\acrshort{dst}$-dependent classes were predefined using theoretical knowledge. Statistical analysis of the data confirmed that distributions of other variables change for different $\acrshort{dst}$ ranges, supporting the validity of the classification. The study assumes that the dependent output variable, the $\acrshort{dst}$ class, can be predicted based on the independent variables used as input. $\acrshort{tec}$ data was obtained using \acrshort{gps} $\acrshort{tec}$ software by Seemala to process \acrshort{rinex} $\acrshort{tec}$ observations \cite{SEEMALA202363}. The INTERMAGNET \cite{Intermagnet2022-cj} dataset contains $\acrshort{dst}$ and $a_{p}$ data from 2014 for a measuring station maintained by Geoscience Australia in Kakadu, referred to as KDU in the database, at $12.69$ degrees of south latitude and $132.47$ degrees of east longitude near Darwin, Nothern Territory, Australia. The two datasets are merged based on location, year, month, day, and time of day in hours.

\subsection{Method}
%\label{subsec:Method}

The methods were selected because they represent larger families of classification methods. \acrshort{svm} models are supervised maximum margin models. \acrshort{dt} models also apply supervised learning. \acrshort{nb} classifiers are probabilistic classifiers that can be parametric or non-parametric, but this study uses a non-parametric approach. \acrshort{pls} is a non-parametric linear regression model. \acrshort{fda} uses multiple non-parametric linear regression models to create a non-linear classification. \acrshort{pca} is a linear dimensionality reduction technique that extracts a predefined number of components for training an \acrshort{nn} model. \acrshort{nn} models imitate the brain using artificial neurons to produce outputs based on the input and the activation function. \acrshort{nn} models require that the structure be predefined, and hyperparameters are tuned. All \acrshort{nn} models were applied based on research by Kuhn for the \textit{R} \textit{caret} package \cite{Kuhn2007, kuhn2008building, kuhn2013applied}.

\subsubsection{Support Vector Machine}
%\label{subsubsec:SupportVectorMachine}

In \acrlong{ml}, a \acrfull{svm} or \acrfull{svn} model is a supervised maximum margin model with associated learning algorithms used for classification. \acrshort{svm} models are also effective for non-linear classification using the hyperplane kernel trick \cite{Boser1992}. Intuitively, a good separation is achieved by the hyperplane with the greatest distance to the nearest point in the training data belonging to any class \cite{HastieRosset2009}. Meyer, Leisch, and Hornik compared \acrshort{svm} models with other classifiers \cite{Meyer2003}. However, it is unclear whether \acrshort{svm} predictions perform better than other linear models, such as logistic, and linear regression. To keep the computational burden reasonable, a kernel probability density function $k(x, y)$ is chosen to fit the problem \cite{Press2007}.

\subsubsection{Decision Tree}
%\label{subsubsec:DecisionTree}

\acrfull{dt} models are used for supervised learning in statistics and \acrlong{ml}. Classification trees use a discrete target variable. \acrshort{dt} models are popular due to their comprehensibility and simplicity \cite{Wu2008}. A tree is recursively partitioned by dividing the original set, or root node, into subsets that form descendants, or successors, using classification rules based on features \cite{ShalevShwartz2014}. C5.0, used in the \textit{caret} package in \textit{R}, has a similar approach and improves the ID3 and C4.5 algorithms.

\subsubsection{Naive Bayes}
%\label{subsubsec:NaiveBayes}

In statistics, \acrfull{nb} models, simple Bayes, or independent Bayes \cite{Hand2001} classifiers are a family of linear "probabilistic classifiers" that assume that, given a target class, the features are conditionally independent. Maximum likelihood training for \acrfull{nb} models evaluates a closed-form expression \cite{Russell1999} in linear time instead of using iterative approximation. However, a comprehensive comparison in 2006 showed that \acrfull{nb} models performed worse than boosted trees or \acrfull{rf} models \cite{Caruana2006}. An advantage of \acrshort{nb} over other models is a smaller amount of required training data \cite{John2013}. \acrshort{nb} models assign probabilities $p(C_{k}\mid x_{1},\ldots, x_{n})$ to $K$ classes $C_{k}$ for an input vector $x = (x_{1},\ldots, x_{n})$ with $n$ features \cite{Murty2011}, and use Bayes' theorem in Equation~\ref{eqn:4}.

\begin{equation}
	p(C_{k}\mid \mathbf{x})={\frac{p(C_{k})\ p(\mathbf{x} \mid C_{k})}{p(\mathbf{x})}}
	\label{eqn:4}
\end{equation}

\subsubsection{Neural Networks}
%\label{NeuralNetworks}

The neurons of human or animal brains provide the basis for a \acrfull{nn} or \acrfull{ann} with connected units or nodes called artificial neurons in \acrlong{ml} \cite{brahme2014comprehensive}. Shallow \acrshort{nn} models typically contain only a few hidden layers for processing between the input layer that receives the data and the final layer that produces the output \cite{olden2002illuminating}. A network with at least two hidden layers \cite{bishop2006pattern} is a deep \acrshort{nn} model. Gradient-based methods such as backpropagation estimate \acrshort{ann} parameters \cite{vapnik2013nature} to minimize the difference or empirical risk between the output and target labels, expressed in a loss function \cite{goodfellow2016deep}. The hyperparameters may also be modified to suit the problem \cite{probst2019tunability} during an extensive tuning process, like the one used in this study. \acrfull{pca} \cite{stewart2019introduction} is a linear dimensionality reduction technique in exploratory data analysis, visualization \cite{jolliffe2016principal}, and preprocessing. The $pcaNNet$ method in the \textit{caret} package in \textit{R} uses \acrshort{pca} in preprocessing \cite{Kuhn2007, kuhn2008building, kuhn2013applied}.

\subsubsection{Partial Least Squares}
%\label{subsubsec:PLS}

\acrfull{pls} regression, or projection to latent structures, \cite{abdi2010partial}, is a linear regression statistical model that transforms the predicted and the observable variables to a new space. \acrshort{pls} methods are bilinear factor models because the $X$ and $Y$ are projected to new spaces. In \acrfull{pls-da}, $Y$ is categorical \cite{saebo2008lpls}. Using $n$ paired observations $\left(\vec{x_{i}}, \vec{y_{i}}\right), i \in 1, \dots, n$. \acrshort{pls} finds the normalized direction $\vec{p_{j}}, \vec{q_{j}}$ that maximizes the covariance in the first step $j = 1$, shown in Equation~\ref{eqn:5}. Many versions of \acrshort{pls} exist for estimating the factor and loading matrices, such as the PLS1 algorithm \cite{GONZALEZ2023104876}.

\begin{equation}
	\max_{{\vec{p}}_{j},{\vec{q}}_{j}}\operatorname{E} [\underbrace{({\vec{p}}_{j}\cdot {\vec{X}})}_{t_{j}}\underbrace{({\vec{q}}_{j}\cdot {\vec{Y}})}_{u_{j}}
	\label{eqn:5}
\end{equation}

\subsubsection{Flexible Discriminant Analysis}
%\label{subsubsec:FDA}

\acrfull{fda} is a general methodology that creates the discriminant surface for a multigroup non-linear classification model \cite{mclachlan2005discriminant} based on a mixture of non-parametric linear regression models, such as \acrfull{mars} and \acrfull{lda}. Many predictors can be used in conjunction in \acrshort{fda} \cite{HastieTibshirani2009}. \acrshort{fda} is complex but execution time and computational load are adequate \cite{reynes2006choice}. Feature normality and equal group covariances are assumed \cite{wetcher2011analyzing}. \acrshort{lda}, \acrfull{nda}, or discriminant function analysis \cite{cohen2013applied} is a generalization of Fisher's linear discriminant defined in 1936 \cite{mclachlan2005discriminant}. The results of \acrshort{lda} may be utilized directly for classification, as demonstrated in this experiment.

\subsection{Data description and analysis}
%\label{subsec:Data}

Dynamic space weather conditions, such as solar activity and geomagnetic storms, can affect  \acrshort{gnss} \acrshort{pnt} performance and high-frequency \acrshort{gps} signals passing through the ionosphere, motivating work on error modeling \cite{zolesi2014ionospheric}. Geomagnetic storms cause signal deterioration by affecting \acrfull{gec} variability. The ionosphere may show changes related to location, geomagnetic and solar activity, sunspots, local time, seasonality, thunderstorms \cite{vellinov1992ionospheric}, nuclear experiments, earthquakes \cite{liu20142013}, and other phenomena. This study focuses on parameters describing disturbances of the Earth's geomagnetic field, most importantly $a_{p}$-indices derived from $K_{p}$-indices, which are calculated using $K$-indices, \acrlong{tec} ($\acrshort{tec}$), \acrlong{dtec} ($\acrshort{dtec}$), and \acrlong{dst} ($\acrshort{dst}$). Incorporating parameters such as the $K_{p}$-indices and $a_{p}$-indices, which provide global measures of geomagnetic activity, alongside local $\acrshort{tec}$ and $\acrshort{dst}$-index values, allows for a more detailed assessment of the space environment and its potential effects on \acrshort{gnss} signals. Values of $a_{p}$, $\acrshort{tec}$, $\acrshort{dtec}$, $\acrshort{dst}$ were used with the $B_x$, $B_y$, and $B_z$ components of the Earth's magnetic field to train machine-learning models.

\subsubsection{Magnetic field indices}
%\label{subsec:BxByBz}

The Earth's magnetic field has similarities to that of a bar magnet. However, plasma gushes from the solar corona and the domain of the Sun influence the interplanetary magnetic field \cite{schwenn2001solar}. The $B_{x}$, $B_{y}$, and $B_{z}$ vectors represent interplanetary magnetic field indices. $B_{x}$ and $B_{y}$ are parallel to the plane of orbits, and the third component $B_{z}$ is perpendicular. Widely available hand-held devices, such as Android smartphones \cite{Bojinov2014}, measure magnetic field indices in micro-Tesla ($\mu T$). The Android magnetometer reports accuracy through a status variable. Readings are calibrated using temperature compensation, factory (or online) soft-iron, and online hard-iron calibration.
 
\subsubsection{Geomagnetic storm indices}
%\label{subsec:K}

The geomagnetic storm $K$-index is an integer from $0$ to $9$ measuring disturbances in global geomagnetic activity. The maximum positive and negative fluctuations of the horizontal components of the Earth's magnetic field, $B_{x}$ and $B_{y}$, during $3$ hours, relative to a quiet day, are added to determine the total maximum fluctuation. Each observatory uses different threshold values to convert the maximum $nT$ (nano-Tesla) fluctuation to a $K$-index value. The thresholds for each observatory are adjusted so that the historical rate of occurrence for each $K$-index value is similar across all observatories. Observatories with a lower geomagnetic latitude use a lower fluctuation in $B_{x}$ and $B_{y}$ to achieve each $K$-index value.

\subsubsection{Planetary geomagnetic storm indices}
%\label{subsec:Kp}

The planetary geomagnetic storm $K_{p}$-index is derived from $3$-hour-based $K$-indices from $13$ magnetometer stations between $44$ and $60$ degrees of north and south latitude. Announcements and warnings of geomagnetic changes and disturbances in the Earth's magnetic field are based on the $K_{p}$-index. Hourly geomagnetic storm index data are available on the \acrshort{nasa} Goddard \acrshort{spdf} web pages. The scale values of the $K_{p}$-index are determined by the change of the geomagnetic field and the geomagnetic storm effect in $nT$. The official planetary $K_{p}$-index is a weighted average of $K$-indices from multiple observatories. When $K$-index data is not available in real-time, operators such as The \acrfull{noaa} \acrfull{swpc} calculate near real-time estimates of the $K_{p}$-index \cite{Myint2022}. The $K_{p}$-index is related to geomagnetic storm descriptions and warnings using the \acrshort{noaa} G scale. For a $K_{p}$-index value of $5$, $6$, $7$, $8$ (including $9_{-}$), and $9$, a \acrshort{noaa} Space Weather Scale Geomagnetic Storm Level of G1, G2, G3, G4, and G5 is assigned, respectively. For a $K_{p}$-index $<5$, the G0 designation means no warning is issued. In March 2021, $K_{p}$ was assigned a \acrfull{doi} with a dataset \cite{Matzka2021a} and a scientific publication \cite{Matzka2021b} for reference.

\subsubsection{Equivalent three hourly range geomagnetic storm indices}
%\label{subsec:A}

The $A$-index represents a daily average level of magnetic activity. Because the relationship between the $K$-index and magnetometer fluctuations is not linear, the $K$-index values are not directly used for calculating average values. Each $K$-index or $K_{p}$-index is converted into the "equivalent three hourly range" $a$-index or $a_{p}$-index that uses a linear scale. An average of $8$ $a$-indices (lowercase) is used as the daily $A$-index (uppercase).  

\subsubsection{Disturbance Storm-Time}
%\label{subsubsec:Dst}

\acrlong{dst} ($\acrshort{dst}$), also known as the geomagnetic activity $\acrshort{dst}$-index, depicts an averaged measure of the geomagnetic storm intensity in the Earth's sub-equatorial region. The $\acrshort{dst}$-index is obtained by post-processing, with the final version often published months after experimental observations were collected. It clearly shows developments of various levels of geomagnetic disturbances, and consequently serves as invaluable input considering \acrshort{gnss} \acrshort{pnt} performance degradation due to space weather events and disturbances. The $\acrshort{dst}$-index is a geomagnetic indicator of magnetic flux changes derived from measurements taken by a network of ground-based magnetometer stations near the magnetic equator, which continuously monitor $B_{x}$ and $B_{y}$, the horizontal components of Earth's magnetic field \cite{zolesi2014ionospheric}. The $\acrshort{dst}$-index describes ring currents forming above the sub-equatorial region and affecting the ionospheric regions in mid-latitudes. To calculate the $\acrshort{dst}$-index, variations in $B_{x}$ and $B_{y}$, the horizontal magnetic field, are obtained from multiple stations and averaged. The average is subtracted from a baseline value representing the quiet-time magnetic field. The resulting $\acrshort{dst}$ value in $nT$ measures the intensity of geomagnetic disturbances, with increasingly negative values indicating stronger geomagnetic storms. The $\acrshort{dst}$-indice measurements as an hourly average were evaluated and published on a web interface by the \acrshort{nasa} Goddard \acrshort{spdf}, and the Geomagnetism and Space Magnetism Data Analysis Center of the Institute of Science, Kyoto University in Japan. Loewe and Prölss \cite{loewe1997classification} classified magnetic activity $\acrshort{dst}$-indices into $5$ storm classes in 1997. Gonzalez et al. \cite{gonzalez1994geomagnetic} used $3$ groups for the same data in 1994, similar to Kamide et al. in 1998 \cite{kamide1998two}, Rozhnoi et al. in 2004 \cite{rozhnoi2004middle}, and Contadakis et al. in 2012 \cite{contadakis2012total}.

\subsubsection{Training and testing dataset}
%\label{subsubsec:DataTrainTest}

The original set of observations has been split into training and testing subsets, considering the results of exploratory statistical, and outlier analysis, and sustaining proportions of data relating to classes of geomagnetic disturbances (quiet/normal geomagnetic conditions, the positive phase of a geomagnetic storm, deep negative depression of a geomagnetic storm, and the negative recovery phase of a geomagnetic storm). Classes are based on $\acrshort{dst}$ and differential $\acrshort{dst}$ time series. Out of $9799$ original samples, $208$ were marked as outliers in exploratory statistical analysis, as the $\acrshort{tec}$ is larger than or equal to $300$ $TECU$. $\acrshort{tec}$ values larger than $300$ $TECU$ usually appear due to measurement errors or errors in the $\acrshort{tec}$ estimation process. The remaining $9591$ samples are used for training and testing. The samples are divided into training and testing datasets as close as possible to a ratio of $80\%$ for training and $20\%$ for testing. The division was stratified so that an approximately equal ratio of classes was present in both the training and testing data, which is a feature of the \textit{createDataPartition} function from the \textit{caret} \textit{R} library that was used \cite{Kuhn2007, kuhn2008building}. Samples were split into $5$ class ranges, based on $\acrshort{dst}$ values derived from theoretical knowledge of different storm phases, similar to Loewe and Prölss \cite{loewe1997classification}. Table~\ref{tab:Dstranges} lists the $\acrshort{dst}$ class ranges used in this study, the total number of samples in each class, and the number of samples used for testing, and training.

\begin{table}[!ht]
    \centering
    \caption{$\acrshort{dst}$-based classification rules used in this study, the total number of samples in each class, and the number of samples used for testing, and training. The upper range limits are excluded, while the lower ones are included.}
    \label{tab:Dstranges}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        $\acrshort{dst}$ & Storm phase classification & Total samples & Test samples & Train samples \\ \hline
        $ \geq 15 $ & positive phase (P) & $135$ & $27$ & $108$ \\ \hline
        $[-20, 15>$ & normal (N) & $7495$ & $1499$ & $5996$ \\ \hline
        $[-55, -20>$ & recovery phase (R) & $1858$ & $371$ & $1487$ \\ \hline
        $[-85, -55>$ & through (T) & $90$ & $18$ & $72$ \\ \hline
        $ < -85$ & extreme (E) & $13$ & $2$ & $11$ \\ \hline
        Any & Any & $95$ & $2$ & $11$ \\ \hline
    \end{tabular}
\end{table}

It is evident from Table~\ref{tab:Dstranges} that the normal (N), and recovery (R) classes are more common than those with very high or low $\acrshort{dst}$ values, impacting model performance.

\subsubsection{Data preprocessing}
%\label{subsubsec:Preprocessing}

Data preprocessing can increase classification accuracy \cite{Fan2008}. There are many ways to standardize data, such as minimum-maximum, normalization by decimal scaling, and Z-score \cite{Mohamad2013}. Subtracting the mean and dividing by the variance for each feature are commonly used for \acrshort{svm} models \cite{Fennell2019} and other models tested in this study, so this approach was chosen. The values \textit{scale} and \textit{center} were used in this study in the \textit{preProcess} parameter for the \textit{train} function from the \textit{R} \textit{caret} package. The option \textit{center} subtracts the mean of each feature while \textit{scale} divides by the standard deviation.

\subsubsection{Distribution analysis for predictors}
%\label{subsubsec:Distribution}

Table~\ref{tab:minmax} provides the minimum, $1^{st}$ quartile, median, arithmetic mean, $3^{rd}$ quartile, and maximum values for all variables when the $\acrshort{tec}$ is less than $300$ $TECU$, suggesting that variables are not normally distributed.

\begin{table}[!ht]
    \centering
    \caption{The minimum, $1^{st}$ quartile, median, arithmetic mean, $3^{rd}$ quartile, and maximum values for all variables when the $\acrshort{tec}$ is less than $300$ $TECU$.}
    \label{tab:minmax}
        \begin{tabular}{|c|c|c|c|c|c|c|c|}
                \hline
                  & $\acrshort{tec}$ & $\acrshort{dtec}$ & $B_{x}$ & $B_{y}$ & $B_{z}$ & $\acrshort{dst}$ & $a_{p}$ \\ \hline
                Min. & $1.76$ & $0.02$ & $35275.0$ & $1913.0$ & $-29649.0$ & $-119.0$ & $0.0$ \\ \hline
                $1^{st}$ Qu. & $8.43$ & $1.12$ & $35402.0$ & $1990.0$ & $-29623.0$ & $-18.0$ & $4.0$ \\ \hline
                Median & $22.83$ & $2.47$ & $35410.0$ & $2004.0$ & $-29620.0$ & $-5.0$ & $6.0$ \\ \hline
                Mean & $24.62$ & $6.208$ & $35413.0$ & $2003.0$ & $-29617.0$ & $-9.678$ & $7.539$ \\ \hline
                $3^{rd}$ Qu. & $34.23$ & $5.54$ & $35423.0$ & $2018.0$ & $-29614.0$ & $-1.0$ & $7.0$ \\ \hline
                Max. & $288.0$ & $997.0$ & $35524.0$ & $2089.0$ & $-29571.0$ & $46.0$ & $94.0$ \\ \hline
        \end{tabular}
\end{table}

The Kolmogorov-Smirnov and Shapiro-Wilk normality tests, using \textit{R} functions \textit{ks.test} and \textit{shapiro.test}, did not yield a $p$-value larger than the selected $\alpha$-value of $0.05$ for any variable, further strengthening the claim based on Table~\ref{tab:minmax} that variables do not follow a normal (Gaussian) distribution.
  
\subsubsection{Correlation analysis for predictors}
%\label{subsubsec:Correlation}

Observations of statistical variables were assessed for their mutual association/correlation to identify the classification model structure, potential predictors, and targets. Figure~\ref{fig:correlation} contains a heat map of the correlation between all variables used in the study.

\begin{figure}[!ht]
 \centering
 \includegraphics[width=0.7\linewidth]{iono3correlation.pdf}
    \caption{A heat map of the correlation between all variables used in this study, when the $\acrshort{tec}$ is less than $300$ $TECU$. Red represents a high positive correlation, blue represents a high negative correlation, and white represents a low correlation. Variables are fully correlated with themselves, so values on the secondary diagonal equal $1$. The matrix is symmetrical concerning the secondary diagonal because the same combination of correlated variables is achieved when swapping the row and column indices.}
    \label{fig:correlation}
\end{figure}

The $B_{x}$ and $\acrshort{dst}$ variables exhibit the largest correlation coefficient in Figure~\ref{fig:correlation}, equaling $0.52$. The association between the two variables seems logical since the $\acrshort{dst}$ index is defined as a result of geomagnetic field conditions described by geomagnetic field components. The second largest correlation coefficient depicts a significant correlation between $a_{p}$ and $\acrshort{dst}$ and equals $-0.48$, thus confirming that the $a_{p}$ variable can be used as a predictor, with the $\acrshort{dst}$ variable as a target, and an opposite trend.

The box plots of all variables for different ranges of $\acrshort{dst}$ values in Figure~\ref{fig:iono3boxplot} are used to support the correlations shown in Figure~\ref{fig:correlation} by exhibiting the trend of each variable.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{iono3boxplot_fix.pdf}
    \caption{Box plots of all variables, when the $\acrshort{tec}$ is less than $300$ $TECU$, for different ranges of $\acrshort{dst}$ values defining the class label used in this study.}
    \label{fig:iono3boxplot}
\end{figure}

Figure~\ref{fig:iono3boxplot} shows the minimum, maximum, and arithmetic mean of $a_{p}$ decreasing for larger $\acrshort{dst}$ values. The opposite is true for $B_{x}$, as indicated by a high correlation in Figure~\ref{fig:correlation}. $B_{y}$ exhibits a reverse trend compared to $B_{x}$, but it is less prominent. $B_{z}$ is the most stable among geomagnetic indices, with the smallest changes related to $\acrshort{dst}$.

\subsection{Confusion Matrix}
%\label{subsec:Metrics}

The metrics and terminology defined in the \textit{R} function \textit{confusionMatrix} in the \textit{caret} library \cite{Kuhn2007, kuhn2008building} were used to evaluate classifier performance. The default approach to a confusion matrix uses only two groups (Yes and No, positive and negative). For multiple classes, results are calculated by a "one versus all" approach, viewing each class as positive and all others as negative.

\subsubsection{McNemar-Bowker test}
%\label{subsubsec:McNemar}

McNemar’s test for correlated proportions used on paired categorical data is based on the chi-squared distribution and was originally designed for methods that differentiate between two classes. The null hypothesis of marginal homogeneity states that marginal probabilities for each outcome are equal, which is more indicative of model difference than directly comparing the sensitivity and specificity of two candidate models.

However, for $k$ groups, the classification can be annotated
in a $k \times k$ contingency table with the number of samples classified using the first method in separate rows by class, divided into columns depending on the class assigned by the opposing method. The McNemar-Bowker test is applied to this table, formulated by Fagerland et al. \cite{fagerland2017statistical}, and Chow et al. \cite{chow2018sample}.

If any element in the matrix is smaller than $6$, as shown using class sizes in Table~\ref{tab:Dstranges}, the distribution is not well-approximated by the chi-squared distribution. Edwards \cite{edwards1948note} developed an approximation of the binomial exact $p$-value for continuity-correction, given in Equation~\ref{eqn:6} for two groups. In Equation~\ref{eqn:6}, $b$ is the number of samples classified in the first group by the first test, and the second group by the second test, while $c$ is the number of samples for which the opposite is true.
 
\begin{equation}
    \chi^{2}=\frac{(|b-c|-1)^{2}}{b+c}
    \label{eqn:6}
\end{equation}

\section{Research results}
\label{sec:Results}

Candidate models were assessed to determine the optimal method and set of predictors to be used in the final model for application in real settings. The accuracy and the execution time in seconds ($s$) utilizing the \textit{R} \textit{system.time} function for all candidate models are displayed in Table~\ref{tab:acc:time}. Research results are presented with the following initial set of six predictors: \acrlong{tec} ($\acrshort{tec}$), \acrlong{dtec} ($\acrshort{dtec}$), $B_{x}$, $B_{y}$, $B_{z}$, and $a_{p}$. $\acrshort{dst}$ was not used as a predictor since the initial classification is derived from $\acrshort{dst}$ values. Candidate models have also been developed using: (1.): all predictors except $\acrshort{tec}$, and $\acrshort{dtec}$ ($B_{x}$, $B_{y}$, $B_{z}$, and $a_{p}$), (2.): Geomagnetic indices ($B_{x}$, $B_{y}$, and $B_{z}$), (3.): $B_{x}$, $B_{y}$, and $a_{p}$, (4.): $B_{x}$, $B_{z}$, and $a_{p}$, and (5.): $B_{y}$, $B_{z}$, and $a_{p}$. The reasoning behind this approach was the assumption that reducing the set of predictors would reduce model complexity and computation time. Additionally, it is theoretically established that geomagnetic indices $B_{x}$, $B_{y}$, and $B_{z}$ affect $\acrshort{dst}$, and in turn $\acrshort{tec}$. This is why geomagnetic indices ($B_{x}$, $B_{y}$, and $B_{z}$), and $a_{p}$ were used as predictors in the final model. The highest accuracy in Table~\ref{tab:acc:time} was achieved using $B_{x}$, $B_{y}$, $B_{z}$, and $a_{p}$ as predictors, supporting the hypothesis that $\acrshort{tec}$, and $\acrshort{dtec}$ should be removed from the set of predictors. The \acrfull{nb} method yielded the model with the highest accuracy, leading to the decision that it should be used in the final model.

\begin{table}[!ht]
    \centering
    \caption{The accuracy (top) and the execution time in seconds ($s$) (bottom) for each candidate model developed using different methods and sets of predictors.}
	\label{tab:acc:time}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
        Candidate & \multicolumn{6}{|c|}{Accuracy} \\ \cline{2-7}
        model & \multicolumn{6}{|c|}{Predictors} \\ \hline
		Method & All & $B_{x}$, $B_{y}$, $B_{z}$, $a_{p}$ & $B_{x}$, $B_{y}$, $B_{z}$ & $B_{x}$ $B_{y}$ $a_{p}$ & $B_{x}$ $B_{z}$ $a_{p}$ & $B_{y}$ $B_{z}$ $a_{p}$ \\ \hline
		\acrshort{svm} Poly & $0.7454$ & $0.7903$ & $0.7887$ & $0.7934$ & $0.7903$ & $0.7814$ \\ \hline
		C5.0 \acrshort{dt} & $0.8529$ & $0.8581$ & $0.8404$ & $0.8508$ & $0.8367$ & $0.8211$ \\ \hline
		\acrshort{nb} & $0.9943$ & $1$ & $1$ & $1$ & $1$ & $0.9995$ \\ \hline
		\acrshort{nn} & $1$ & $1$ & $1$ & $1$ & $1$ & $1$ \\ \hline
		\acrshort{pls} & $0.8258$ & $0.8232$ & $0.8174$ & $0.8153$ & $0.8143$ & $0.8007$ \\ \hline
		\acrshort{fda} & $0.8472$ & $0.8472$ & $0.8252$ & $0.8419$ & $0.8258$ & $0.7997$ \\ \hline
		\acrshort{pca} \acrshort{nn} & $0.8492$ & $0.8492$ & $0.8399$ & $0.8456$ & $0.8367$ & $0.8185$ \\ \hline
        Candidate & \multicolumn{6}{|c|}{Execution time in seconds ($s$)} \\ \cline{2-7}
        model & \multicolumn{6}{|c|}{Predictors} \\ \hline
		Method & All & $B_{x}$, $B_{y}$, $B_{z}$, $a_{p}$ & $B_{x}$, $B_{y}$, $B_{z}$ & $B_{x}$ $B_{y}$ $a_{p}$ & $B_{x}$ $B_{z}$ $a_{p}$ & $B_{y}$ $B_{z}$ $a_{p}$ \\ \hline
		\acrshort{svm} Poly & $337.38$ & $391.42$ & $565.11$ & $727.45$ & $649.28$ & $856.54$ \\ \hline
		C5.0 \acrshort{dt} & $681.36$ & $470.88$ & $350.02$ & $360.72$ & $333.90$ & $388.50$ \\ \hline
		\acrshort{nb} & $207.13$ & $170.22$ & $145.93$ & $149.69$ & $151.67$ & $155.84$ \\ \hline
		\acrshort{nn} & $630.47$ & $617.97$ & $590.97$ & $616.92$ & $632.41$ & $651.09$ \\ \hline
		\acrshort{pls} & $61.29$ & $52.75$ & $36.47$ & $51.37$ & $41.37$ & $59.63$ \\ \hline
		\acrshort{fda} & $92.49$ & $135.62$ & $141.52$ & $159.61$ & $167.59$ & $187.72$ \\ \hline
		\acrshort{pca} \acrshort{nn} & $420.25$ & $460.76$ & $449.86$ & $475.48$ & $491.82$ & $524.88$ \\ \hline
	\end{tabular}
\end{table}

The experiment was run on \textit{Windows} 11 using \textit{R Studio} version 2024.04.2+764 and \textit{R} version 4.4.1, the AMD Radeon RX 6600 \acrfull{gpu}, $16$ GB of \acrfull{ram}, and the AMD Ryzen 5 PRO 4650G \acrfull{cpu} with $6$ cores. Execution time is significant because built-in systems for mobile devices using \acrshort{gnss} \acrshort{pnt} have low computational capabilities. The candidate models using the \acrfull{pls} method in Table~\ref{tab:acc:time} have the lowest execution time.

The performance assessment of candidate models revealed the success of several methods, such as the \acrfull{nn} and \acrfull{nb} classifiers. Accuracy results in Table~\ref{tab:acc:time} may lead to an incorrect conclusion as the dataset is unbalanced, as shown in Table~\ref{tab:Dstranges}. Further assessment is needed to determine if a candidate model differs significantly from others. The results of
McNemar’s test are presented only for candidate models using either the \acrfull{nb} method or $B_{x}$, $B_{y}$, $B_{z}$, and $a_{p}$ as predictors variables, as this combination presented as the most accurate among tested candidate models and has a lower computational load than competing models.

\subsection{Results of McNemar's test}
%\label{subsec:ResultsMcNemar}

McNemar’s test can help assess if a candidate model performs significantly worse or better than others. Figure~\ref{fig:pvalueplot} contains $p$-values of McNemar's test when comparing candidate models using the \acrfull{nb} method and various predictor variable sets, or $B_{x}$, $B_{y}$, $B_{z}$, and $a_{p}$ as predictor variables combined with various classification methods. A higher $p$-value indicates that the marginal probabilities for classification outcomes are unequal for a pair of candidate models, and the null hypothesis can be rejected. A lower $p$-value indicates that the marginal probabilities for classification outcomes are the same for a pair of candidate models, and the null hypothesis cannot be rejected.

\begin{figure}[!ht]
 \centering
    \includegraphics[width=\linewidth]{pvalueplot_total_horizontal.pdf}
    \caption{Candidate model comparison using $p$-values of McNemar's test, and $B_{x}$, $B_{y}$, $B_{z}$, and $a_{p}$ as predictor variables (excluding \acrlong{tec} ($\acrshort{tec}$), and \acrlong{dtec} ($\acrshort{dtec}$)) combined with various classification methods (left), or the \acrfull{nb} method combined with various predictor variable sets (right). Black represents a low $p$-value near $0$, white represents a high $p$-value near $1$, and red represents a $p$-value near $0.5$ between the two extremes. Each candidate model is equal to itself, so values on the primary diagonal equal $1$. The matrix is symmetrical concerning the primary diagonal because the same result is achieved when swapping the order of the first and second compared candidate models.}
    \label{fig:pvalueplot}
\end{figure}

The marginal probabilities for each outcome are different when comparing the candidate model using the \acrshort{nb} method, and $B_{x}$, $B_{y}$, $B_{z}$, and $a_{p}$ predictor variables, to the models using the same method, and the full set of predictors (\acrlong{tec} ($\acrshort{tec}$), \acrlong{dtec} ($\acrshort{dtec}$), $B_{x}$, $B_{y}$, $B_{z}$, and $a_{p}$), or $B_{y}$, $B_{z}$, and $a_{p}$ as predictors.

This was concluded by rejecting the null hypothesis using $p$-values of McNemar's test that equal $1$, as given in Figure~\ref{fig:pvalueplot}. The $p$-values support the conclusion based on accuracy values, which are highest when using $B_{x}$, $B_{y}$, $B_{z}$, and $a_{p}$. This adds validity to removing $\acrshort{tec}$, and $\acrshort{dtec}$ from the set of predictors.

The marginal probabilities for each outcome are different when comparing the candidate model using $B_{x}$, $B_{y}$, $B_{z}$, and $a_{p}$ as predictor variables, and the \acrshort{nb} method, to the models using the same set of predictors, and the \acrshort{svm} method with a Polynomial Kernel, or the \acrshort{fda} method. The $p$-values of McNemar's test that equal $1$, as shown in Figure~\ref{fig:pvalueplot}, add validity to accuracy comparison results, which indicate a lower accuracy when using a method other than \acrshort{nb} and predictors other than $B_{x}$, $B_{y}$, $B_{z}$, and $a_{p}$.

\section{Discussion}
\label{sec:Discussion}

The \acrfull{svm} method with a Polynomial Kernel, C5.0 \acrfull{dt}, \acrfull{nn}, and \acrshort{nn} method with \acrfull{pca} applied in preprocessing have the highest execution time, above $300$ $s$, so they were excluded from further analysis and application in the final model.

The \acrfull{nn} method has a high execution time for any set of predictors due to its complexity and extensive training, evident from the data in Table~\ref{tab:acc:time}. However, it achieved a $100\%$ accuracy, also shown in Table~\ref{tab:acc:time}. The \acrfull{nb} method has the same accuracy when using $B_{x}$, $B_{y}$, $B_{z}$, and $a_{p}$ as predictors, and the training time for the \acrshort{nn} model is more than twice as long, so the \acrshort{nb} method is preferred as the approach in the final model.

An analysis in 2004 showed reasonable theoretical reasons for the seemingly incredible performance of \acrshort{nb} classifiers \cite{Zhang2004}. The expression used for the \acrshort{nb} method is a linear time algorithm if time complexity is expressed as a function of the size of the input and observing asymptotic behavior, explaining the reduced execution time \cite{Russell1999}. Despite their simplicity, \acrshort{nb} classifier models have performed well in real-world situations \cite{Metsis2006}, even with a relative lack of data compared to other approaches \cite{John2013, Mccallum2001}.

All candidate models developed using methods other than \acrshort{nn} and \acrshort{nb} fail to achieve accuracy over $90\%$. This indicates that they are less suitable for this particular application. The \acrshort{svm} method with a Polynomial Kernel is consistently the worst-performing for any set of predictors, never achieving an accuracy over $80\%$. All models achieved an accuracy over $70\%$, suggesting they could indicate \acrfull{gnss} \acrfull{pnt} performance.

\section{Conclusion}
\label{sec:Conclusion}

The presented study aims to classify ambient conditions of space weather events for sub-equatorial regions. \acrfull{gnss} \acrfull{pnt} performance is significantly affected by such events. It would be beneficial to warn users of a geomagnetic/ionospheric storm.

Classification models using \acrlong{ml} were applied to descriptions of the geomagnetic field expressed in \acrlong{tec} ($\acrshort{tec}$), \acrlong{dtec} ($\acrshort{dtec}$), $B_{x}$, $B_{y}$, $B_{z}$ (geomagnetic field indices), and $a_{p}$. It was assumed observations contained independent variables to generate the dependent variable representing the \acrlong{dst} ($\acrshort{dst}$) class. 

Statistical analysis confirmed that other variables change distribution based on $\acrshort{dst}$, not $\acrshort{tec}$. Continuous $\acrshort{dst}$ values in different ranges were converted into discrete classes based on statistics, previous theories, and research. 

An \acrfull{svm} with a Polynomial Kernel, C5.0 \acrfull{dt}, \acrfull{nb}, \acrfull{nn}, \acrfull{pls}, \acrfull{fda}, and \acrfull{pca} \acrshort{nn} model created a $\acrshort{dst}$-based classification from multiple combinations of input variables.

The \acrshort{nb} method using $B_{x}$, $B_{y}$, $B_{z}$, and $a_{p}$ as predictors achieved perfect accuracy on the test set. The total execution time is at least two times shorter than for the \acrshort{nn} method, making it more suitable for compact, low-performance, and low-cost portable devices such as smartphones. The exclusion of $\acrshort{tec}$ and $\acrshort{dtec}$ is supported by theory since they contain redundant information already presented by $B_{x}$, $B_{y}$, and $B_{z}$ which impact $\acrshort{dst}$, and in turn  $\acrshort{tec}$.

\clearpage

\printglossary[type=\acronymtype]

\section{Declarations}
%\label{sec:Declarations}

\subsection{Availability of data and materials}
%\label{subsec:Availability}

The datasets used during the current study are available from the corresponding author upon reasonable request.

\subsection{Competing interests}
%\label{subsec:Competing}

The authors declare no conflict of interest.

\subsection{Funding}
%\label{subsec:Funding}

The authors have no funding sources to declare.

\subsection{Authors' contributions}
%\label{subsec:Contributions}

L\v{Z} contributed to conceptualization, methodology, software, validation, formal analysis, investigation, data curation, original draft writing, text review and editing, and visualization. DK contributed to conceptualization, methodology, investigation, original draft writing, and text review and editing. TI contributed to validation, formal analysis, investigation, text review and editing, supervision, and project administration. RF contributed to conceptualization, methodology, investigation, resources, original draft writing, text review and editing, and supervision. All authors read and approved the final manuscript.

\subsection{Acknowledgements}
%\label{subsec:Acknowledgements}

Not applicable

\subsection{Authors' information}
%\label{subsec:Information}

L\v{Z} is a doctoral student at the Faculty of Engineering, University of Rijeka, currently employed as an assistant at the Department of Computer Science. Her research interests include applied \acrlong{ml} in biology, chemistry, medicine, and transportation. Maritime transportation is also the subject of her doctoral research.

DK is a doctoral student at the Faculty of Engineering, University of Rijeka, currently employed as an assistant at the Department of Computer Science. His research interests include applied \acrlong{ml} in object recognition and image segmentation. Image classification and labeling are also the subject of his doctoral research.

TI earned his MSc and PhD in telecommunication engineering from the University of Ruse “Angel Kanchev,” Ruse, Bulgaria, in 1999 and 2007, respectively. He is a full professor at the Department of Telecommunication, University of Ruse “Angel Kanchev.” His research interests include mobile communications networks, signal processing, wireless technologies, and satellite navigation. Professor Iliev is a Member of IEEE.

RF is an external Professor of Electronics Engineering with the Department for Computer Science, Faculty of Engineering, and Center for Artificial Intelligence and Cybersecurity, both at the University of Rijeka, Croatia, and Head of the Laboratory for Spatial Intelligence at Hrvatsko Zagorje Krapina University of Applied Science, Krapina, Croatia. He holds a BSc, MSc, and PhD in electrical engineering, obtained in 1987, 1994, and 2007, respectively, from the Faculty of Electrical Engineering and Computing, University of Zagreb, Croatia. His professional interests include \acrfull{aa2} \acrfull{gnss} \acrfull{pnt}, spatial uncertainty quantification, spatial statistical learning, predictive modeling, statistical signal processing, trajectory analysis and prediction, and occupancy modeling. Professor Filjar is a Fellow of \acrfull{rin} (London, United Kingdom), a Member of \acrfull{ion} (Manassas, Virginia), a Senior Member of \acrfull{ursi} (Ghent, Belgium) and a Member of The Society for Industrial and Applied Mathematics  (Philadelphia, Philadelphia).

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%%\input sn-article.bbl

\end{document}