%Version 3 October 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%
\usepackage{anyfontsize}

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
%\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

%\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

%\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\usepackage[acronym]{glossaries}

\makeglossaries
\newacronym{dst}{Dst}{Disturbance Storm-Time}
\newacronym{rin}{RIN}{The Royal Institute of Navigation}
\newacronym{ion}{ION}{The Institute of Navigation}
\newacronym{ursi}{URSI}{Union Radio-Scientifique Internationale}
\newacronym{rinex}{RINEX}{Receiver Independent Exchange Format}
\newacronym{tec}{TEC}{Total Electron Content}
\newacronym{dtec}{dTEC}{standard deviation of Total Electron Content}
\newacronym{gnss}{GNSS}{Global Navigation Satellite System}
\newacronym{pnt}{PNT}{Positioning, Navigation, and Timing}
\newacronym{svm}{SVM}{Support Vector Machine}
\newacronym{svn}{SVN}{Support Vector Network}
\newacronym{svc}{SVC}{Support Vector Clustering}
\newacronym{dt}{DT}{Decision Tree}
\newacronym{tdidt}{TDIDT}{Top-Down Induction of Decision Trees}
\newacronym{nb}{NB}{Naive Bayes}
\newacronym{ci}{CI}{Confidence Interval}
\newacronym{nir}{NIR}{No Information Rate}
\newacronym{nn}{NN}{Neural Network}
\newacronym{ann}{ANN}{Artificial Neural Network}
\newacronym{pls}{PLS}{Partial Least Squares}
\newacronym{pls-da}{PLS-DA}{Partial Least Squares Discriminant Analysis}
\newacronym{fda}{FDA}{Flexible Discriminant Analysis}
\newacronym{lda}{LDA}{Linear Discriminant Analysis}
\newacronym{pca}{PCA}{Principal Component Analysis}
\newacronym{manova}{MANOVA}{Multivariate Analysis Of Variance}
\newacronym{mars}{MARS}{Multivariate Adaptive Regression Splines}
\newacronym{nda}{NDA}{Normal Discriminant Analysis}
\newacronym{nas}{NAS}{Neural Architecture Search}
\newacronym{dbscan}{DBSCAN}{Density-Based Spatial Clustering}
\newacronym{cnn}{CNN}{Convolutional Neural Network}
\newacronym{rf}{RF}{Random Forest}
\newacronym{tp}{TP}{True Positive}
\newacronym{tn}{TN}{True Negative}
\newacronym{fp}{FP}{False Positive}
\newacronym{fn}{FN}{False Negative}
\newacronym{tpr}{TPR}{True Positive Rate}
\newacronym{tnr}{TNR}{True Negative Rate}
\newacronym{ppv}{PPV}{Positive Predictive Value}
\newacronym{npv}{NPV}{Negative Predictive Value}
\newacronym{dr}{DR}{Detection Rate}
\newacronym{dp}{DP}{Detection Prevalence}
\newacronym{ba}{BA}{Balanced Accuracy}
\newacronym{sar}{SAR}{Synthetic Aperture Radar}
\newacronym{cca}{CCA}{Canonical Correlation Analysis}
\newacronym{pod}{POD}{Proper Orthogonal Decomposition}
\newacronym{svd}{SVD}{Singular Value Decomposition}
\newacronym{eof}{EOF}{Empirical Orthogonal Functions}
\newacronym{evd}{EVD}{Eigenvalue Decomposition}
\newacronym{gps}{GPS}{Global Positioning System}
\newacronym{klt}{KLT}{Karhunen–Loève Theorem}
\newacronym{gpu}{GPU}{Graphics Processing Unit}
\newacronym{cpu}{CPU}{Central Processing Unit}
\newacronym{ram}{RAM}{Random Access Memory}
\newacronym{ml}{ML}{machine learning}
\newacronym{aa2}{(AA)2}{Ambient-Aware Application-Aligned}
\newacronym{tid}{TID}{Traveling Ionospheric Disturbance}
\newacronym{epb}{EPB}{Equatorial Plasma Bubbles}
\newacronym{gec}{GEC}{Global Electric Current}
\newacronym{spdf}{SPDF}{Space Physics Data Facility}
\newacronym{noaa}{NOAA}{National Oceanic and Atmospheric Administration}
\newacronym{swpc}{SWPC}{Space Weather Prediction Center}
\newacronym{nrcan}{NRCAN}{Natural Resources Canada}
\newacronym{gfz}{GFZ}{German Research Centre for Geosciences}
\newacronym{nasa}{NASA}{National Aeronautics and Space Administration}
\newacronym{euv}{EUV}{Extreme ultraviolet}

\begin{document}

\title[A $\acrshort{dst}$-based space weather conditions \acrlong{ml} classification  model for \acrshort{gnss} \acrshort{pnt} performance analysis]{A $\acrshort{dst}$-based space weather conditions \acrlong{ml} classification  model for \acrshort{gnss} \acrshort{pnt} performance analysis}

\author[1,2]{\fnm{Lucija} \sur{\v{Z}u\v{z}i\'{c}}}\email{lucija.zuzic@uniri.hr}
\equalcont{These authors contributed equally to this work.}

\author[1,2]{\fnm{Deni} \sur{Klen}}\email{deni.klen@uniri.hr}
\equalcont{These authors contributed equally to this work.}

\author[3]{\fnm{Teodor B.} \sur{Iliev}}\email{tiliev@uni-ruse.bg}

\author*[1,2,4]{\fnm{Renato} \sur{Filjar}}\email{renato.filjar@uniri.hr}

\affil*[1]{\orgdiv{Department of Computer Engineering}, \orgname{Faculty of Engineering, University of Rijeka}, \orgaddress{\street{Vukovarska 58}, \city{Rijeka}, \postcode{51000}, \country{Croatia}}}

\affil[2]{\orgdiv{Center for Artificial Intelligence and Cybersecurity}, \orgname{University of Rijeka}, \orgaddress{\street{Radmile Matejcic 2}, \city{Rijeka}, \postcode{51000}, \country{Croatia}}}

\affil[3]{\orgdiv{Department of Telecommunication}, \orgname{University of Ruse}, \orgaddress{\street{8 Studentska str.}, \city{Ruse}, \postcode{7017}, \country{Bulgaria}}}

\affil[4]{\orgdiv{Laboratory for Spatial Intelligence}, \orgname{Hrvatsko Zagorje Krapina University of Applied Sciences}, \orgaddress{\street{Setaliste hrvatskog narodnog preporoda 6}, \city{Krapina}, \postcode{49000}, \country{Croatia}}}

\abstract{Ambient conditions classification enables systematic mitigation of adversarial effects on the \acrfull{gnss} \acrfull{pnt} performance. This research contributes to the problem by proposing a classification model of space weather events for sub-equatorial regions. The proposed model uses \acrlong{ml}-based classification applied to the experimental observations of geomagnetic field components, observed \acrlong{tec} ($\acrshort{tec}$), and \acrlong{dst} ($\acrshort{dst}$-index). A \acrshort{svm} with a Polynomial Kernel, C5.0 \acrfull{dt}, \acrfull{nb}, shallow \acrfull{nn}, \acrfull{pls}, \acrfull{fda}, and shallow \acrshort{nn} using \acrfull{pca} was applied to develop the candidate model to classify observations of the geomagnetic field in $\acrshort{tec}$, combined with other variables, into one of the scenarios of space weather conditions. Performance is assessed using a confusion matrix and development time to yield the \acrshort{nb} as the best performer. The proposed $\acrshort{dst}$-based classification model serves as an indicator of a geomagnetic/ionospheric storm in progress, thus alerting \acrshort{gnss} users of potential degradation in \acrshort{gnss} \acrshort{pnt} performance and setting up a framework for the development of a tailored \acrshort{gnss} ionospheric correction model for specific classes of the space weather conditions.}

\keywords{\acrfull{gnss} \acrfull{pnt}, space weather conditions, \acrfull{ml}, classification model, \acrfull{dt}, \acrfull{nn}, sensor observations aggregation, geomagnetic field, \acrlong{tec} ($\acrshort{tec}$), \acrlong{dst} ($\acrshort{dst}$-index)}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}
%\label{sec:Intro}

The \acrfull{gnss} and its \acrfull{pnt} service have matured to become an essential part of national infrastructure, public goods, and enablers of a vast number of emerging technology and socio-economic applications \cite{filjar2022application}. Maintaining the \acrshort{gnss} \acrshort{pnt} quality has appeared fundamental for the sustainable development of modern economy and society \cite{filjar2022application}. 

Overcoming the shortcomings and vulnerabilities of \acrshort{gnss} \acrshort{pnt} is a scientific challenge, and the need of a wide variety of scientists, developers, operators, regulators, and users of \acrshort{gnss}-based systems and services \cite{filjar2024ambient, filjar2022application}. The Earth’s ionosphere, a part of the Earth’s atmosphere stretching from $50$ $km$ to $2000$ $km$ above the Earth’s surface and comprised of rare but mostly charged atoms and molecules, is the major natural cause of \acrshort{gnss} \acrshort{pnt} degradation \cite{davies1990ionospheric, filic2018modelling}.

This phenomenon especially affects most currently used \acrshort{gnss} receivers, which work as single-frequency receivers exposed to \acrshort{gnss} ionospheric effects \cite{spilker1996global, filjar2022application}. Driven by powerful and random flows of energy and particles from the Sun (space weather conditions), the ionospheric conditions define the properties of \acrshort{gnss} signal propagation through the Earth’s atmosphere and the resulting ionospheric delay \cite{davies1990ionospheric, oxley2017uncertainties, filic2018modelling}. The \acrshort{gnss} ionospheric delay causes errors in \acrshort{gnss} \acrshort{pnt} algorithm deployment, designed to produce position, velocity, and time estimates for a \acrshort{gnss} receiver \cite{spilker1996global, filic2018modelling}. 

The ionosphere affects \acrshort{gnss} satellite signals for position estimation by introducing signal propagation delay \cite{spilker1996global, filic2018modelling}. The \acrshort{gnss} ionospheric delay is a stochastic variable, whose value is determined by complex physical processes of space weather \cite{filic2018modelling, filjar2022application}.

How space weather affects \acrshort{gnss} \acrshort{pnt} performance was explained using the Space weather – \acrshort{gnss} \acrshort{pnt} performance coupling model \cite{filic2018modelling}, as depicted in Table~\ref{tab:SpaceWeather}.

\begin{table}[!ht]
    \centering
    \caption{The Space weather – \acrshort{gnss} \acrshort{pnt} performance coupling model, after \cite{filic2018modelling}.}
    \label{tab:SpaceWeather}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        SPACE & GEOMAGNETIC & IONOSPHERE & \acrshort{gnss} & \acrshort{gnss} \\
        WEATHER & FIELD & & PSEUDO- & POSITIONING \\
         & & & RANGES & PERFORMANCE \\
        \hline
    \end{tabular}
\end{table}

The \acrshort{gnss} signal encounters a certain number of charged particles from the satellite aerial to a mobile unit’s (\acrshort{gnss} receiver’s) aerial \cite{filic2018modelling, filic2018modelling}. Such an encounter is quantified using the \acrlong{tec} ($\acrshort{tec}$) defined by Equation~\ref{eqn:1} in $electrons/m^{2}$  \cite{filic2018modelling, davies1990ionospheric}, where $h$ denotes the height above the Earth’s mean sea level in $m$, $N(h)$ represents the vertical ionospheric profile, the volume density of charged particles at height $h$, in $electrons/m^{2}$. The $\acrshort{tec}$ dataset used in this study was derived from \acrfull{rinex} $\acrshort{tec}$ observations using \acrfull{gps} $\acrshort{tec}$ software by Seemala \cite{SEEMALA202363}.

\begin{equation}
	\acrshort{tec} = \int_{lower \quad ionsopheric \quad boundary}^{upper \quad ionsopheric \quad boundary}N(h)dh
	\label{eqn:1}
\end{equation}

It should be noted that in the sense of Equation~\ref{eqn:1}, $\acrshort{tec}$ is defined as a result, a consequence, of the ionospheric conditions, and not their descriptor \cite{filjar2022application}. The \acrshort{gnss} ionospheric delay may be determined by derivation from the Appleton-Hartree equation, as given in Equation~\ref{eqn:2}, where $\Delta t_{iono}$ denotes the \acrshort{gnss} ionospheric delay in $s$, $c$ denotes the velocity of an electromagnetic wave in vacuum in $m/s$, and $f$ denotes the carrier wave frequency of the satellite signal in $Hz$ \cite{spilker1996global, filic2018modelling}.

\begin{equation}
	\Delta t_{iono} = \frac{40.3}{c f^{2}} \int_{lower \quad ionsopheric \quad boundary}^{upper \quad ionsopheric \quad boundary}N(h)dh
	\label{eqn:2}
\end{equation}

Combining Equation~\ref{eqn:1} and Equation~\ref{eqn:2}, one can conclude the linear relation between $\Delta t_{iono}$ and $\acrshort{tec}$, as given in Equation~\ref{eqn:3}, a sub-model incorporated in the Space weather – \acrshort{gnss} \acrshort{pnt} performance coupling model \cite{filic2018modelling}.

\begin{equation}
	\Delta t_{iono} = \frac{40.3}{c f^{2}} \acrshort{tec}
	\label{eqn:3}
\end{equation}

The \acrshort{gnss} ionospheric delay has been identified as a source of \acrshort{gnss} \acrshort{pnt} degradation since the dawn of \acrshort{gnss}. \acrshort{gnss} systems offer various standard \acrshort{gnss} ionospheric delay estimation (correction) models to mitigate the deteriorating effects on \acrshort{gnss} \acrshort{pnt}, such as the Klobuchar model \cite{spilker1996global, filic2018modelling}. The standard ionospheric correction models are global, and insufficiently flexible to update to mitigate \acrshort{gnss} ionospheric delay to satisfy rising demands on \acrshort{gnss} \acrshort{pnt} performance \cite{filjar2024ambient, filic2018modelling}. The development of regional and local models attempts to solve the problem of \acrshort{gnss} \acrshort{pnt} sustainable performance in various ionospheric conditions \cite{sikirica2021risk}. In recent developments, our team has proposed the \acrfull{aa2} \acrshort{pnt} to take into account the actual ionospheric and geomagnetic conditions near a mobile unit (a \acrshort{gnss} receiver) \cite{filjar2022application, filic2018modelling, filjar2024ambient}. Direct measurements of the immediate geomagnetic and ionospheric condition variables may be supplied to a \acrlong{ml}-based adapted \acrshort{gnss} ionospheric correction model, thus solving the single-frequency \acrshort{gnss} problem \cite{filjar2022application}. Previous research has identified predictors and target variables (descriptors of geomagnetic, ionospheric, and \acrshort{gnss} \acrshort{pnt} conditions) \cite{filic2018modelling, natras2022ensemble, natras2023regional}. Current space weather severity scales, such as the one provided by \acrfull{noaa} \cite{NOAA2024eSpace}, are based on global space weather and geomagnetic indices averaged over a certain period (for instance, $3$ hours for the global $K_{p}$ index). The current space weather severity scales do not directly address classifying scenarios of \acrshort{gnss} performance deterioration and have limited potential in deployment for a \acrshort{gnss} ionospheric correction model. The classification of different scenarios of \acrshort{gnss} ionospheric conditions with adverse effects on the \acrshort{gnss} \acrshort{pnt} remained an unsolved precondition needed for the development of a \acrlong{ml}-based \acrshort{gnss} ionospheric delay correction model to render the \acrshort{gnss} \acrshort{pnt} algorithm ionospheric conditions-agnostic \cite{filjar2022application, filjar2024ambient}.

A methodology for a \acrlong{ml}-based classification of ionospheric conditions based entirely on observations of geomagnetic indices is described in this study. The proposed method is inspired by previous lightweight machine-learning techniques \cite{filjar2020comparison} for classifying sensor readings. The model is sufficiently simple to be applied on computationally capable platforms with suitable geomagnetic field sensors, such as smartphones and connected/autonomous vehicles. The research presented acquires ambient data and analyses its statistical properties. The dataset is split into training and test sets. Several candidates for the \acrshort{gnss} ionospheric delay model are developed based on \acrlong{dst} ($\acrshort{dst}$) data taken from the INTERMAGNET \cite{Intermagnet2022-cj} dataset, and reformatted to match the format of $\acrshort{tec}$ data. The \acrfull{ml} models include a \acrfull{svm} with a Polynomial Kernel, C5.0 \acrfull{dt}, \acrfull{nb}, shallow \acrfull{nn}, \acrfull{pls}, \acrfull{fda} and shallow \acrfull{nn} using \acrfull{pca} of the input data. A tailored set of validation methods is used to assess their performance. The optimal \acrshort{gnss} ionospheric delay correction model is identified based on \acrshort{gnss} \acrshort{pnt}-related objective criteria, and its performance is demonstrated in an independent case study.

\section{Method and Data}
\label{sec:Dataset}

A \acrfull{svm} with a Polynomial Kernel, C5.0 \acrfull{dt}, \acrfull{nb}, \acrfull{nn}, \acrfull{pls}, \acrfull{fda} and shallow \acrfull{nn} using \acrfull{pca} of the input data were tested based on their ability to classify a set of observations of the geomagnetic field in $\acrshort{tec}$, and other predictors, into one of the scenarios of space weather conditions based on $\acrshort{dst}$. 

Multiple $\acrshort{dst}$-dependent classes were predefined using theoretical knowledge. Statistical analysis of the data confirmed that distributions of other variables change for different $\acrshort{dst}$ ranges, supporting the validity of the classification. The study assumes that the dependent output variable, the $\acrshort{dst}$ class, can be predicted based on the independent variables used as input.

$\acrshort{tec}$ data was obtained using \acrshort{gps} $\acrshort{tec}$ software by Seemala to process \acrshort{rinex} $\acrshort{tec}$ observations \cite{SEEMALA202363}. The INTERMAGNET \cite{Intermagnet2022-cj} dataset contains $\acrshort{dst}$ and $a_{p}$ data from 2014 for a measuring station maintained by Geoscience Australia in Kakadu, referred to as KDU in the database, at $-12.69$ degrees latitude and $132.47$ degrees longitude near Darwin, Nothern Territory, Australia. The two datasets are merged based on location, year, month, day, and time of day in hours.

\subsection{Method}
%\label{subsec:Method}

The models were selected because they represent larger families of classification methods. \acrshort{svm} models are supervised maximum margin models. \acrshort{dt} models also apply supervised learning. \acrshort{nb} classifiers are probabilistic classifiers that can be parametric or non-parametric, but this study uses a non-parametric approach. \acrshort{pls} is a non-parametric linear regression model. \acrshort{fda} uses multiple non-parametric linear regression models to create a non-linear classification. \acrshort{pca} is a linear dimensionality reduction technique that extracts a predefined number of components for training an \acrshort{nn} model. \acrshort{nn} models imitate the brain using artificial neurons to produce outputs based on the input and the activation function. \acrshort{nn} models require that the structure be predefined, and hyperparameters are tuned.

All \acrshort{nn} models were applied based on research by Kuhn for the \textit{R} \textit{caret} package \cite{kuhn2013applied, KuhnCaret2024, rprojectProjectStatistical}.

\subsubsection{Support Vector Machine}
%\label{subsubsec:SupportVectorMachine}

In \acrlong{ml}, a \acrfull{svm} or \acrfull{svn} model is a supervised maximum margin model with associated learning algorithms that analyzes data for classification, regression \cite{Cortes1995}, or outlier detection \cite{scikit-learn2023}. \acrshort{svm} models are also effective for non-linear classification using the hyperplane kernel trick \cite{Boser1992, Aizerman1964}. Intuitively, a good separation is achieved by the hyperplane with the greatest distance to the nearest point in the training data belonging to any class \cite{HastieRosset2009}. \acrfull{svc} \cite{BenHur2001} is a related method for clustering unlabeled data.

Meyer, Leisch, and Hornik compared \acrshort{svm} models with other classifiers \cite{Meyer2003}. However, it is unclear whether \acrshort{svm} predictions perform better than other linear models, such as logistic, and linear regression.

A multidimensional feature space increases the generalization error of \acrshort{svm} models, so additional samples are needed to enhance performance \cite{Jin2012}. To keep the computational burden reasonable, a kernel probability density function $k(x, y)$ is chosen to fit the problem \cite{Press2007}.

The performance of \acrshort{svm} depends on the kernel probability density function, its parameters, and the "soft" margin parameter $\lambda$ chosen in cross-validation to train the final classification model \cite{Hsu2003}.

\subsubsection{Decision Tree}
%\label{subsubsec:DecisionTree}

\acrfull{dt} models are used for supervised learning in statistics, data mining \cite{Rokach2014}, and \acrlong{ml}. Classification trees use a discrete target variable, while regression trees can handle continuous values \cite{Studer2011}. \acrshort{dt} models are popular due to their comprehensibility and simplicity \cite{Wu2008}.

A tree is recursively partitioned by dividing the original set, or root node, into subsets that form descendants, or successors, using classification rules based on features \cite{ShalevShwartz2014}. This process of \acrfull{tdidt} \cite{Quinlan1986} is an example of a greedy algorithm that is the most common strategy for learning \acrshort{dt} models from data \cite{Rokach2005}. C5.0, used in the \textit{caret} package in \textit{R}, has a similar approach and improves the ID3 and C4.5 algorithms.

\subsubsection{Naive Bayes}
%\label{subsubsec:NaiveBayes}

In statistics, \acrfull{nb} models, simple Bayes, or independent Bayes \cite{Hand2001} classifiers are a family of linear "probabilistic classifiers" that assume that, given a target class, the features are conditionally independent. These classifiers are among the simplest Bayesian network models \cite{McCallum2011}, but a \acrshort{nb} classifier model is not necessarily a Bayesian method. Maximum likelihood training evaluates a closed-form expression \cite{Russell1999} in linear time instead of using iterative approximation.

Despite their simplicity, \acrshort{nb} classifier models have performed well in real-world situations \cite{Metsis2006}. However, a comprehensive comparison in 2006 showed that Bayesian classification performed worse than boosted trees or \acrfull{rf} \cite{Caruana2006}. An advantage of \acrshort{nb} over other models is a smaller amount of training data \cite{John2013, Mccallum2001}.

In the abstract, \acrshort{nb} models assign conditional probabilities $p(C_{k}\mid x_{1},\ldots, x_{n})$ to each of the $K$ possible outcomes or classes $C_{k}$ for an instance to be classified, represented by the vector $x = (x_{1},\ldots, x_{n})$ encoding $n$ features \cite{Murty2011}. The model can be reformulated using Bayes' theorem in Equation~\ref{eqn:4}.

\begin{equation}
	p(C_{k}\mid \mathbf{x})={\frac{p(C_{k})\ p(\mathbf{x} \mid C_{k})}{p(\mathbf{x})}}
	\label{eqn:4}
\end{equation}

\subsubsection{Neural Networks}
%\label{NeuralNetworks}

The neurons of human or animal brains provide the basis for a \acrfull{nn} or \acrfull{ann} with connected units or nodes called artificial neurons in \acrlong{ml} \cite{mitExplainedNeural, brahme2014comprehensive}. 

Shallow \acrshort{nn} models typically contain only a few hidden layers for processing between the input layer that receives the data and the final layer that produces the output \cite{olden2002illuminating, ozesmi1999artificial}. A network with at least two hidden layers \cite{bishop2006pattern} is a deep \acrshort{nn} model.

Gradient-based methods such as backpropagation estimate \acrshort{ann} parameters \cite{vapnik2013nature} to minimize the difference or empirical risk between the output and target labels, expressed in a loss function \cite{goodfellow2016deep}. The hyperparameters may also be modified to suit the problem \cite{probst2019tunability} during an extensive tuning process, like the one used in this study.

\acrfull{pca} \cite{pearson1901liii, stewart2019introduction} is a linear dimensionality reduction technique in exploratory data analysis, visualization \cite{jolliffe2016principal}, and preprocessing. The $pcaNNet$ method in the \textit{caret} package in \textit{R} \cite{ripley2007pattern} uses \acrshort{pca} in preprocessing.

\subsubsection{Partial Least Squares}
%\label{subsubsec:PLS}

\acrfull{pls} regression, or projection to latent structures, \cite{wold2001pls, abdi2010partial}, is a linear regression statistical model that transforms the predicted and the observable variables to a new space. \acrshort{pls} methods are bilinear factor models because the $X$ and $Y$ are projected to new spaces. In \acrfull{pls-da}, $Y$ is categorical \cite{saebo2008lpls}.

Using $n$ paired observations $\left(\vec{x_{i}}, \vec{y_{i}}\right), i \in 1, \dots, n$. \acrshort{pls} finds the normalized direction $\vec{p_{j}}, \vec{q_{j}}$ that maximizes the covariance in the first step $j = 1$, shown in Equation~\ref{eqn:5}.

\begin{equation}
	\max_{{\vec{p}}_{j},{\vec{q}}_{j}}\operatorname{E} [\underbrace{({\vec{p}}_{j}\cdot {\vec{X}})}_{t_{j}}\underbrace{({\vec{q}}_{j}\cdot {\vec{Y}})}_{u_{j}}
	\label{eqn:5}
\end{equation}

The scores (projections) form an orthogonal basis in \acrshort{pls} regression, and the loadings are chosen to achieve this. In \acrshort{pca} orthogonality is not imposed on scores \cite{lindgren1993kernel} but onto loadings instead \cite{de1994comments, dayal1997improved}. Many versions of \acrshort{pls} exist \cite{de1993simpls, rannar1994pls} for estimating the factor and loading matrices, such as the PLS1 algorithm \cite{takane2016pls, hoskuldsson1988pls}. 

\subsubsection{Flexible Discriminant Analysis}
%\label{subsubsec:FDA}

\acrfull{fda} is a general methodology that creates the discriminant surface for a multigroup non-linear classification model  \cite{fisher1936use, mclachlan2005discriminant} based on a mixture of non-parametric linear regression models \cite{hastie1995penalized}, such as \acrfull{mars} and \acrfull{lda}.

Many predictors can be used at once in \acrshort{fda} \cite{HastieTibshirani2009}, and variable interactions are automatically noted \cite{hastie1994flexible}. \acrshort{fda} is complex but execution time and computational load are adequate \cite{reynes2006choice}. The algorithm is not largely affected by outliers \cite{phillips2017applying}.

Changed settings significantly alter \acrshort{fda} \cite{hallgren2019species}, and estimation might fail if predictors are correlated. \acrshort{fda} is prone to overfitting \cite{thuiller2016ensemble}, and challenging to comprehend \cite{quillfeldt2017influence}. Feature normality and equal group covariances are assumed \cite{zhang2019using, wetcher2011analyzing}.

\acrshort{lda}, \acrfull{nda}, or discriminant function analysis \cite{cohen2013applied} is a generalization of Fisher's linear discriminant defined in 1936 \cite{fisher1936use, archive765Discriminant, rao1948utilization}. The results of \acrshort{lda} may be utilized directly, as demonstrated in this experiment, but they are more often used to reduce dimensionality \cite{hansen2005using}.

\subsection{Data Description and Analysis}
%\label{subsec:Data}

Dynamic space weather conditions, such as solar activity and geomagnetic storms, can affect  \acrshort{gnss} \acrshort{pnt} performance and high-frequency \acrshort{gps} signals passing through the ionosphere, motivating work on error modeling \cite{klobuchar1986design, zolesi2014ionospheric}. Changes in the ionosphere include the density distribution, $\acrshort{tec}$, and the current balance \cite{komjathy2023global}.

Geomagnetic storms cause signal deterioration by affecting \acrfull{gec} variability \cite{NOAA2024aRadioCommunications}. The ionosphere may show changes related to location, geomagnetic and solar activity, sunspots, local time \cite{kotz2005encyclopedia}, seasonality, thunderstorms \cite{vellinov1992ionospheric}, nuclear experiments, earthquakes \cite{liu20142013}, and other phenomena.

Many parameters can describe the Earth's geomagnetic field \cite{ulukavak2018analysis}. This study focuses on parameters describing disturbances, most importantly $a_{p}$ indices calculated from $K_{p}$ and $K$ indices, \acrlong{tec} ($\acrshort{tec}$), \acrlong{dtec} ($\acrshort{dtec}$), and \acrlong{dst} ($\acrshort{dst}$). 

Incorporating parameters such as the $K_{p}$-indices and $a_{p}$-indices, which provide global measures of geomagnetic activity, alongside local $\acrshort{tec}$ and $\acrshort{dst}$-index values, allows for a more detailed assessment of the space environment and its potential effects on \acrshort{gnss} signals. Values of $a_{p}$, $\acrshort{tec}$, $\acrshort{dtec}$, $\acrshort{dst}$ were used with the $B_x$, $B_y$, and $B_z$ components of the Earth's magnetic field to train machine-learning models.

\subsubsection{Magnetic Field Indices}
%\label{subsec:BxByBz}

The Earth's magnetic field has similarities to that of a bar magnet. However, plasma gushes from the solar corona and the domain of the Sun influence the interplanetary magnetic field \cite{schwenn2001solar, melnikov1990relationships}. The $B_{x}$, $B_{y}$, and $B_{z}$ vectors represent interplanetary magnetic field indices. $B_{x}$ and $B_{y}$ are parallel to the plane of orbits, and the third component $B_{z}$ is perpendicular. Widely available hand-held devices, such as Android smartphones \cite{Android2024SensorTypes}, measure magnetic field indices in micro-Tesla ($\mu T$). The Android magnetometer reports accuracy through a status variable. Readings are calibrated using temperature compensation, factory (or online) soft-iron, and online hard-iron calibration.
 
\subsubsection{Geomagnetic Storm Indices}
%\label{subsec:K}

The geomagnetic storm $K$-index is an integer from $0$ to $9$ measuring disturbances in global geomagnetic activity. The maximum positive and negative fluctuations of the horizontal components of the Earth's magnetic field \cite{de2007ionosphere}, $B_{x}$ and $B_{y}$, during $3$ hours, relative to a quiet day, are added to determine the total maximum fluctuation.

Each observatory uses different threshold values to convert the maximum $nT$ (nano-Tesla) fluctuation to a $K$-index value. The thresholds for each observatory are adjusted so that the historical rate of occurrence for each $K$-index value is similar across all observatories. Observatories with a lower geomagnetic latitude use a lower fluctuation in $B_{x}$ and $B_{y}$ to achieve each $K$-index value. 

The threshold valid in Boulder, Colorado \cite{NOAA2024bKindex} for transforming the maximum $B_{x}$ and $B_{y}$ fluctuation measured in $nT$ using a magnetometer into a $K$-index value is given in Table~\ref{tab:K}.

\begin{table}[!ht]
    \centering
    \caption{The relationship between the $K$-index and  magnetometer observations of $B_{x}$ and $B_{y}$ in $nT$ in Boulder, Colorado.}
    \label{tab:K}
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        $K$ & $0$ & $1$ & $2$ & $3$ & $4$ \\ \hline
        $B_{x}$ and $B_{y}$ ($nT$)& $0$ - $5$ & $5$ - $10$ & $10$ - $20$ & $20$ - $40$ & $40$ - $70$ \\ \hline
        $K$ & $5$ & $6$ & $7$ & $8$ & $9$ \\ \hline
        $B_{x}$ and $B_{y}$ ($nT$) & $70$ - $120$ & $120$ - $200$ & $200$ - $330$ & $330$ - $500$ & $>500$ \\ \hline
    \end{tabular}
\end{table}

\subsubsection{Planetary Geomagnetic Storm Indices}
%\label{subsec:Kp}

The planetary geomagnetic storm $K_{p}$-index \cite{Matzka2024} is derived from $3$-hour-based $K$ indices from $13$ magnetometer stations between $44$ and $60$ degrees of north and south latitude. Announcements and warnings \cite{NOAA2024cAlertsWatches} of geomagnetic changes and disturbances in the Earth's magnetic field are based on the $K_{p}$-index.

Hourly geomagnetic storm index data are available on the \acrshort{nasa} Goddard \acrshort{spdf} \cite{Papitashvili2024About, Papitashvili2024Explorer} web pages. The scale values of the $K_{p}$-index are determined by the change of the geomagnetic field and the geomagnetic storm effect in $nT$.

The official planetary $K_{p}$-index is a weighted average of $K$-indices from multiple observatories. When $K$-index data is not available in real-time, operators such as The \acrfull{noaa} \acrfull{swpc} calculate near real-time estimates of the $K_{p}$-index \cite{Myint2022}.

The $K_{p}$-index as related to geomagnetic storm descriptions and warnings using the \acrshort{noaa} G scale \cite{NOAA2024eSpace, NOAA2024fScales} is given in Table~\ref{tab:Kp}.

In March 2021, $K_{p}$ was assigned a DOI with a dataset \cite{Matzka2021a} and a scientific publication \cite{Matzka2021b} for reference.

\begin{table}[!ht]
    \centering
    \caption{The $K_{p}$-index as related to geomagnetic storm descriptions and warnings using the \acrshort{noaa} G scale \cite{NOAA2024eSpace, NOAA2024fScales}.}
    \label{tab:Kp}
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        $K_{p}$-index & $<5$ & $5$ & $6$ & $7$ & $8, 9_{-}$ & $9$ \\ \hline
        \acrshort{noaa} Space Weather Scale Geomagnetic Storm Level & G0 & G1 & G2 & G3 & G4 & G5 \\ \hline
    \end{tabular}
\end{table}

\subsubsection{Equivalent Three Hourly Range Geomagnetic Storm Indices}
%\label{subsec:A}

The $A$-index \cite{NOAA2024gGeomagneticIndices} represents a daily average level of magnetic activity. Because the relationship between the $K$-index and magnetometer fluctuations is not linear, the $K$-index values are not directly used for calculating average values. Each $K$-index or $K_{p}$-index is converted into the "equivalent three hourly range" $a$-index or $a_{p}$-index that uses a linear scale. Table~\ref{tab:a} illustrates the conversion between $K$-index and $a$-index values \cite{NOAA2024bKindex}. Table~\ref{tab:ap} illustrates the conversion between $K_{p}$-index and $a_{p}$-index values \cite{NOAA2024gGeomagneticIndices}. An average of $8$ $a$-indices (lowercase) is used as the daily $A$-index (uppercase). As an example, if $8$ $K$-indices were $3$, $4$, $6$, $5$, $3$, $2$, $2$, and $1$, the $a$-indices equal $15$, $27$, $80$, $48$, $15$, $7$, $7$, and $3$, and the $A$-index equals $A = (15 + 27 + 80 + 48 + 15 + 7 + 7 + 3)/8 = 202 / 8 = 25.25$.

\begin{table}[!ht]
    \centering
    \caption{The $a$-index values for each $K$-index, from \cite{NOAA2024bKindex}.}
    \label{tab:a}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
        \hline
        $K$ & $0$ & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $8$ & $9$ \\ \hline
        $a$ & $0$ & $3$ & $7$ & $15$ & $27$ & $48$ & $80$ & $140$ & $240$ & $400$ \\ \hline
    \end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{The $a_{p}$-index values for each $K_{p}$-index, from \cite{NOAA2024gGeomagneticIndices}.}
    \label{tab:ap}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
        \hline
        $K_{p}$ & $0_{o}$ & $0_{+}$ & $1_{-}$ & $1_{o}$ & $1_{+}$ & $2_{-}$ & $2_{o}$ & $2_{+}$ & $3_{-}$ & $3_{o}$ & $3_{+}$ & $4_{-}$ & $4_{o}$ & $4_{+}$ \\ \hline
        $a_{p}$ & $0$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $9$ & $12$ & $15$ & $18$ & $22$ & $27$ & $32$ \\ \hline
        $K_{p}$ & $5_{-}$ & $5_{o}$ & $5_{+}$ & $6_{-}$ & $6_{o}$ & $6_{+}$ & $7_{-}$ & $7_{o}$ & $7_{+}$ & $8_{-}$ & $8_{o}$ & $8_{+}$ & $9_{-}$ & $9_{o}$ \\ \hline
        $a_{p}$ & $39$ & $48$ & $56$ & $67$ & $80$ & $94$ & $111$ & $132$ & $154$ & $179$ & $207$ & $236$ & $300$ & $400$ \\ \hline
 \end{tabular}
\end{table}

\subsubsection{Disturbance Storm-Time}
%\label{subsubsec:Dst}

The next relevant parameter is \acrlong{dst} ($\acrshort{dst}$), also known as the geomagnetic activity $\acrshort{dst}$-index, another measure of geomagnetic storm intensity. The $\acrshort{dst}$-index is a geomagnetic indicator of magnetic flux changes derived from measurements taken by a network of ground-based magnetometer stations near the magnetic equator, which continuously monitor $B_{x}$ and $B_{y}$, the horizontal components of Earth's magnetic field \cite{zolesi2014ionospheric}. The $\acrshort{dst}$-index describes ring currents forming above the sub-equatorial region and affecting the ionospheric regions in mid-latitudes.

To calculate the $\acrshort{dst}$-index, variations in $B_{x}$ and $B_{y}$, the horizontal magnetic field, are obtained from multiple stations and averaged. The average is subtracted from a baseline value representing the quiet-time magnetic field. The resulting $\acrshort{dst}$ value in $nT$ measures the intensity of geomagnetic disturbances, with increasingly negative values indicating stronger geomagnetic storms. 

The $\acrshort{dst}$-indice measurements as an hourly average were evaluated and published on a web interface by the \acrshort{nasa} Goddard \acrshort{spdf} \cite{Papitashvili2024About, Papitashvili2024Explorer}, and the Geomagnetism and Space Magnetism Data Analysis Center of the Institute of Science, Kyoto University in Japan.

Loewe and Prölss \cite{loewe1997classification} classified magnetic activity $\acrshort{dst}$-indices into five storm classes in 1997, as shown in Table~\ref{tab:loewe1997}. Gonzalez et al. \cite{gonzalez1994geomagnetic} used three groups for the same data in 1994, as shown in Table~\ref{tab:gonzalez1994}, similar to Kamide et al. in 1998 \cite{kamide1998two}, Rozhnoi et al. in 2004 \cite{rozhnoi2004middle}, and Contadakis et al. in 2012 \cite{contadakis2012total}.

\begin{table}[!ht]
    \centering
    \caption{Storm classification from Loewe and Prölss, in 1997 \cite{loewe1997classification}.}
    \label{tab:loewe1997}
    \begin{tabular}{lrrrrrcr}
        \hline
        \\
        Storm class & \multicolumn{2}{c}{Fraction} & $\acrshort{dst}_{min}$-Range & $\overline{\acrshort{dst}_{min}}$ & $\overline{a_{p_{max}}}$ & $\overline{K_{p_{max}}}$ & $\overline{A_{E_{max}}}$ \\
        \\
        \hline
        \\
        weak & $482$ & $44\%$ & $-30$ to $-50$ $nT$ & $-36$ $nT$ & $27$ & $4_{o}$ & $542$ $nT$ \\
        moderate & $346$ & $32\%$ & $-50$ to $-100$ $nT$ & $-68$ $nT$ & $48$ & $5_{o}$ & $728$ $nT$ \\
        strong & $206$ & $19\%$ & $-100$ to $-200$ $nT$ & $-131$ $nT$ & $111$ & $7_{-}$ & $849$ $nT$ \\
        severe & $45$ & $4\%$ & $-200$ to $-350$ $nT$ & $-254$ $nT$ & $236$ & $8_{+}$ & $1017$ $nT$ \\
        great & $6$ & $1\%$ & $<-350$ $nT$ & $-427$ $nT$ & $300$ & $9_{-}$ & $1335$ $nT$ \\
        \\
        \hline
        \\
        \multicolumn{8}{l}{The bars above the magnetic indices indicate median values.} \\
    \end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{Storm classification from Gonzales et al., in 1994 \cite{gonzalez1994geomagnetic}.}
    \label{tab:gonzalez1994}
    \begin{tabular}{lrrc}
        \hline
        \\
         & $\acrshort{dst}$, $nT$ & $B_{z}$, $nT$ & $\Delta T$, hours \\
        \\
        \hline
        \\
        Intense & $-100$ & $-10$ & $3$ \\
        Moderate & $-50$ & $-5$ & $2$ \\
        Small (typical substorm) & $-30$ & $-3$ & $1$ \\
        \\
        \hline
    \end{tabular}
\end{table}

\subsubsection{Training and Testing Dataset}
%\label{subsubsec:DataTrainTest}

Among $9799$ samples, $208$ were considered outliers because the $\acrshort{tec}$ is larger than or equal to $300$ $nT$. The remaining $9591$ samples are used for training and testing. The samples are divided into training and testing datasets as close as possible to a ratio of $80\%$ for training and $20\%$ for testing. The division was stratified so that an approximately equal ratio of classes was present in both the training and testing data, which is a feature of the \textit{createDataPartition} function from the \textit{caret} \textit{R} library that was used \cite{KuhnDataSplitting2024, hyndman2018forecasting, createDataPartition2024}.

Samples were split into $5$ class ranges, based on $\acrshort{dst}$ values derived from theoretical knowledge of different storm phases and similar to Loewe and Prölss \cite{loewe1997classification}. The upper range limits are excluded, while the lower ones are included. 

Table \ref{tab:Dstranges} lists the $\acrshort{dst}$ class ranges used in this study, the total number of samples in each class, and the number of samples used for testing, and training. It is evident from this division that the normal (N), and recovery (R) classes are far more common than other classes with very high or low $\acrshort{dst}$ values, impacting model performance.

\begin{table}[!ht]
    \centering
    \caption{$\acrshort{dst}$-based classification rules used in this study, the total number of samples in each class, and the number of samples used for testing, and training.}
    \label{tab:Dstranges}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        $\acrshort{dst}$ & Storm phase classification & Total samples & Test samples & Train samples \\ \hline
        $ \geq 15 $ & positive phase (P) & $135$ & $27$ & $108$ \\ \hline
        $[-20, 15>$ & normal (N) & $7495$ & $1499$ & $5996$ \\ \hline
        $[-55, -20>$ & recovery phase (R) & $1858$ & $371$ & $1487$ \\ \hline
        $[-85, -55>$ & through (T) & $90$ & $18$ & $72$ \\ \hline
        $ < -85$ & extreme (E) & $13$ & $2$ & $11$ \\ \hline
        Any & Any & $95$ & $2$ & $11$ \\ \hline
    \end{tabular}
\end{table}

\subsubsection{Data Preprocessing}
%\label{subsubsec:Preprocessing}

Data preprocessing can increase classification accuracy \cite{Fan2008}. There are many ways to standardize data, such as minimum-maximum, normalization by decimal scaling, and Z-score \cite{Mohamad2013}. Subtracting the mean and dividing by the variance for each feature are commonly used for \acrshort{svm} models \cite{Fennell2019} and other models tested in this study, so this approach was chosen. 

The values \textit{scale} and \textit{center} were used in the code for this study in the \textit{preProcess} parameter for the \textit{train} function from the \textit{caret} package developed for \textit{R}. The option \textit{center} subtracts the mean of each feature while \textit{scale} divides by the standard deviation.

\subsubsection{Correlation}
%\label{subsubsec:Correlation}

Figure~\ref{fig:correlation} contains a heat map of the correlation between all variables used in this study. The highest correlation of $0.52$ is between $B_{x}$ and $\acrshort{dst}$, as expected based on theoretical knowledge, and supporting the hypothesis that $\acrshort{dst}$ can be predicted using $B_{x}$. The second highest correlation is between $a_{p}$ and $\acrshort{dst}$ and equals $-0.48$, meaning $a_{p}$ can be used to predict  $\acrshort{dst}$ with an opposite trend.

\begin{figure}
 \centering
 \includegraphics[width=0.9\linewidth]{iono3correlation.pdf}
    \caption{A heat map of the correlation between all variables used in this study, when the $\acrshort{tec}$ is less than $300$ $nT$. Red represents a high positive correlation, blue represents a high negative correlation, and white represents a low correlation. Variables are fully correlated with themselves, so values on the secondary diagonal equal $1$. The matrix is symmetrical concerning the secondary diagonal because the same combination of correlated variables is achieved when swapping the row and column.}
    \label{fig:correlation}
\end{figure}

The box plots of all variables for different ranges of $\acrshort{dst}$ values in Figure~\ref{fig:iono3boxplot} demonstrate that the minimum, maximum, and arithmetic mean of $a_{p}$ decrease for larger $\acrshort{dst}$. The opposite is true for $B_{x}$, as indicated by a high correlation in Figure~\ref{fig:correlation}. $B_{y}$ exhibits a reverse trend when compared to $B_{x}$, but it is less prominent. $B_{z}$ is the most stable with the smallest changes related to $\acrshort{dst}$ when observing $B_{x}$, $B_{y}$, and $B_{z}$.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{iono3boxplot_fix.pdf}
    \caption{Box plots of all variables, when the $\acrshort{tec}$ is less than $300$ $nT$, for different ranges of $\acrshort{dst}$ values defining the class label used in this study.}
    \label{fig:iono3boxplot}
\end{figure}

Table~\ref{tab:minmax} provides the minimum, $1^{st}$ quartile, median, arithmetic mean, $3^{rd}$ quartile, and maximum values for all variables when the $\acrshort{tec}$ is less than $300$ $nT$. These values suggest that variables are not normally distributed. The Kolmogorov-Smirnov and Shapiro-Wilk normality tests, using \textit{R} functions \textit{ks.test} \cite{ks.test2024} and \textit{shapiro.test} \cite{shapiro.test2024}, did not yield a $p$-value larger than the selected $\alpha$-value of $0.05$ for any variable, further strengthening the claim that variables do not follow a normal (Gaussian) distribution. 

\begin{table}[!ht]
    \centering
    \caption{The minimum, $1^{st}$ quartile, median, arithmetic mean, $3^{rd}$ quartile, and maximum values for all variables when the $\acrshort{tec}$ is less than $300$ $nT$.}
    \label{tab:minmax}
        \begin{tabular}{|c|c|c|c|c|c|c|c|}
                \hline
                  & $\acrshort{tec}$ & $\acrshort{dtec}$ & $B_{x}$ & $B_{y}$ & $B_{z}$ & $\acrshort{dst}$ & $a_{p}$ \\ \hline
                Min. & $1.76$ & $0.02$ & $35275.0$ & $1913.0$ & $-29649.0$ & $-119.0$ & $0.0$ \\ \hline
                $1^{st}$ Qu. & $8.43$ & $1.12$ & $35402.0$ & $1990.0$ & $-29623.0$ & $-18.0$ & $4.0$ \\ \hline
                Median & $22.83$ & $2.47$ & $35410.0$ & $2004.0$ & $-29620.0$ & $-5.0$ & $6.0$ \\ \hline
                Mean & $24.62$ & $6.208$ & $35413.0$ & $2003.0$ & $-29617.0$ & $-9.678$ & $7.539$ \\ \hline
                $3^{rd}$ Qu. & $34.23$ & $5.54$ & $35423.0$ & $2018.0$ & $-29614.0$ & $-1.0$ & $7.0$ \\ \hline
                Max. & $288.0$ & $997.0$ & $35524.0$ & $2089.0$ & $-29571.0$ & $46.0$ & $94.0$ \\ \hline
        \end{tabular}
\end{table}

\subsection{Performance Metrics}
%\label{subsec:Metrics}

The metrics and terminology used to evaluate the classifier performance were taken from the confusion matrix defined in the \textit{R} function \textit{confusionMatrix} in the \textit{caret} library \cite{kuhn2008building, confusionMatrix2024}.

The examples for each performance metric use only two groups (Yes and No, positive and negative). For multiple classes, results are calculated by a "one versus all" approach, viewing each class as positive and all others as negative.

\subsubsection{Confusion Matrix}
%\label{subsubsec:ConfusionMatrix}

A \acrfull{tp}, or \acrfull{tn} result in the confusion matrix correctly indicates that the sample belongs to a class. A \acrfull{fp}, or \acrfull{fn} classification result incorrectly claims that the sample belongs to a class and represents a type I or type II error, respectively.

Sensitivity, recall, hit rate, or \acrfull{tpr}, calculated as $\mathrm{\acrshort{tp}}/(\mathrm{\acrshort{tp}}+\mathrm{\acrshort{fn}})$, and specificity, selectivity, or \acrfull{tnr}, calculated as $\mathrm{\acrshort{tn}}/(\mathrm{\acrshort{tn}}+\mathrm{\acrshort{fp}})$, represent the proportion of correctly classified samples among those that truly belong to a class \cite{altman1994diagnostic1, altman1994diagnostic2}.

Precision, or \acrfull{ppv}, calculated as $\mathrm{\acrshort{tp}}/(\mathrm{\acrshort{tp}}+\mathrm{\acrshort{fp}})$, and \acrfull{npv}, calculated as $\mathrm{\acrshort{tn}}/(\mathrm{\acrshort{tn}}+\mathrm{\acrshort{fn}})$, represent the proportion of correctly classified samples among those classified to a class.

Prevalence, calculated as $(\mathrm{\acrshort{tp}}+\mathrm{\acrshort{fn}})/(\mathrm{\acrshort{tp}}+\mathrm{\acrshort{fp}}+\mathrm{\acrshort{fn}}+\mathrm{\acrshort{tn}})$, and \acrfull{dp}, calculated as $(\mathrm{\acrshort{tp}}+\mathrm{\acrshort{fp}})/(\mathrm{\acrshort{tp}}+\mathrm{\acrshort{fp}}+\mathrm{\acrshort{fn}}+\mathrm{\acrshort{tn}})$ represent the proportion of samples that truly belong to or are classified to each class, respectively. Prevalence values other than an even split by the number of classes indicate a class imbalance.

The \acrfull{dr}, calculated as $\mathrm{\acrshort{tp}}/(\mathrm{\acrshort{tp}}+\mathrm{\acrshort{fp}}+\mathrm{\acrshort{fn}}+\mathrm{\acrshort{tn}})$, and accuracy (Acc), calculated as $(\mathrm{\acrshort{tp}} + \mathrm{\acrshort{tn}}) / (\mathrm{\acrshort{tp}} + \mathrm{\acrshort{tn}} + \mathrm{\acrshort{fp}} + \mathrm{\acrshort{fn}})$, represent the proportion of correctly classified samples for separate classes or any class, respectively.

Since accuracy is unusable for unbalanced classes, \acrfull{ba}  \cite{velez2007balanced} is calculated as $(\mathrm{\acrshort{tpr}} + \mathrm{\acrshort{tnr}}) / 2$ and represents the arithmetic mean of \acrshort{tpr} and \acrshort{tnr}, which are centered on the positive and the negative classes, respectively.

\section{Research Results}
\label{sec:Results}

The accuracy for all models when using all variables, all variables except $\acrshort{dst}$ (no $\acrshort{dst}$), all variables except $\acrshort{dst}$, $\acrshort{tec}$, and $\acrshort{dtec}$ (no $\acrshort{tec}$), only $B_{x}$, $B_{y}$, and $B_{z}$ (coord), only $B_{x}$, $B_{y}$, and $a_{p}$ ($x$ $y$ $a_{p}$), only $B_{x}$, $B_{z}$, and $a_{p}$ ($x$ $z$ $a_{p}$), or only $B_{y}$, $B_{z}$, and $a_{p}$ ($y$ $z$ $a_{p}$) as input is displayed in Table~\ref{tab:acc:all}. Accuracy results can lead to an incorrect conclusion if the dataset is unbalanced. Out of $9591$ samples for training and testing, $7495$ belonged to the largest N class, and $13$ to the smallest E class, as listed in Table~\ref{tab:Dstranges}. More comprehensive results by class are presented only for the \acrfull{nb} classifier, the second most successful model. The \acrfull{nn} classifier made no errors, so further discussion is redundant.

\begin{table}[!ht]
    \centering
    \caption{The accuracy for each model using different combinations of variables as input.}
	\label{tab:acc:all}
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline
		Model & all & no $\acrshort{dst}$ & no $\acrshort{tec}$ & coord & $x$ $y$ $a_{p}$ & $x$ $z$ $a_{p}$ & $y$ $z$ $a_{p}$ \\ \hline
		\acrshort{svm} Poly & $0.7866$ & $0.7454$ & $0.7903$ & $0.7887$ & $0.7934$ & $0.7903$ & $0.7814$ \\ \hline
		C5.0 \acrshort{dt} & $0.9995$ & $0.8529$ & $0.8581$ & $0.8404$ & $0.8508$ & $0.8367$ & $0.8211$ \\ \hline
		\acrshort{nb} & $0.9953$ & $0.9943$ & $1$ & $1$ & $1$ & $1$ & $0.9995$ \\ \hline
		\acrshort{nn} & $1$ & $1$ & $1$ & $1$ & $1$ & $1$ & $1$ \\ \hline
		\acrshort{pls} & $0.9212$ & $0.8258$ & $0.8232$ & $0.8174$ & $0.8153$ & $0.8143$ & $0.8007$ \\ \hline
		\acrshort{fda} & $0.9025$ & $0.8472$ & $0.8472$ & $0.8252$ & $0.8419$ & $0.8258$ & $0.7997$ \\ \hline
		\acrshort{pca} \acrshort{nn} & $0.9963$ & $0.8492$ & $0.8492$ & $0.8399$ & $0.8456$ & $0.8367$ & $0.8185$ \\ \hline
	\end{tabular}
\end{table}

The execution time in seconds ($s$) utilizing the \textit{R} \textit{system.time} function \cite{system.time2024} for each model and different combinations of variables as input is displayed in Table~\ref{tab:time:total}. The experiment was run on \textit{Windows} 11 using \textit{R Studio} version 2024.04.2+764 and \textit{R} version 4.4.1, the AMD Radeon RX 6600 \acrfull{gpu}, $16$ GB of \acrfull{ram}, and the AMD Ryzen 5 PRO 4650G \acrfull{cpu} with $6$ cores. Execution time is significant because built-in systems for mobile devices using \acrshort{gnss} \acrshort{pnt} have low computational capabilities.

\begin{table}[!ht]
	\centering
    \caption{The execution time in seconds ($s$) for each model when using different combinations of variables as input.}
	\label{tab:time:total}
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline
		Model & all & no $\acrshort{dst}$ & no $\acrshort{tec}$ & coord & $x$ $y$ $a_{p}$ & $x$ $z$ $a_{p}$ & $y$ $z$ $a_{p}$ \\ \hline
		\acrshort{svm} Poly & $148.72$ & $330.67$ & $392.05$ & $582.47$ & $739.55$ & $670.54$ & $876.43$ \\ \hline
		C5.0 \acrshort{dt} & $186.08$ & $670.61$ & $466.42$ & $347.16$ & $357.43$ & $330.81$ & $382.92$ \\ \hline
		\acrshort{nb} & $206.65$ & $200.83$ & $164.03$ & $141.87$ & $143.97$ & $147.26$ & $149.87$ \\ \hline
		\acrshort{nn} & $582.41$ & $629.67$ & $619.30$ & $587.37$ & $614.07$ & $629.48$ & $644.44$ \\ \hline
		\acrshort{pls} & $52.59$ & $63.48$ & $54.26$ & $36.90$ & $52.39$ & $41.92$ & $59.00$ \\ \hline
		\acrshort{fda} & $56.20$ & $98.11$ & $142.19$ & $144.53$ & $164.17$ & $172.64$ & $185.78$ \\ \hline
		\acrshort{pca} \acrshort{nn} & $341.32$ & $422.86$ & $468.73$ & $452.75$ & $480.02$ & $495.66$ & $517.78$ \\ \hline
	\end{tabular}
\end{table}

\subsection{Comprehensive Results for the Naive Bayes Model}
%\label{subsec:ResultsNB}

The confusion matrix and the performance indicators derived from it for the \acrfull{nb} model, when using different combinations of variables as input, are depicted in Table~\ref{tab:cm:merged}, Table~\ref{tab:cs:reverse:all:nb}, Table~\ref{tab:cs:reverse:no_DisturbanceStormTime:nb}, and Table~\ref{tab:cs:reverse:yzap:nb}. The \acrshort{nb} model was chosen because of a low computational load and high accuracy. The results for combinations of input variables with a $100\%$ accuracy using the \acrshort{nb} model are ommited.

\begin{table}[!ht]
    \centering
    \caption{The confusion matrix for the \acrlong{nb} model when using all variables as input (left), all variables except $\acrshort{dst}$ as input (center), and only $B_{y}$, $B_{z}$, and $a_{p}$ as input (right).}
	\label{tab:cm:merged}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		 & \multicolumn{5}{|c|}{Reference - all} & \multicolumn{5}{|c|}{Reference - no $\acrshort{dst}$} & \multicolumn{5}{|c|}{Reference - $y$ $z$ $a_{p}$} \\ \hline
		 Pred. & E & N & P & R & T & E & N & P & R & T & E & N & P & R & T \\ \hline
		 E & $2$ & $0$ & $0$ & $0$ & $0$ & $2$ & $1$ & $0$ & $1$ & $0$ & $1$ & $0$ & $0$ & $0$ & $0$ \\ \hline
		 N & $0$ & $1492$ & $0$ & $1$ & $0$ & $0$ & $1492$ & $0$ & $1$ & $0$ & $1$ & $1499$ & $0$ & $0$ & $0$ \\ \hline
		 P & $0$ & $5$ & $27$ & $0$ & $0$ & $0$ & $4$ & $27$ & $0$ & $0$ & $0$ & $0$ & $27$ & $0$ & $0$ \\ \hline
		 R & $0$ & $2$ & $0$ & $369$ & $0$ & $0$ & $2$ & $0$ & $368$ & $1$ & $0$ & $0$ & $0$ & $371$ & $0$ \\ \hline
		 T & $0$ & $0$ & $0$ & $1$ & $18$ & $0$ & $0$ & $0$ & $1$ & $17$ & $0$ & $0$ & $0$ & $0$ & $18$ \\ \hline
	\end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{The performance indicators derived from the confusion matrix for the \acrlong{nb} model when using all variables as input.}
	\label{tab:cs:reverse:all:nb}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		 & \multicolumn{5}{c|}{Class} \\ \hline
		Statistics & E & N & P & R & T \\ \hline
		Sensitivity & $100\%$ & $99.53\%$ & $100\%$ & $99.46\%$ & $100\%$ \\ \hline
		Specificity & $100\%$ & $99.76\%$ & $99.735\%$ & $99.87\%$ & $99.9473\%$ \\ \hline
		\acrshort{ppv} & $100\%$ & $99.93\%$ & $84.375\%$ & $99.46\%$ & $94.7368\%$ \\ \hline
		\acrshort{npv} & $100\%$ & $98.35\%$ & $100\%$ & $99.87\%$ & $100\%$ \\ \hline
		\acrshort{dr} & $0.1043\%$ & $77.83\%$ & $1.408\%$ & $19.25\%$ & $0.939\%$ \\ \hline
		\acrshort{dp} & $0.1043\%$ & $77.88\%$ & $1.669\%$ & $19.35\%$ & $0.9911\%$ \\ \hline
		\acrshort{ba} & $100\%$ & $99.65\%$ & $99.868\%$ & $99.67\%$ & $99.9737\%$ \\ \hline
	\end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{The performance indicators derived from the confusion matrix for the \acrlong{nb} model when using all variables except $\acrshort{dst}$ as input.}
	\label{tab:cs:reverse:no_DisturbanceStormTime:nb}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		 & \multicolumn{5}{c|}{Class} \\ \hline
		Statistics & E & N & P & R & T \\ \hline
		Sensitivity & $100\%$ & $99.53\%$ & $100\%$ & $99.19\%$ & $94.4444\%$ \\ \hline
		Specificity & $99.8956\%$ & $99.76\%$ & $99.788\%$ & $99.81\%$ & $99.9473\%$ \\ \hline
		\acrshort{ppv} & $50\%$ & $99.93\%$ & $87.097\%$ & $99.19\%$ & $94.4444\%$ \\ \hline
		\acrshort{npv} & $100\%$ & $98.35\%$ & $100\%$ & $99.81\%$ & $99.9473\%$ \\ \hline
		\acrshort{dr} & $0.1043\%$ & $77.83\%$ & $1.408\%$ & $19.2\%$ & $0.8868\%$ \\ \hline
		\acrshort{dp} & $0.2087\%$ & $77.88\%$ & $1.617\%$ & $19.35\%$ & $0.939\%$ \\ \hline
		\acrshort{ba} & $99.9478\%$ & $99.65\%$ & $99.894\%$ & $99.5\%$ & $97.1959\%$ \\ \hline
	\end{tabular}
\end{table}

\begin{table}[!ht]
    \centering
    \caption{The performance indicators derived from the confusion matrix for the \acrlong{nb} model when using only $B_{y}$, $B_{z}$, and $a_{p}$ as input.}
	\label{tab:cs:reverse:yzap:nb}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		 & \multicolumn{5}{c|}{Class} \\ \hline
		Statistics & E & N & P & R & T \\ \hline
		Sensitivity & $50\%$ & $100\%$ & $100\%$ & $100\%$ & $100\%$ \\ \hline
		Specificity & $100\%$ & $99.76\%$ & $100\%$ & $100\%$ & $100\%$ \\ \hline
		\acrshort{ppv} & $100\%$ & $99.93\%$ & $100\%$ & $100\%$ & $100\%$ \\ \hline
		\acrshort{npv} & $99.9478\%$ & $100\%$ & $100\%$ & $100\%$ & $100\%$ \\ \hline
		\acrshort{dr} & $0.0522\%$ & $78.2\%$ & $1.408\%$ & $19.35\%$ & $0.939\%$ \\ \hline
		\acrshort{dp} & $0.0522\%$ & $78.25\%$ & $1.408\%$ & $19.35\%$ & $0.939\%$ \\ \hline
		\acrshort{ba} & $75\%$ & $99.88\%$ & $100\%$ & $100\%$ & $100\%$ \\ \hline
	\end{tabular}
\end{table}

\section{Discussion}
\label{sec:Discussion}

The \acrfull{nn} model has the highest execution time for any subset of the input variables due to its complexity and extensive training, evident from the data in Table~\ref{tab:time:total}. However, it achieved a $100\%$ accuracy, shown in Table~\ref{tab:acc:all}. The \acrfull{nb} model has the second highest accuracy, over $99\%$. The \acrshort{nb} model achieved a $100\%$ accuracy, except when using all input variables, all input variables except \acrlong{dst} ($\acrshort{dst}$), or only $B_{y}$, $B_{z}$, and $a_{p}$. The difference in accuracy between the \acrshort{nn} and the \acrshort{nb} model is negligible, and the training time for the \acrshort{nn} model is more than twice as long. An analysis in 2004 showed reasonable theoretical reasons for the seemingly incredible performance of \acrshort{nb} classifiers \cite{Zhang2004}.

All other models fail to achieve an accuracy over $90\%$, except when using all input variables, including $\acrshort{dst}$. The $\acrshort{dst}$ class was derived by thresholding the continuous $\acrshort{dst}$ value and discretizing it by converting it to a single character, so the original $\acrshort{dst}$ value should not be used as model input. This labeling method explains the increase in accuracy when adding the $\acrshort{dst}$ input variable in all except the two best-performing models. For example, the C5.0 \acrshort{dt} model did not consider any variable except $\acrshort{dst}$, indicated by a $100\%$ variable importance. The \acrfull{svm} model with a Polynomial Kernel is consistently the worst-performing for any subset of input variables, never achieving an accuracy over $80\%$. None of the models achieved an accuracy under $70\%$, so they were all moderately successful.

When studying the performance of the \acrshort{nb} model by individual class in Table~\ref{tab:cm:merged}, Table~\ref{tab:cs:reverse:all:nb}, Table~\ref{tab:cs:reverse:no_DisturbanceStormTime:nb}, and Table~\ref{tab:cs:reverse:yzap:nb}, samples of the T class are all correctly classified except for one case, but the testing is less extensive since it is the second smallest class. Samples of the P class are all correctly classified. Samples of the R class are sometimes erroneously assigned to the N, T, or less commonly the E class. Two of the most common misclassifications were assigning samples of the N class with the highest detection rate and prevalence to the P or R classes with the third and second highest detection rate and prevalence. This is due to the largest number of samples in the N class between the P and R classes with the narrowest $\acrshort{dst}$ range. Samples of the N class are least often included in the E class, whose range is the furthest apart from the N class. A sample of the E class was attributed to the N class only once. The E class is the smallest with only two testing samples, which must be accounted for when interpreting these results, such as the lowest balanced accuracy of $75\%$ and sensitivity of $50\%$ when using only $B_{y}$, $B_{z}$, and $a_{p}$ as input, or the lowest positive predictive value of $50\%$ when using all variables as input.

\section{Conclusion}
\label{sec:Conclusion}

The presented study aims to classify ambient conditions of space weather events for sub-equatorial regions. \acrfull{gnss} \acrfull{pnt} performance is significantly affected by such events. It would be beneficial to warn users of a geomagnetic/ionospheric storm.

Classification models using \acrlong{ml} were applied to descriptions of the geomagnetic field expressed in \acrlong{tec} ($\acrshort{tec}$) and other input data. It was assumed observations contained independent variables to generate the dependent variable representing the \acrlong{dst} ($\acrshort{dst}$) class. 

Statistical analysis confirmed that other variables change distribution based on $\acrshort{dst}$, not $\acrshort{tec}$. Continuous $\acrshort{dst}$ values in different ranges were converted into discrete classes based on statistics, previous theories, and research. 

An \acrfull{svm} with a Polynomial Kernel, C5.0 \acrfull{dt}, \acrfull{nb}, \acrfull{nn}, \acrfull{pls}, \acrfull{fda}, and \acrfull{pca} \acrshort{nn} model created a $\acrshort{dst}$-based classification from multiple combinations of input variables.

The \acrshort{nb} model achieved perfect accuracy for most tested combinations of input variables, and over $99\%$ in all cases. It is slightly less successful than the \acrshort{nn} model, and the total execution time is at least two times shorter, making it more suitable for use in compact, low-performance, and low-cost portable devices such as smartphones.

\clearpage

\printglossary[type=\acronymtype]

\section{Declarations}
%\label{sec:Declarations}

\subsection{Availability of data and materials}
%\label{subsec:Availability}

The datasets used during the current study are available from the corresponding author upon reasonable request.

\subsection{Competing interests}
%\label{subsec:Competing}

The authors declare no conflict of interest.

\subsection{Funding}
%\label{subsec:Funding}

The authors have no funding sources to declare.

\subsection{Authors' contributions}
%\label{subsec:Contributions}

L\v{Z} and DK contributed to conceptualization, methodology, software, validation, formal analysis, investigation, data curation, writing of the original draft, reviewing and editing the text, and visualization. TI contributed to validation, formal analysis, investigation, review and editing, supervision, and project administration. RF contributed to conceptualization, methodology, investigation, resources, writing of the original draft, reviewing and editing the text, and supervision. All authors read and approved the final manuscript.

\subsection{Acknowledgements}
%\label{subsec:Acknowledgements}

Not applicable

\subsection{Authors' information}
%\label{subsec:Information}

LŽ is a doctoral student at the Faculty of Engineering, University of Rijeka, currently employed as an assistant at the Department of Computer Science. Her research interests include applied \acrlong{ml} in biology, chemistry, medicine, and transportation. Maritime transportation is also the subject of her doctoral research.

DK is a doctoral student at the Faculty of Engineering, University of Rijeka, currently employed as an assistant at the Department of Computer Science. His research interests include applied \acrlong{ml} in object recognition and image segmentation. Image classification and labeling are also the subject of his doctoral research.

TI earned his MSc and PhD in telecommunication engineering from the University of Ruse “Angel Kanchev,” Ruse, Bulgaria, in 1999 and 2007, respectively. He is a full professor at the Department of Telecommunication, University of Ruse “Angel Kanchev.” His research interests include mobile communications networks, signal processing, wireless technologies, and satellite navigation. Professor Iliev is a Member of IEEE.

RF is an external Professor of Electronics Engineering with the Department for Computer Science, Faculty of Engineering, and Center for Artificial Intelligence and Cybersecurity, both at the University of Rijeka, Croatia, and Head of the Laboratory for Spatial Intelligence at Hrvatsko Zagorje Krapina University of Applied Science, Krapina, Croatia. He holds a BSc, MSc, and PhD in electrical engineering, obtained in 1987, 1994, and 2007, respectively, from the Faculty of Electrical Engineering and Computing, University of Zagreb, Croatia. His professional interests include \acrfull{aa2} \acrfull{gnss} \acrfull{pnt}, spatial uncertainty quantification, spatial statistical learning, predictive modeling, statistical signal processing, trajectory analysis and prediction, and occupancy modeling. Professor Filjar is a Fellow of \acrfull{rin} (London, United Kingdom), a Member of \acrfull{ion} (Manassas, Virginia), a Senior Member of \acrfull{ursi} (Ghent, Belgium) and a Member of The Society for Industrial and Applied Mathematics  (Philadelphia, Philadelphia). %He established, and organises the traditional Baška SIF (Spatial Information Fusion) Forum, held annually in Baška, Krk Island, Croatia.

% vidi Philadelphia možda clan, zemlje u razvoju samo 15 dolara godišnje, ciljane sekcije
% uncertainty modeling souther California

% smanjti reference ko je moguce

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%%\input sn-article.bbl

\end{document}